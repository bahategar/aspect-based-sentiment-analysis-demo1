{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c615831-571b-45e7-ac47-8b869dc50e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "eac21c66-2234-4386-87f0-6d9c63bdd187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Preparation text\n",
    "\n",
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "##========== PREPARATION TEXT ===========##\n",
    "\n",
    "# Contraction\n",
    "def expand_contractions(sentence, contraction_mapping=CONTRACTION_MAP):\n",
    "    \"\"\"\n",
    "    Expand the contractions in a sentence. For example don't => do not.\n",
    "    \n",
    "    Paramters:\n",
    "    sentence (str): The input sentence to clean.\n",
    "    contraction_mapping (dict): A dictionary for mapping contractions.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    str: The expanded contraction sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expanded_match(contraction):\n",
    "        \"\"\"\n",
    "        Filter for expanding the matched contraction.\n",
    "        \n",
    "        Parameters:\n",
    "        contraction (str): The input of contraction\n",
    "        \n",
    "        Returns:\n",
    "        str: The expanded contraction.\n",
    "        \"\"\"\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n",
    "        \n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_sentence = contractions_pattern.sub(expanded_match, sentence)\n",
    "    return expanded_sentence\n",
    "\n",
    "\n",
    "def remove_extra_spaces(sentence):\n",
    "    # Use regex to replace multiple spaces with a single space\n",
    "    return re.sub(r'\\s+', ' ', sentence).strip()\n",
    "\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"\n",
    "    Remove all non-ASCII characters from the text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to clean.\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned text with only ASCII characters.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return ''.join([char for char in text if ord(char) < 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "2314849b-0de2-4d27-ae9b-c48be7df3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper get specific token and handling token\n",
    "\n",
    "\n",
    "##=========== EXTRACT ASPECT ============##\n",
    "# Cross product two lists\n",
    "def cross_product_str(first, second):\n",
    "    \"\"\"\n",
    "    Do cross product\n",
    "\n",
    "    parameters\n",
    "    -----------\n",
    "    first: list/string\n",
    "    second: list/string\n",
    "\n",
    "    return: list of string\n",
    "    \"\"\"\n",
    "    temp = []\n",
    "    if type(first) == str:\n",
    "        first = [first]\n",
    "    if type(second) == str:\n",
    "        second = [second]\n",
    "    for i in first:\n",
    "        for j in second:\n",
    "            text = (i + ' ' + j).strip()\n",
    "            temp.append(text)\n",
    "    return temp\n",
    "\n",
    "def cross_product_tuple(first, second):\n",
    "    \"\"\"\n",
    "    Do cross product\n",
    "\n",
    "    parameters\n",
    "    -----------\n",
    "    first: list/string\n",
    "    second: list/string\n",
    "\n",
    "    return: list of tuple\n",
    "    \"\"\"\n",
    "    temp = []\n",
    "    if type(first) == str:\n",
    "        first = [first]\n",
    "    if type(second) == str:\n",
    "        second = [second]\n",
    "    for i in first:\n",
    "        for j in second:\n",
    "            temp.append((i, j))\n",
    "    return temp\n",
    "\n",
    "# Cross product flatten\n",
    "def cross_product_flatten(input_1, input_2):\n",
    "    # Check if input_2 is a list of tuples or a list of lists\n",
    "    if not isinstance(input_2, list) or not all(isinstance(i, (tuple, list)) for i in input_2):\n",
    "        raise ValueError(\"input_2 must be a list of tuples or a list of lists.\")\n",
    "\n",
    "    if type(input_1) == str:\n",
    "        input_1 = [input_1]\n",
    "                \n",
    "    result = []\n",
    "    for name in input_1:\n",
    "        for item in input_2:\n",
    "            result.append((name, *item))\n",
    "    return result\n",
    "\n",
    "def cross_product_flatten_append(input_1, input_2):\n",
    "    # Check if input_2 is a list of tuples or a list of lists\n",
    "    if not isinstance(input_2, list) or not all(isinstance(i, (tuple, list)) for i in input_2):\n",
    "        raise ValueError(\"input_2 must be a list of tuples or a list of lists.\")\n",
    "\n",
    "    if type(input_1) == str:\n",
    "        input_1 = [input_1]\n",
    "                \n",
    "    result = []\n",
    "    for name in input_1:\n",
    "        for item in input_2:\n",
    "            result.append((*item, name))\n",
    "    return result\n",
    "    \n",
    "# Get neglection text\n",
    "def get_neglect(token):\n",
    "    if token:\n",
    "        for t in token.children:\n",
    "            if (t.dep_ == 'neg') or (t.dep_ == 'det' and t.text.lower() == 'no'):\n",
    "                return 'not'\n",
    "    return ''\n",
    "\n",
    "# Get token specific pos tag\n",
    "def get_token_pos(token, pos):\n",
    "    if type(pos) == str:\n",
    "        pos = [pos]\n",
    "    for t in token.children:\n",
    "        if t.pos_ in pos:\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_token_pos_left(token, pos):\n",
    "    if type(pos) == str:\n",
    "        pos = [pos]\n",
    "    for t in token.children:\n",
    "        if (t.pos_ in pos) and (t.i < token.i):\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_token_pos_right(token, pos):\n",
    "    if type(pos) == str:\n",
    "        pos = [pos]\n",
    "    for t in token.children:\n",
    "        if (t.pos_ in pos) and (t.i > token.i):\n",
    "            return t\n",
    "    return None\n",
    "    \n",
    "# Get token spcific dependency\n",
    "def get_token_dep(token, dep):\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if t.dep_ in dep:\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_token_dep_left(token, dep):\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if (t.dep_ in dep) and (t.i < token.i):\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_token_dep_right(token, dep):\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if (t.dep_ in dep) and (t.i > token.i):\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_all_token_dep(token, dep):\n",
    "    result = []\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if t.dep_ in dep:\n",
    "            result.append(t)\n",
    "    return result\n",
    "\n",
    "def get_all_token_dep_right(token, dep):\n",
    "    result = []\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if t.dep_ in dep and t.i > token.i:\n",
    "            result.append(t)\n",
    "    return result\n",
    "\n",
    "def get_all_token_dep_left(token, dep):\n",
    "    result = []\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if t.dep_ in dep and t.i < token.i:\n",
    "            result.append(t)\n",
    "    return result\n",
    "\n",
    "# Crawling all possibile conjunct\n",
    "def extract_conj(token, neglect=False, lemma=False):\n",
    "    result = []\n",
    "    current = get_token_dep(token, dep='conj')\n",
    "    while current:\n",
    "        if neglect:\n",
    "            neg = get_neglect(current)\n",
    "            # If lemma\n",
    "            if lemma:\n",
    "                text = (neg + ' ' + current.lemma_).strip()\n",
    "            else:\n",
    "                text = (neg + ' ' + current.text).strip()\n",
    "                    \n",
    "            result.append(text)\n",
    "        else:\n",
    "            result.append(current.text)\n",
    "        current = get_token_dep(current, dep='conj')\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_all_token_conj(token):\n",
    "    result = []\n",
    "    current = get_token_dep(token, dep='conj')\n",
    "    while current:\n",
    "        result.append(current)\n",
    "        current = get_token_dep(current, dep='conj')\n",
    "    return result\n",
    "\n",
    "# Get sentences that include coordinating conjunction and its conjunct\n",
    "def get_text_conj(token):\n",
    "    # Get all sentence of series include the conjugation\n",
    "    tokens = [token]\n",
    "    # Get all token\n",
    "    tokens += extract_conj(token, all_token=True)\n",
    "\n",
    "    text = ''\n",
    "    for i, t in enumerate(tokens):\n",
    "        text = text + t.text\n",
    "        if i < len(tokens) - 1:\n",
    "            if t.dep_ == 'cc':\n",
    "                text += ' '\n",
    "            else:\n",
    "                text += ', '\n",
    "\n",
    "    # text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Crawling all possibile pre modifier object\n",
    "def extract_pre_amod(token, lemma=False):\n",
    "    result = []\n",
    "    current_idx = token.i\n",
    "    for child in token.children:\n",
    "        if child.dep_ in ['amod', 'compound'] and child.i < current_idx:\n",
    "            if lemma:\n",
    "                result.append((child.lemma_, child.i))\n",
    "            else:\n",
    "                result.append((child.text, child.i))\n",
    "\n",
    "    # Sort by its index\n",
    "    result = sorted(result, key=lambda x: x[1])\n",
    "\n",
    "    # Return only list of string\n",
    "    result = [item[0] for item in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "# Crawling all possible post modifier object\n",
    "def extract_post_amod(token, lemma=False):\n",
    "    result = []\n",
    "    current_idx = token.i\n",
    "    for child in token.children:\n",
    "        if child.dep_ == 'amod' and child.i > current_idx:\n",
    "            if lemma:\n",
    "                result.append((child.lemma_, child.i))\n",
    "            else:\n",
    "                result.append((child.text, child.i))\n",
    "\n",
    "    # Sort by its index\n",
    "    result = sorted(result, key=lambda x: x[1])\n",
    "\n",
    "    # Return only list of string\n",
    "    result = [item[0] for item in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "# Crawling all possible adverb\n",
    "def extract_adv(token, lemma=True):\n",
    "    conjunctions = [\n",
    "    # Coordinating conjunctions\n",
    "    \"for\", \"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\",\n",
    "    \n",
    "    # Subordinating conjunctions\n",
    "    \"although\", \"because\", \"since\", \"if\", \"when\", \"while\", \"before\", \"after\", \"unless\", \"though\",\n",
    "    \n",
    "    # Correlative conjunctions (listed as single strings)\n",
    "    \"either\", \"neither\", \"both\", \"also\", \"whether\", \"as\",\n",
    "    \n",
    "    # Conjunctive adverbs\n",
    "    \"however\", \"therefore\", \"moreover\", \"consequently\", \"nevertheless\", \"thus\", \"furthermore\"\n",
    "    ]\n",
    "\n",
    "    result_pre = []\n",
    "    result_post = []\n",
    "    current_idx = token.i\n",
    "    for child in token.children:\n",
    "        # If pre-position adverb\n",
    "        if child.pos_ == 'ADV' and child.i < current_idx and child.lemma_.lower() not in conjunctions:\n",
    "            if lemma:\n",
    "                result_pre.append((child.lemma_, child.i))\n",
    "            else:\n",
    "                result_pre.append((child.text, child.i))\n",
    "\n",
    "        # If post-position adverb\n",
    "        if child.pos_ == 'ADV' and child.i > current_idx and child.lemma_.lower() not in conjunctions:\n",
    "            if lemma:\n",
    "                result_post.append((child.lemma_, child.i))\n",
    "            else:\n",
    "                result_post.append((child.text, child.i))\n",
    "\n",
    "    # Sort by its index\n",
    "    result_pre = sorted(result_pre, key=lambda x: x[1])\n",
    "    result_post = sorted(result_post, key=lambda x: x[1])\n",
    "\n",
    "    # Return only list of string\n",
    "    result_pre = [item[0] for item in result_pre]\n",
    "    result_post = [item[0] for item in result_post]\n",
    "\n",
    "    return result_pre, result_post\n",
    "\n",
    "# Crawling preposition phrase after particullar token\n",
    "def crawling_after_token_prep_phrase(token, neglect=False):\n",
    "    result = []\n",
    "    basis_idx = token.i\n",
    "    prep = get_all_token_dep(token, dep='prep')\n",
    "    if prep:\n",
    "        # If contain children: dep pcomp dep VERB pos tag; Until reach dobj or pobj\n",
    "        for p in prep:\n",
    "            prep_idx = p.i\n",
    "            # If the preposition on the left basis token index, continue\n",
    "            if basis_idx > prep_idx:\n",
    "                continue\n",
    "                \n",
    "            current = get_token_dep(p, dep=['pcomp', 'dobj', 'pobj'])\n",
    "            # Store objects\n",
    "            obj = []\n",
    "            # Store complement\n",
    "            comp = [p.text]\n",
    "            while current:\n",
    "                text = current.text\n",
    "                # If current token is object, get the pre-modifier adjective\n",
    "                if current.dep_ in ['dobj', 'pobj']:\n",
    "                    pre_adj = ' '.join(extract_pre_amod(current))\n",
    "                    obj += cross_product_str(pre_adj, text)\n",
    "\n",
    "                    # Extract conjunct object\n",
    "                    obj_conj = extract_conj(current, neglect=neglect)\n",
    "                    if len(obj_conj) > 0:\n",
    "                        obj += obj_conj\n",
    "                else:\n",
    "                    comp = cross_product_str(comp, text)\n",
    "                    \n",
    "                current = get_token_dep(current, dep=['pcomp', 'dobj', 'pobj'])\n",
    "\n",
    "            result += cross_product_str(comp, obj)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def get_sentence_location(mapper, position):\n",
    "    for s in mapper.keys():\n",
    "        interval = mapper[s]\n",
    "        if position >= interval[0] and position < interval[1]:\n",
    "            return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "0a6b5412-19a8-4275-86fa-34f09ad41ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coreference Resolution\n",
    "\n",
    "# Generate mapper pronouns-antecedents (subject only)\n",
    "def get_mapper_pron_ant(doc):\n",
    "    \n",
    "    def locate_subject_ant_pron(_doc):\n",
    "        # Locate potential antecedents and pronouns (subject only)\n",
    "    \n",
    "        # Define local variables\n",
    "        antecedents = []\n",
    "        pron = []\n",
    "        prohibit_pronouns = [ 'i', 'you', 'me', 'my', 'mine']\n",
    "    \n",
    "        # Get sentence mapper\n",
    "        sentence_points = {}\n",
    "        for i, s in enumerate(_doc.sents):\n",
    "            sentence_points[i] = (s.start, s.end)\n",
    "        \n",
    "        for token in _doc:\n",
    "            # Condition potential antecedents\n",
    "            # If the token is not pronouns and it's a subject\n",
    "            if (token.pos_ in ['NOUN', 'PROPN']) and (token.dep_ == 'nsubj'):\n",
    "                start = token.i\n",
    "                end = start + 1\n",
    "                location_sentence = get_sentence_location(sentence_points, start)\n",
    "                antecedents.append((token, start, location_sentence))\n",
    "                # Check is there any conj\n",
    "                # antecedents += extract_conj(token, only_token=True)\n",
    "        \n",
    "            # if (token.pos_ != 'PRON') and (token.dep_ == 'dobj' or token.dep_ == 'pobj'):\n",
    "            #     start = token.i\n",
    "            #     end = start + 1\n",
    "            #     location_sentence = get_sentence_location(sentence_points, start)\n",
    "            #     antecedents.append((token, start, location_sentence))\n",
    "            #     # Check is there any conj\n",
    "            #     # antecedents += extract_conj(token, only_token=True)    \n",
    "        \n",
    "            # Condition potential pronouns\n",
    "            # Rule 1\n",
    "            # If pron is subject (it could be same sentence or previously)\n",
    "            if (token.pos_ == 'PRON' and token.text.lower() not in prohibit_pronouns) and (token.dep_ == 'nsubj'):\n",
    "                start = token.i\n",
    "                end = start + 1\n",
    "                location_sentence = get_sentence_location(sentence_points, start)\n",
    "                pron.append((token, start, location_sentence))\n",
    "                \n",
    "            # Rule 2\n",
    "            # If pron is possesion (ant is subject in the same sentence)\n",
    "            if (token.pos_ == 'PRON' and token.text.lower() not in prohibit_pronouns) and (token.dep_ == 'poss'):\n",
    "                start = token.i\n",
    "                end = start + 1\n",
    "                location_sentence = get_sentence_location(sentence_points, start)\n",
    "                pron.append((token, start, location_sentence))\n",
    "        \n",
    "            # Rule 3\n",
    "            # If pron is object\n",
    "            # if (token.pos_ == 'PRON') and (token.dep_ == 'dobj' or token.dep_ == 'pobj'):\n",
    "            #     start = token.i\n",
    "            #     end = start + 1\n",
    "            #     location_sentence = get_sentence_location(sentence_points, start)\n",
    "            #     pron.append((token, start, location_sentence))\n",
    "        \n",
    "        \n",
    "        return (antecedents, pron)\n",
    "\n",
    "    # Filter sentence\n",
    "    def filter_sentence(_list, location):\n",
    "        temp = []\n",
    "        for e in _list:\n",
    "            if e[-1] == location:\n",
    "                temp.append(e)\n",
    "        return temp\n",
    "\n",
    "    # Define local variable\n",
    "    mapper = {}\n",
    "    result = None\n",
    "\n",
    "    antecedents, pronouns = locate_subject_ant_pron(doc)\n",
    "    \n",
    "    if len(pronouns) > 0:\n",
    "        for p in pronouns:\n",
    "            # Current status\n",
    "            is_success = False\n",
    "\n",
    "            # Get current text, index token, and location sentence token\n",
    "            token_pron, index_pron, sent_pron = p\n",
    "            current_sentence = sent_pron\n",
    "            \n",
    "            while current_sentence > -1:\n",
    "                # Get the antecedents\n",
    "                filter_antecedents = filter_sentence(antecedents, current_sentence)\n",
    "\n",
    "                # If the filter antecedents exist\n",
    "                if len(filter_antecedents) > 0:\n",
    "                    for ant in filter_antecedents:\n",
    "                        token_ant, index_ant, sent_ant = ant\n",
    "                        # If antecedent is subject and pronouns is subject or possession and antecedent on the left of pronoun\n",
    "                        if ('subj' in token_ant.dep_) and ('subj' in token_pron.dep_ or 'poss' in token_pron.dep_) and (index_ant < index_pron):\n",
    "                            mapper[index_pron] = index_ant\n",
    "                            is_success = True\n",
    "                            break\n",
    "                        # if ('obj' in token_ant.dep_ and 'obj' in token_pron.dep_) and (index_ant < index_pron):\n",
    "                        #     mapper[index_pron] = index_ant\n",
    "                        #     is_success = True\n",
    "                        #     break\n",
    "                \n",
    "                # If already success, break it.\n",
    "                if is_success:\n",
    "                    break\n",
    "                    \n",
    "                current_sentence -= 1\n",
    "\n",
    "    return mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "ae6cb69b-3794-4586-94af-e7d17835e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main aspect extraction\n",
    "\n",
    "# Extract all raw aspects\n",
    "def get_raw_aspects(doc):\n",
    "    \"\"\"\n",
    "        return: list of tuple, tuple: (aspect, start, end)\n",
    "    \"\"\"\n",
    "    # Define global variables\n",
    "    global bing_liu_opinion_words\n",
    "    \n",
    "    # Define local variables\n",
    "    storage = []\n",
    "\n",
    "    # Define helper function\n",
    "    def is_abnormal_noun(text):\n",
    "        \"\"\"\n",
    "            If text only contains special character/number/both OR total length less than 3 it specified as abnormal.\n",
    "        \"\"\"\n",
    "        if re.match(r'^[0-9\\W]+$', token.text) or len(token.text) < 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Going through all token\n",
    "    for idx, token in enumerate(doc):\n",
    "        # Make sure the text is not abnormal\n",
    "        if is_abnormal_noun(token.text):\n",
    "            continue\n",
    "\n",
    "        # If the word is noun and preceded by an adjective\n",
    "        if idx != 0 and (token.pos_ == 'NOUN' and doc[idx - 1].pos_ == 'ADJ'):\n",
    "            # If the adjective is an opinion\n",
    "            if doc[idx - 1].text not in bing_liu_opinion_words:\n",
    "                # Concatenate adj + word then add to storage\n",
    "                text = doc[idx - 1].text + ' ' + token.text\n",
    "                storage.append((text, idx - 1, idx + 1))\n",
    "            else:\n",
    "                # Else, add noun only\n",
    "                text = token.text\n",
    "                storage.append((text, idx, idx + 1))\n",
    "            continue\n",
    "            \n",
    "        # If the word is noun and preceded by another noun\n",
    "        if idx != 0 and (token.pos_ == 'NOUN' and doc[idx - 1].pos_ == 'NOUN'):\n",
    "            text = doc[idx - 1].text + ' ' + token.text\n",
    "            storage.append((text, idx - 1, idx + 1))\n",
    "            continue\n",
    "\n",
    "        # If the word is noun and direct object\n",
    "        if token.pos_ == 'NOUN' and (token.dep_ == 'dobj'):\n",
    "            text = token.text\n",
    "            storage.append((text, idx, idx + 1))\n",
    "            continue\n",
    "    \n",
    "        # If the word is noun and a subject of sentence\n",
    "        if token.pos_ == 'NOUN' and token.dep_ == 'nsubj':\n",
    "            text = token.text\n",
    "            storage.append((text, idx, idx + 1))\n",
    "            continue\n",
    "    \n",
    "        # If the word is noun and a conj of another noun\n",
    "        if (token.pos_ == 'NOUN' and token.dep_ == 'conj') and (token.head.pos_ == 'NOUN'):\n",
    "            text = token.text\n",
    "            storage.append((text, idx, idx + 1))\n",
    "            continue\n",
    "    \n",
    "        # # If the sentence contains SUBJECT VERB, then makes it true\n",
    "        # if token.dep_ == 'nsubj' and token.head.pos_ == 'VERB':\n",
    "        #     is_contain_subject_verb = True\n",
    "    \n",
    "        # # If token is word that contain pre-modifier\n",
    "        # if (token.dep_ == 'amod' and token.head.pos_ == 'NOUN'):\n",
    "        #     if token.head.i != idx + 1:\n",
    "        #         continue\n",
    "        #     text = token.text + ' ' + token.head.text\n",
    "        #     storage.append((text, idx, token.head.i + 1))\n",
    "    \n",
    "        # # If token is word that contain post-modifier\n",
    "        # if (token.dep_ == 'pobj' and token.pos_ == 'NOUN'):\n",
    "        #     if token.head.dep_ == 'prep' and token.head.head.pos_ == 'NOUN':\n",
    "        #         text = token.head.head.text + ' ' + token.head.text + ' ' + token.text\n",
    "        #         start = token.head.head.i\n",
    "        #         storage.append((text, start, idx + 1))\n",
    "            \n",
    "        \n",
    "        # If token is adverb modifier and its head is NOUN then store it.\n",
    "        if (token.dep_ == 'advmod' and token.head.pos_ == 'NOUN'):\n",
    "            text = token.head.text + ' ' + token.text\n",
    "            storage.append((text, token.head.i, idx + 1))\n",
    "            # adv_adj_mod.append((text, idx, idx + 1))\n",
    "\n",
    "    # Sort storage\n",
    "    storage = list(set(storage))\n",
    "    storage = sorted(storage, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "    return storage\n",
    "\n",
    "# Prunning raw aspect\n",
    "def prunning_aspect(list_, doc):\n",
    "    # Define local variables\n",
    "    drop_idx = []\n",
    "    storage = {}\n",
    "    \n",
    "    # Get sentence mapper\n",
    "    sentence_points = {}\n",
    "    for i, s in enumerate(doc.sents):\n",
    "        sentence_points[i] = (s.start, s.end)\n",
    "\n",
    "    for idx, item in enumerate(list_):\n",
    "        # As long as current idx does not more than maximum list_ index\n",
    "        if idx != len(list_) - 1:\n",
    "            # Get the next item\n",
    "            next_item = list_[idx + 1]\n",
    "            # If current item start position and next item end position are overlapping\n",
    "            if item[-1] - 1 == next_item[1]:\n",
    "                # We merge the text based on last text in current item and first text in next item\n",
    "                append_text = ' '.join(next_item[0].split()[1:])\n",
    "                # Update next item values\n",
    "                new_text = item[0] + ' ' + append_text\n",
    "                new_start = item[1]\n",
    "                new_end = next_item[-1]\n",
    "                list_[idx + 1] = (new_text, new_start, new_end)\n",
    "\n",
    "                # Add current index into dropped index list\n",
    "                drop_idx.append(idx)\n",
    "            \n",
    "            # If current item start position = next item end position (They are next to each other)\n",
    "            if item[-1] == next_item[1]:\n",
    "                # Update the next value (do not have to merge the text based on specific text).\n",
    "                new_text = item[0] + ' ' + next_item[0]\n",
    "                new_start = item[1]\n",
    "                new_end = next_item[-1]\n",
    "                list_[idx + 1] = (new_text, new_start, new_end)\n",
    "\n",
    "                # Add current index into dropped index list\n",
    "                drop_idx.append(idx)\n",
    "                \n",
    "    list_ = [list_[i] for i in range(len(list_)) if i not in drop_idx]\n",
    "\n",
    "    # Create return as aspect-list of sentence mapper\n",
    "    for i, s in enumerate(list_):\n",
    "        aspect, start, end = s\n",
    "        sentence_location = get_sentence_location(sentence_points, start)\n",
    "        sentence = list(doc.sents)[sentence_location].text\n",
    "        # Update value and store text as lowercase\n",
    "        # storage[sentence_location].append(aspect.lower())\n",
    "        if not storage.get(aspect.lower()):\n",
    "            storage[aspect.lower()] = [sentence]\n",
    "        else:\n",
    "            storage[aspect.lower()].append(sentence)\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed9b695-0a2c-4dbd-a017-dc774b9eba6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259fd811-0755-4a9b-9652-3dfe85fba8e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "2f42562f-6132-4aaf-878f-42847795364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Subject active rules (Conjunct Handling)\n",
    "\n",
    "\n",
    "##==================== CONJUNCT HANDLING ==============================##\n",
    "\n",
    "def ability_obj_conjunct(obj, base, set_rep=False):\n",
    "    #====== Conjunct Object =======#\n",
    "    result = []\n",
    "    reps =  [] # Representative conjunct storage\n",
    "    neg = ' '\n",
    "    conjuncts = get_all_token_conj(obj)\n",
    "    if len(conjuncts) > 0:\n",
    "        for conjunct in conjuncts:\n",
    "            # Get neglection object\n",
    "            pre_amod_token = get_token_dep_left(conjunct, dep='amod')\n",
    "            # If the neglection does not appear at front of object, it may refers to the most left pre modifier\n",
    "            neg = get_neglect(conjunct) or get_neglect(pre_amod_token)\n",
    "            # Get pre adjectvie modifier of conjunct\n",
    "            pre_adj = ' '.join(extract_pre_amod(conjunct))\n",
    "                \n",
    "            # Concatenate components (compliment) into: aux (optional) + not (optional) + adv (optional) \n",
    "            #                                             + verb + aux-comp (optional) + compliment + not (optional)\n",
    "            # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + not (optional)\n",
    "            # Concatenate components into: base + not (optional)\n",
    "            ability = cross_product_str(base, neg)\n",
    "            # Concatenate components (compliment) into: aux (optional) + not (optional) + adv (optional) \n",
    "            #                                             + verb + aux-comp (optional) + compliment + not (optional) + adj (optional)\n",
    "            # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + not (optional) + adj (optional)\n",
    "            # Concatenate components into: base + not (optional) + adj (optional)\n",
    "            ability = cross_product_str(ability, pre_adj)\n",
    "            # Concatenate components (compliment) into: aux (optional) + not (optional) + adv (optional) \n",
    "            #                                             + verb + aux-comp (optional) + compliment + not (optional) + adj (optional) + Conjunct object\n",
    "            # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + not (optional) + adj (optional) + Conjunct object\n",
    "            # Concatenate components into: base + not (optional) + adj (optional) + Conjunct object\n",
    "            ability = cross_product_str(ability, conjunct.text)\n",
    "        \n",
    "            # Add the ability into abilities\n",
    "            # IF with representative\n",
    "            if set_rep:\n",
    "                result += cross_product_tuple(ability, [(conjunct.lemma_,)])\n",
    "            else:\n",
    "                result += ability\n",
    "            # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb + adj (optional) + Conjunct object\n",
    "            # EXPECTED PATTERN (compliment) : aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional) + compliment \n",
    "            #                                   + adj (optional) + Conjunct object\n",
    "            # Note: Since normaly, If direct object is noun/propn/pron the conjuncts are noun/propn/pron too.\n",
    "            #        This rule follow this concept. In somehow, the conjunct could be adjective or another verb.\n",
    "    return result\n",
    "\n",
    "def ability_adj_conjunct(adj, base, set_rep=False):\n",
    "    #====== Conjunct Adjective =======#\n",
    "    result = []\n",
    "    conjuncts = get_all_token_conj(adj)\n",
    "    if len(conjuncts) > 0:\n",
    "        for conjunct in conjuncts:\n",
    "            # Get neglection adjective\n",
    "            neg = get_neglect(conjunct)\n",
    "    \n",
    "            # Concatenate components into: aux (optional) + not (optional) + adj\n",
    "            ability = cross_product_str(base, conjunct.text)\n",
    "    \n",
    "            # IF with representative\n",
    "            if set_rep:\n",
    "                result += cross_product_tuple(ability, [(conjunct.lemma_,)])\n",
    "            else:\n",
    "                result += ability\n",
    "    return result\n",
    "\n",
    "\n",
    "def ability_adv_conjunct(advmod, base, set_rep=False):\n",
    "    result = []\n",
    "    conjuncts = get_all_token_conj(advmod)\n",
    "    \n",
    "    if len(conjuncts) > 0:\n",
    "        for conjunct in conjuncts:\n",
    "            # Get pre adverb modifier\n",
    "            pre_adv = get_token_dep_left(conjunct, dep=['advmod', 'npadvmod'])\n",
    "            if pre_adv:\n",
    "                # Get pre and post adverb after pre adverb main verb\n",
    "                pre_advmod_temp, _ = extract_adv(pre_adv)\n",
    "                pre_advmod_temp = ' '.join(pre_advmod_temp)\n",
    "            \n",
    "                pre_adv = [pre_advmod_temp, pre_adv.text]\n",
    "                pre_adv = (' '.join(pre_adv)).strip()\n",
    "            else:\n",
    "                pre_adv = ' '\n",
    "                \n",
    "            # Get neglection adjective\n",
    "            neg = get_neglect(conjunct)\n",
    "            # Concatenate components into: base + not (optional)\n",
    "            ability = cross_product_str(base, neg)\n",
    "            # Concatenate components into: base + not (optional) + pre-adv (optional)\n",
    "            ability = cross_product_str(ability, pre_adv)\n",
    "            # Concatenate components into: base + not (optional) + pre-adv (optional) + adv\n",
    "            ability = cross_product_str(ability, conjunct.text)\n",
    "    \n",
    "            # IF with representative\n",
    "            if set_rep:\n",
    "                result += cross_product_tuple(ability, [(conjunct.lemma_,)])\n",
    "            else:\n",
    "                result += ability\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "05e7c81f-9263-43f8-a3d9-7e97cd508914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: subject active rules (Head is verb) components\n",
    "\n",
    "def get_components_verb(verb):\n",
    "    # Get main components after verb that generate sentences.\n",
    "    \n",
    "    # Get direct object verb token\n",
    "    obj = get_token_dep(verb, dep='dobj')\n",
    "    # Get post-modifier adverb token\n",
    "    advmod = get_token_dep_right(verb, dep=['advmod', 'npadvmod'])\n",
    "    # Get preposition after verb token\n",
    "    prep = get_token_dep(verb, dep='prep')\n",
    "    # Get adjectival compliment\n",
    "    acomp = get_token_dep(verb, dep='acomp')\n",
    "    \n",
    "    return {'obj': obj, 'advmod': advmod, 'prep': prep, 'acomp': acomp}\n",
    "\n",
    "def base_sentence(main_aux, main_verb, neg):\n",
    "    \n",
    "    if (main_verb) or (main_aux):\n",
    "        # Concatenate components into: not (optional)\n",
    "        ability = cross_product_str(neg, ' ')\n",
    "        if main_verb:\n",
    "            # Get pre advmod if exist\n",
    "            pre_adv = get_token_dep_left(main_verb, dep=['advmod', 'npadvmod'])\n",
    "            if pre_adv:\n",
    "                # Get pre and post adverb after pre adverb main verb\n",
    "                pre_advmod_temp, post_advmod_temp = extract_adv(pre_adv)\n",
    "                pre_advmod_temp = ' '.join(pre_advmod_temp)\n",
    "                post_advmod_temp = ' '.join(post_advmod_temp)\n",
    "                        \n",
    "                pre_adv = [pre_advmod_temp, pre_adv.text, post_advmod_temp]\n",
    "                pre_adv = (' '.join(pre_adv)).strip()\n",
    "            else:\n",
    "                pre_adv = ' '\n",
    "                \n",
    "            # Concatenate components into: not (optional) + adv (optional)\n",
    "            ability = cross_product_str(ability, pre_adv)\n",
    "            # Concatenate components into: not (optional) + adv (optional) + verb\n",
    "            ability = cross_product_str(ability, main_verb.text)\n",
    "        # If auxiliary token exist\n",
    "        if main_aux:\n",
    "            # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb\n",
    "            ability = cross_product_str(main_aux.text, ability)\n",
    "        return ability\n",
    "    return [' ']\n",
    "\n",
    "\n",
    "def ability_advmod(advmod, base):\n",
    "    result = []\n",
    "\n",
    "    pre_adv = get_token_dep_left(advmod, dep=['advmod', 'npadvmod'])\n",
    "    if pre_adv:\n",
    "        # Get pre and post adverb after pre adverb main verb\n",
    "        pre_advmod_temp, _ = extract_adv(pre_adv)\n",
    "        pre_advmod_temp = ' '.join(pre_advmod_temp)\n",
    "    \n",
    "        pre_adv = [pre_advmod_temp, pre_adv.text]\n",
    "        pre_adv = (' '.join(pre_adv)).strip()\n",
    "    else:\n",
    "        pre_adv = ' '\n",
    "    \n",
    "    # Concatenate components: pre-adv (optional) + adv\n",
    "    ability = cross_product_str(pre_adv, advmod.text)\n",
    "        \n",
    "    # Concatenate components into (if custom base exist): base + pre-adv (optional) + adv\n",
    "    ability = cross_product_str(base, ability)\n",
    "    \n",
    "    # Get preposition after adverb\n",
    "    prep_after_advmod = crawling_after_token_prep_phrase(advmod)\n",
    "    # If preposition after adverb exist\n",
    "    if prep_after_advmod:\n",
    "        # Concatenate components into (if custom base exist): base + pre-adv (optional) + adv + preposition phrase (optional)\n",
    "        ability = cross_product_str(ability, prep_after_advmod)\n",
    "\n",
    "    result += ability\n",
    "    \n",
    "    # Conjunct adverb handling\n",
    "    result += ability_adv_conjunct(advmod, base, set_rep=False)\n",
    "    \n",
    "    # EXPECTED PATTERN (if custom base exist): base + pre-adv (optional) + adv + preposition phrase (optional)\n",
    "    return result\n",
    "\n",
    "def ability_dobj(obj, base):\n",
    "    result = []\n",
    "\n",
    "    # Get neglection direct object\n",
    "    pre_amod_token = get_token_dep_left(obj, dep='amod')\n",
    "    # If the neglection does not appear at front of object, it may refers to the most left pre modifier\n",
    "    neg = get_neglect(obj) or get_neglect(pre_amod_token)\n",
    "    \n",
    "    # Get pre adjectvie modifier of object\n",
    "    pre_adj = ' '.join(extract_pre_amod(obj))\n",
    "    \n",
    "    # Concatenate components: adj (optional) + Direct object\n",
    "    ability = cross_product_str(pre_adj, obj.text)\n",
    "\n",
    "    # Concatenate components: not (optional) + adj (optional) + Direct object\n",
    "    ability = cross_product_str(neg, ability)\n",
    "\n",
    "    # Concatenate components into: base + not (optional) + adj (optional) + Direct object\n",
    "    ability = cross_product_str(base, ability)\n",
    "\n",
    "    # GET PREPOSITION AFTER OBJ and ADNOMINAL CLAUSE\n",
    "    prep = get_token_dep(obj, dep='prep')\n",
    "    acl = get_token_dep(obj, dep='acl')\n",
    "    if (prep) or (acl):\n",
    "        if prep:\n",
    "            # Get phrase: preposition + preposition-compliment (optional) + pre-adj (optional) + object\n",
    "\n",
    "            phrase = ability_prep(prep)        \n",
    "            # EXPECTED PATTERN (compliment): base + not (optional) + adj (optional) + Direct object + preposition + preposition-compliment (optional) \n",
    "            #                                  not (optional) + pre-adj (optional) + object\n",
    "            result += cross_product_str(ability, phrase)\n",
    "        if acl:\n",
    "            # Get aux acl\n",
    "            aux_acl = get_token_dep(acl, dep='aux')\n",
    "            # Neglection acl\n",
    "            neg = get_neglect(acl)\n",
    "            # Define base Adnominal Clause\n",
    "            # Concatenate components into (if custom base exist): base + not (optional) + adj (optional) + Direct object + not (optional)\n",
    "            temp = cross_product_str(ability, neg)\n",
    "            if aux_acl:\n",
    "                # Concatenate components into (if custom base exist): base + not (optional) + adj (optional) + Direct object + not (optional) + aux-acl\n",
    "                temp = cross_product_str(temp, aux_acl.text)\n",
    "                \n",
    "            # Concatenate components into (if custom base exist): base + not (optional) + adj (optional) + Direct object + not (optional) + aux-acl\n",
    "            #                                                       + acl\n",
    "            temp = cross_product_str(temp, acl.text)\n",
    "            # EXPECTED PATTERN: base + not (optional) + adj (optional) + Direct object + not (optional) + aux-acl + acl + all possible option\n",
    "            result += ability_adnominal_clause(acl=acl, base=temp)\n",
    "    else:\n",
    "        # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb + adj (optional) + not (optional) + Direct object\n",
    "        # EXPECTED PATTERN (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional)\n",
    "        #                                  + compliment + adj (optional) + not (optional) + Direct object\n",
    "        result += ability\n",
    "    \n",
    "    # Conjunct object handling\n",
    "    result += ability_obj_conjunct(obj, base=base)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def ability_prep(prep, base=None):\n",
    "    # Get object of preposition\n",
    "    obj = get_token_dep_right(prep, dep=['dobj', 'pobj'])\n",
    "    pcomp = None\n",
    "    # If object does not exist\n",
    "    if not obj:\n",
    "        # Get the preposition complement\n",
    "        pcomp = get_token_dep_right(prep, dep='pcomp')\n",
    "        if pcomp:\n",
    "            # Get the object that refers to preposition complement\n",
    "            obj = get_token_dep_right(pcomp, dep=['dobj', 'pobj'])\n",
    "    \n",
    "    # Concatenate components: preposition\n",
    "    ability = cross_product_str(prep.text, ' ')\n",
    "    if (pcomp) or (obj):\n",
    "        # If preposition compliment exist\n",
    "        if pcomp:\n",
    "            # Concatenate components: preposition + preposition-compliment (optional)\n",
    "            ability = cross_product_str(ability, pcomp.text)\n",
    "        # If object exist\n",
    "        if obj:\n",
    "            # Get neglection object\n",
    "            pre_amod_token = get_token_dep_left(obj, dep='amod')\n",
    "            # If the neglection does not appear at front of object, it may refers to the most left pre modifier\n",
    "            neg = get_neglect(obj) or get_neglect(pre_amod_token)\n",
    "            # Temporary storage\n",
    "            temp = []\n",
    "            # Get pre adjective modifier object\n",
    "            pre_adj = ' '.join(extract_pre_amod(obj))\n",
    "            # Concatenate components: preposition + preposition-compliment (optional) + not (optional)\n",
    "            temporary = cross_product_str(ability, neg)\n",
    "            # Concatenate components: preposition + preposition-compliment (optional) + not (optional) + pre-adj (optional)\n",
    "            temporary = cross_product_str(temporary, pre_adj)\n",
    "            # Concatenate components: preposition + preposition-compliment (optional) + not (optional) + pre-adj (optional) + object\n",
    "            temp += cross_product_str(temporary, obj.text)\n",
    "            \n",
    "            # Conjunct object handling\n",
    "            temp += ability_obj_conjunct(obj, base=ability)\n",
    "            \n",
    "            ability = temp\n",
    "\n",
    "        # If base is None, return phrase only\n",
    "        if not base:\n",
    "            # EXPECTED PATTERN: preposition + preposition-compliment (optional) + pre-adj (optional) + object\n",
    "            return ability\n",
    "        else:\n",
    "            # EXPECTED PATTERN: base + preposition + preposition-compliment (optional) + pre-adj (optional) + object\n",
    "            ability = cross_product_str(base, ability)\n",
    "            return ability\n",
    "            \n",
    "    return []\n",
    "\n",
    "\n",
    "def ability_acomp(acomp, base):\n",
    "    # Get neglection \n",
    "    neg = get_neglect(acomp)\n",
    "    # if not base:\n",
    "    #     if comp:\n",
    "    #         # Get base sentence: aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional) + compliment\n",
    "    #         base = base_sentence_comp(comp, **kwargs)  \n",
    "    #     else:\n",
    "    #         # Get base sentence: aux (optional) + not (optional) + adv (optional) + verb\n",
    "    #         base = base_sentence(main_aux=kwargs.get('main_aux'),\n",
    "    #                                 main_verb=kwargs.get('main_verb'),\n",
    "    #                                 neg=kwargs.get('neg'))\n",
    "\n",
    "    # Concatenate components into: not (optional) + acomp\n",
    "    ability = cross_product_str(neg, acomp.text)\n",
    "    # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + not (optional) + acomp\n",
    "    # Concatenate components into (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional) \n",
    "    #                                             + compliment + not (optional) + acomp\n",
    "    # Concatenate components into (if custom base exist): base + not (optional) + acomp\n",
    "    ability = cross_product_str(base, ability)                    \n",
    "    # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb + adv + preposition phrase (optional)\n",
    "    # EXPECTED PATTERN (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional)\n",
    "    #                                             + compliment + adv + preposition phrase (optional)\n",
    "    # EXPECTED PATTERN (if custom base exist): base + adv + preposition phrase (optional)\n",
    "    return ability\n",
    "\n",
    "\n",
    "def ability_adnominal_clause(acl, base):\n",
    "    # Define local variable\n",
    "    result = []\n",
    "    \n",
    "    # Get intransitive rate score\n",
    "    int_rate_acl = map_verb_intrans.get(acl.text) or map_verb_intrans.get(acl.lemma_)\n",
    "    # If the verb is not in the mapper ( we assume it is transitive verb )\n",
    "    if not int_rate_acl:\n",
    "        int_rate_acl = 0\n",
    "\n",
    "    \n",
    "    temp = ability_relative_verb(acl, base=base)\n",
    "    if len(temp) > 0:\n",
    "        # result += cross_product_tuple(temp, [tuple(reps)])\n",
    "        result += temp\n",
    "    else:\n",
    "        # If do not contain any of that, but intransitive verb ==> Subject + aux (optional) + not (optional) + adv (optional) + verb\n",
    "        if int_rate_acl > 0.5 and comp.lemma_.lower() not in ['be', 'do', 'have']:\n",
    "            ability = cross_product_str(base, aux_acl.text)\n",
    "            ability = cross_product_str(ability, acl.text)\n",
    "            # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb\n",
    "            result += ability\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9be3ac54-76a5-43b9-8c73-bea173f42a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Case Head is verb\n",
    "\n",
    "def ability_relative_verb(verb, base):\n",
    "    # Get ability that relative to particlar verb\n",
    "    result = []\n",
    "    comp = None\n",
    "\n",
    "        \n",
    "    # Extract all token\n",
    "    obj, advmod, prep, acomp = get_components_verb(verb).values()\n",
    "    if (advmod) or (prep) or (obj) or (acomp):\n",
    "        # If adverb after verb exist\n",
    "        if advmod:\n",
    "            ability = ability_advmod(advmod, base=base)\n",
    "            # EXPECTED PATTERN: Subject + aux (optional) + not (optional) + adv (optional) + verb + adv + prepositional phrase (optional)\n",
    "            result += ability\n",
    "    \n",
    "        # If prep after verb\n",
    "        if prep:\n",
    "            ability = ability_prep(prep, base=base)\n",
    "            # EXPECTED PATTERN: Subject + aux (optional) + not (optional) + adv (optional) + verb + preposition phrase\n",
    "            result += ability\n",
    "    \n",
    "        # If direct object exist\n",
    "        if obj:\n",
    "            ability = ability_dobj(obj, base=base)\n",
    "            result += ability\n",
    "    \n",
    "        # If adjective compliment exist\n",
    "        if acomp:\n",
    "            ability = ability_acomp(acomp, base=base)\n",
    "            result += ability\n",
    "    \n",
    "    return result\n",
    "\n",
    "def ability_verb(main_verb):\n",
    "    result = []\n",
    "    reps = [main_verb.lemma_] # Representative ability.\n",
    "    ###============ DEFINE VARIABLES ================###\n",
    "    # Get main auxiliary\n",
    "    aux = get_token_dep(main_verb, dep='aux')\n",
    "    # Get neglect; If there is no neglect, return empty text.\n",
    "    neg = get_neglect(main_verb)\n",
    "    # Get intransitive rate score\n",
    "    int_rate = map_verb_intrans.get(main_verb.text) or map_verb_intrans.get(main_verb.lemma_)\n",
    "    # If the verb is not in the mapper ( we assume it is transitive verb )\n",
    "    if not int_rate:\n",
    "        int_rate = 0\n",
    "\n",
    "    #==================== COMPLIMENT ========================#\n",
    "    # Get compliment verb\n",
    "    comp = get_token_dep(main_verb, dep=['xcomp', 'ccomp'])\n",
    "    if comp and (comp.pos_ not in ['VERB', 'AUX'] or get_token_dep(comp, dep='auxpass')):\n",
    "        comp = None\n",
    "        \n",
    "    # NOTE: a single verb to directly have both a ccomp and an xcomp dependency simultaneously \n",
    "    #         is rare and typically wouldn't occur. If a verb does have two clausal complements, \n",
    "    #         each clause would serve a different function or role in the sentence.\n",
    "    int_rate_comp = map_verb_intrans.get(main_verb.text) or map_verb_intrans.get(main_verb.lemma_)\n",
    "    if not int_rate_comp:\n",
    "        int_rate_comp = 0\n",
    "        \n",
    "    ###===================== CONDITION =====================###\n",
    "    components = {'main_aux': aux, 'main_verb': main_verb, 'neg': neg, }\n",
    "    base = base_sentence(**components)\n",
    "    \n",
    "    temp = ability_relative_verb(main_verb, base=base)\n",
    "    result += cross_product_tuple(temp, [tuple(reps)])\n",
    "    # result += ability_relative_verb(main_verb, **components)\n",
    "    if comp:\n",
    "        # Add compliment verb as representative\n",
    "        reps.append(comp.lemma_)\n",
    "        # Add auxiliary compliment into components\n",
    "        aux_comp = get_token_dep(comp, dep='aux')\n",
    "        # Update base\n",
    "        if aux_comp:\n",
    "            base = cross_product_str(base, aux_comp.text)\n",
    "        base = cross_product_str(base, comp.text)\n",
    "            \n",
    "        temp = ability_relative_verb(comp, base=base)\n",
    "        # temp = ability_relative_verb(comp, base=base, is_comp=True, **components)\n",
    "        if len(temp) > 0:\n",
    "            result += cross_product_tuple(temp, [tuple(reps)])\n",
    "        else:\n",
    "            # If do not contain any of that, but intransitive verb ==> Subject + aux (optional) + not (optional) + adv (optional) + verb\n",
    "            if int_rate_comp > 0.5 and comp.lemma_.lower() not in ['be', 'do', 'have']:\n",
    "                # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb\n",
    "                result += cross_product_tuple(base, [tuple(reps)])       \n",
    "\n",
    "    if len(result) == 0:\n",
    "        # If do not contain any of that, but intransitive verb ==> Subject + aux (optional) + not (optional) + adv (optional) + verb\n",
    "        if int_rate > 0.5 and main_verb.lemma_.lower() not in ['be', 'do', 'have']:\n",
    "            ability = base_sentence(**components)\n",
    "            # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb\n",
    "            result += cross_product_tuple(ability, [tuple(reps)])\n",
    "\n",
    "    # Labeling VERB\n",
    "    result = cross_product_flatten_append('VERB', result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "e3c891e8-53a4-40d2-a281-518ba9329622",
   "metadata": {},
   "outputs": [],
   "source": [
    "###============ AUXILIARY ================###\n",
    "\n",
    "def ability_aux(aux):\n",
    "    # Define local variables\n",
    "    result = []\n",
    "    # Get neglect of auxiliary\n",
    "    neg = get_neglect(aux)\n",
    "    components = {'main_aux': aux, 'neg': neg, 'main_verb': None}\n",
    "    base = base_sentence(**components)\n",
    "    \n",
    "    # Get the component tokens\n",
    "    \n",
    "    # Get adjective token\n",
    "    # NOTE: if 'AUX' is root, only have one adjective with dependency acomp.\n",
    "    adj = get_token_dep(aux, dep='acomp')\n",
    "    \n",
    "    # Get noun token\n",
    "    noun = get_token_pos_right(aux, pos=['NOUN', 'PROPN'])\n",
    "    if noun and noun.dep_ in ['nsubj', 'nsubjpass', 'csubj', 'csubjpass']:\n",
    "        noun = None\n",
    "        \n",
    "    # Get prepositinal token\n",
    "    prep = get_token_dep(aux, dep='prep')\n",
    "\n",
    "    ### CONDITIONAL TOKEN ###\n",
    "    if adj:\n",
    "        temp = ability_aux_adj(adj, base=base)\n",
    "        ability = cross_product_flatten_append('ADJ', temp)\n",
    "        # EXPECTED PATTERN: Subject + aux + not (optional) + adj\n",
    "        result += ability\n",
    "\n",
    "    if noun:\n",
    "        temp = ability_aux_noun(noun, base=base)\n",
    "        ability = cross_product_flatten_append('OTHER', temp)\n",
    "        # EXPECTED PATTERN: Subject + aux + not (optional) + pre-modifier adjective (optional) + noun\n",
    "        result += ability\n",
    "\n",
    "    if prep:\n",
    "        temp = ability_aux_prep(prep, base=base)\n",
    "        ability = cross_product_flatten_append('OTHER', temp)\n",
    "        # EXPECTED PATTERN: Subject + aux + not (optional) + phrase\n",
    "        result += ability\n",
    "\n",
    "    return result\n",
    "\n",
    "def ability_aux_adj(adj, base):\n",
    "    result = []\n",
    "    reps = [adj.lemma_] # Representative ability.\n",
    "\n",
    "    # GET PREPOSITION AFTER ADJ\n",
    "    prep = get_token_dep(adj, dep='prep')\n",
    "    # Concatenate components into: base + adj\n",
    "    ability = cross_product_str(base, adj.text)\n",
    "    if prep:\n",
    "        phrase = ability_prep(prep)\n",
    "        # Concatenate components into: base + adj + preposition phrase (optional)\n",
    "        ability = cross_product_str(ability, phrase)\n",
    "        \n",
    "    # Concatenate components into: base + adj + preposition phrase (optional)\n",
    "    result += cross_product_tuple(ability, [tuple(reps)])\n",
    "\n",
    "    # GET CONJUNCT\n",
    "    temp = ability_adj_conjunct(adj, base=base, set_rep=True)\n",
    "    if len(temp) > 0:\n",
    "        result += temp\n",
    "    return result\n",
    "\n",
    "def ability_aux_noun(noun, base):\n",
    "    result = []\n",
    "    reps = [noun.lemma_] # Representative ability\n",
    "    \n",
    "    \n",
    "    # Get pre-modifier adjective of noun\n",
    "    pre_adj = ' '.join(extract_pre_amod(noun))\n",
    "    # Concatenate components into: base + pre-modifier adjective (optional)\n",
    "    ability = cross_product_str(base, pre_adj)\n",
    "    # Concatenate components into: base + pre-modifier adjective (optional) + noun\n",
    "    ability = cross_product_str(base, noun.text)\n",
    "\n",
    "    result += cross_product_tuple(ability, [tuple(reps)])\n",
    "    return result\n",
    "\n",
    "def ability_aux_prep(prep, base):\n",
    "    result = []\n",
    "    \n",
    "    # Get preposition phrase\n",
    "    phrase = ability_prep(prep)\n",
    "    \n",
    "    # Conncatenate components into: base + Prepositional phrase\n",
    "    ability = cross_product_str(base, phrase)\n",
    "    # NOTE: It included preposition object conjuncts.\n",
    "\n",
    "    # Extract representative word\n",
    "    for a in ability:\n",
    "        temp = a.split()\n",
    "        temp = cross_product_tuple(a, [(temp[-1],)])\n",
    "        result += temp\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "e5ea923f-20e3-4128-82f1-2fa3eff66f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ability_pass_agent(agent, base):\n",
    "    result = []\n",
    "    # Get object agent\n",
    "    obj_agent = get_token_dep(agent, dep=['pobj', 'dobj'])\n",
    "\n",
    "    if obj_agent:   \n",
    "        # Update Base\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + agent\n",
    "        base = cross_product_str(base, agent.text)\n",
    "\n",
    "        # Get neglection object\n",
    "        pre_amod_token = get_token_dep_left(obj_agent, dep='amod')\n",
    "        # If the neglection does not appear at front of object, it may refers to the most left pre modifier\n",
    "        neg = get_neglect(obj_agent) or get_neglect(pre_amod_token)\n",
    "        # Get pre adjectvie modifier of conjunct\n",
    "        pre_adj = ' '.join(extract_pre_amod(obj_agent))\n",
    "\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + agent + adj (optional)\n",
    "        ability = cross_product_str(base, pre_adj)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + agent + adj (optional) + object\n",
    "        ability = cross_product_str(base, obj_agent.text)\n",
    "        # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + agent + adj (optional) + object\n",
    "        result += ability\n",
    "\n",
    "        # Handling object conjuncts\n",
    "        result += ability_obj_conjunct(obj_agent, base=base, set_rep=False)\n",
    "        \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "4731a9da-4f41-4c16-a4fb-13221e843618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Helper subject active rules\n",
    "\n",
    "def is_contain_question(token):\n",
    "    questions = ['what', 'who', 'why', 'whom', 'when', 'which', 'where', 'whose', 'how']\n",
    "    tokens = get_all_token_dep(token, dep=['advmod', 'attr'])\n",
    "    for t in tokens:\n",
    "        if t.text.lower() in questions:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_perfect_sentence(sent):\n",
    "    for token in sent:\n",
    "        if token.dep_ in ['nsubj', 'nsubjpass', 'csubj', 'csubjpass']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def imperfect_sentence_rules(token):\n",
    "    properties = []\n",
    "    result = []\n",
    "    if token.head.text == token.text:\n",
    "        # Get compound or amod\n",
    "        properties = get_all_token_dep_left(token, dep=['compound', 'amod'])\n",
    "    \n",
    "    if len(properties) > 0:\n",
    "        for p in properties:\n",
    "            temp = cross_product_str('be', p.text.lower())\n",
    "            if p.pos_ == 'ADJ':\n",
    "                label = 'ADJ'\n",
    "            else:\n",
    "                label = 'OTHER'\n",
    "            temp = cross_product_tuple(temp, [(p.lemma_,)])\n",
    "            result += cross_product_flatten_append(label, temp)\n",
    "            # result += cross_product_tuple(temp, [(p.lemma_,)])\n",
    "        # properties = cross_product_tuple(temp, [tuple(properties)])\n",
    "        return result\n",
    "        \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "6b09456f-f615-44f8-bea0-505fe07d7131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_active_rules(token, subject):\n",
    "    abilities = []\n",
    "    # Go to its head\n",
    "    head = token.head\n",
    "\n",
    "    # If head is Verb and it is root\n",
    "    # if (head.pos_ == 'VERB') and ((head.head.text == head.text) or head.dep_ == 'conj'):\n",
    "    if (head.pos_ == 'VERB'):\n",
    "        any_question = is_contain_question(head)\n",
    "        if any_question:\n",
    "            return abilities\n",
    "        \n",
    "        elif (head.head.text == head.text):\n",
    "            verb_conjunct = [head]\n",
    "    \n",
    "            verb_conjunct += get_all_token_conj(head)\n",
    "            ###============ GET ALL TOKENS ================###\n",
    "            for verb in verb_conjunct:\n",
    "                compare = get_token_dep(verb, dep=['nsubj', 'nsubjpass'])\n",
    "                if not compare or (compare.text == subject.text):\n",
    "                    abilities += ability_verb(verb)\n",
    "\n",
    "        elif (head.pos_ == 'VERB') and (head.dep_ in ['conj', 'advcl']):\n",
    "            compare = get_token_dep(head, dep=['nsubj'])\n",
    "            if compare and (subject.text == compare.text):\n",
    "                verb_conjunct = [head]\n",
    "                if head.dep_ == 'advcl':\n",
    "                    verb_conjunct += get_all_token_conj(head)\n",
    "                for verb in verb_conjunct:\n",
    "                    abilities += ability_verb(verb)\n",
    "        \n",
    "    # If head is aux\n",
    "    elif head.pos_ == 'AUX':\n",
    "        abilities += ability_aux(head)           \n",
    "    return abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7db8d92-2fb6-4b4e-b6b7-ddcd95358cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "81ca2f51-4475-42ca-a153-9671f52eff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_sentence_passive(main_auxpass, main_verb, main_aux, neg):\n",
    "    # Get pre advmod if exist\n",
    "    pre_adv = get_token_dep_left(main_verb, dep=['advmod', 'npadvmod'])\n",
    "    if pre_adv:\n",
    "        # Get pre and post adverb after pre adverb main verb\n",
    "        pre_advmod_temp, post_advmod_temp = extract_adv(pre_adv)\n",
    "        pre_advmod_temp = ' '.join(pre_advmod_temp)\n",
    "        post_advmod_temp = ' '.join(post_advmod_temp)\n",
    "                        \n",
    "        pre_adv = [pre_advmod_temp, pre_adv.text, post_advmod_temp]\n",
    "        pre_adv = (' '.join(pre_adv)).strip()\n",
    "    else:\n",
    "        pre_adv = ' '\n",
    "                    \n",
    "    # If aux exist\n",
    "    if main_aux:\n",
    "        # Concatenate components: aux (optional) + neg (optional)\n",
    "        ability = cross_product_str(main_aux.text, neg)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "        ability = cross_product_str(ability, main_auxpass.text)\n",
    "    else:\n",
    "        # Concatenate components: auxpass + neg (optional)\n",
    "        ability = cross_product_str(main_auxpass.text, neg)\n",
    "\n",
    "    # Concatenate components: aux (optional) + neg (optional) + auxpass + adv (optional)\n",
    "    ability = cross_product_str(ability, pre_adv)\n",
    "    \n",
    "    # Concatenate components: aux (optional) + neg (optional) + auxpass + adv (optional) + verb\n",
    "    ability = cross_product_str(ability, main_verb.text)\n",
    "\n",
    "    return ability    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "b39907a7-a551-491d-bd93-ad0b64a4fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_passive_rules(token):\n",
    "    abilities = []\n",
    "    result = []\n",
    "    reps = [token.head.lemma_]\n",
    "    # Get the token head (verb). Since passive form at least form: Subject + auxpass + verb \n",
    "    head = token.head\n",
    "    if head.pos_ != 'VERB':\n",
    "        return []\n",
    "\n",
    "    ##================= GET ALL POSSIBLE COMPONENTS ===============##\n",
    "    # 1. Get Possible Tokens (on Left Side) #\n",
    "    # Get neglect; If there is no neglect, return empty text.\n",
    "    neg = get_neglect(head)\n",
    "\n",
    "    # Get aux token\n",
    "    aux = get_token_dep(head, dep='aux')\n",
    "    # Get auxpass token\n",
    "    auxpass = get_token_dep(head, dep='auxpass')\n",
    "    if not auxpass:\n",
    "        # Since passive sentence must have auxpass in its component.\n",
    "        return []\n",
    "\n",
    "    # 2. Get Possible Tokens (on Right Side) #\n",
    "    # Get the agent token\n",
    "    agent = get_token_dep_right(head, dep='agent')\n",
    "    \n",
    "    # Get advmod after verb token\n",
    "    advmod = get_token_dep_right(head, dep=['advmod', 'npadvmod'])\n",
    "\n",
    "    # Get prepositional phrase\n",
    "    prep = get_token_dep(head, dep='prep')\n",
    "\n",
    "    # Get xcomp token\n",
    "    comp = get_token_dep(head, dep=['xcomp', 'ccomp'])\n",
    "\n",
    "    ##================= STORING ABILITIES ===============##    \n",
    "    components = {'main_auxpass': auxpass , 'main_verb': head, 'main_aux': aux, 'neg': neg}\n",
    "    base = base_sentence_passive(main_auxpass=components.get('main_auxpass'), \n",
    "                                 main_verb=components.get('main_verb'), \n",
    "                                 main_aux=components.get('main_aux'), \n",
    "                                 neg=components.get('neg'))\n",
    "    \n",
    "    # Store ability: If adverb modifier exist\n",
    "    if advmod:\n",
    "        # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + object\n",
    "        abilities += ability_advmod(advmod, base=base)\n",
    "\n",
    "    # Store ability: If agent and object agent token exist\n",
    "    if agent:\n",
    "        # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + object\n",
    "        abilities += ability_pass_agent(agent, base=base)\n",
    "\n",
    "    # Store ability: If preposition after verb exist\n",
    "    if prep:\n",
    "        # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + preposition phrase\n",
    "        abilities += ability_prep(prep, base=base)\n",
    "\n",
    "    # Store into result storage\n",
    "    if len(abilities) > 0:\n",
    "        result += cross_product_tuple(abilities, [tuple(reps)])\n",
    "\n",
    "    # Store ability: If xcomp exist\n",
    "    if comp:\n",
    "        # Add compliment verb as representative\n",
    "        reps.append(comp.lemma_)\n",
    "        # Add auxiliary compliment into components\n",
    "        aux_comp = get_token_dep(comp, dep='aux')\n",
    "        # Update base\n",
    "        if aux_comp:\n",
    "            base = cross_product_str(base, aux_comp.text)\n",
    "        base = cross_product_str(base, comp.text)\n",
    "            \n",
    "        temp = ability_relative_verb(comp, base=base)\n",
    "        if len(temp) > 0:\n",
    "            result += cross_product_tuple(temp, [tuple(reps)])\n",
    "        else:\n",
    "            # If do not contain any of that, but intransitive verb ==> Subject + aux (optional) + not (optional) + adv (optional) + verb\n",
    "            if int_rate_comp > 0.5 and comp.lemma_.lower() not in ['be', 'do', 'have']:\n",
    "                # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb\n",
    "                result += cross_product_tuple(base, [tuple(reps)]) \n",
    "\n",
    "    # Labeling VERB\n",
    "    result = cross_product_flatten_append('VERB', result)\n",
    "    # return abilities\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da8673d-a86f-4fab-98ed-f0536bea3543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1433c-3784-41c5-b769-ad8537d68cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37da49-3665-427d-8d05-e370ffba0388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e75d90-194d-49e8-a7de-61399fe12029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816a70f-64cd-45da-9b91-7e6febfc0149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36781dc0-57cf-4e83-8522-a940228555a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "4e8afc4b-5808-42a7-b556-efb2acc1ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_abilities(doc, ant_first_pron='the user'):\n",
    "    # Define local variable.\n",
    "    storage = {}\n",
    "    first_person_pronouns = [ 'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
    "    pronouns = [\n",
    "    \"he\", \"she\", \"they\", \"it\", # Personal Pronouns (Subjective)\n",
    "    \"him\", \"her\", \"them\", \"it\", \"you\",  # Personal Pronouns (Objective)\n",
    "    \"his\", \"hers\", \"theirs\", \"its\", \"mine\", \"yours\", \"ours\",  # Possessive Pronouns\n",
    "    \"her\", \"their\", \"its\",  # Possessive Adjectives\n",
    "    \"himself\", \"herself\", \"themself\", \"themselves\", \"Itself\",  # Reflexive Pronouns,\n",
    "    \"this\", \"that\", \"these\", \"those\", # Demonstrative Pronouns\n",
    "    \"who\", \"whom\", \"whose\", \"which\", \"that\"  # Relative Pronouns\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Get sentence mapper, prepare storage, and type of sentence\n",
    "    sentence_points = {}\n",
    "    type_sentence = {}\n",
    "    for i, s in enumerate(doc.sents):\n",
    "        sentence_points[i] = (s.start, s.end)\n",
    "        storage[i] = []\n",
    "        \n",
    "        if is_perfect_sentence(s):\n",
    "            type_sentence[i] = 'perfect'\n",
    "        else:\n",
    "            type_sentence[i] = 'imperfect'\n",
    "\n",
    "    # Get mapper pronoun and antecedents\n",
    "    mapper_pron_ant = get_mapper_pron_ant(doc)\n",
    "\n",
    "    # Define local variable.\n",
    "    result = []\n",
    "    for idx, token in enumerate(doc):\n",
    "        abilities = []\n",
    "\n",
    "        ## ==================== SUBJECT ACTIVE SENTENCE =========================== ##\n",
    "        # If token is subject (should be nsubj and nsubjpass). This time only nsubj\n",
    "        # In case active sentence form\n",
    "        if token.dep_ == 'nsubj':\n",
    "            abilities += subject_active_rules(token, subject=token)\n",
    "\n",
    "        ## ==================== SUBJECT PASSIVE SENTENCE =========================== ##\n",
    "        # If sentence is passive form.\n",
    "        if token.dep_ == 'nsubjpass':\n",
    "            abilities += subject_passive_rules(token)\n",
    "\n",
    "        ## ==================== IF SENTENCE IS IMPERFECT ========================== ##\n",
    "        sentence_location = get_sentence_location(sentence_points, idx)\n",
    "        if (type_sentence[sentence_location] == 'imperfect') and token.pos_ in ['NOUN', 'PROPN', 'PRON']:\n",
    "            abilities += imperfect_sentence_rules(token)\n",
    "            \n",
    "                \n",
    "        # Store final result\n",
    "        if len(abilities) > 0:\n",
    "            # Subject handling\n",
    "            subject = token.lemma_\n",
    "            # subjects = [token] + get_all_token_conj(token)\n",
    "            # for subject in subjects:\n",
    "            #     # Get posession\n",
    "            #     temp = get_token_dep_left(token, dep='poss')\n",
    "            #     if temp:\n",
    "            #         subjects = cross_product_str((temp.text + \"'s\") if temp.pos_ == 'PROPN' else temp.text, subject.text)\n",
    "                \n",
    "            # current_idx = token.i\n",
    "            # If the subject is pronouns and first person pronouns\n",
    "            # if token.pos_ == 'PRON' and token.text.lower() in first_person_pronouns:\n",
    "            #     subject = ant_first_pron\n",
    "            # # If subject is pronouns and its token location in mapper_pron_ant\n",
    "            # elif token.pos_ == 'PRON' and idx in mapper_pron_ant.keys():\n",
    "            #     # Get the antecedent index location\n",
    "            #     idx_map = mapper_pron_ant[idx]\n",
    "            #     # Change current token subject\n",
    "            #     token = doc[idx_map]\n",
    "            #     subject = token.lemma_\n",
    "            # # If the current child is pronoun (but not in mapper_pron_ant keys)\n",
    "            # elif token.pos_ == 'PRON' and token.text.lower() in pronouns:\n",
    "            #     continue\n",
    "            # # If token only contains special characters or numbers, or length text less than 3 (NOT PRONOUNS)\n",
    "            # elif (re.match(r'^[0-9\\W]+$', token.text)) or (len(token.text) < 3):\n",
    "            #     continue\n",
    "                \n",
    "            # Get all conj subject + current subject\n",
    "            subjects = [subject] + extract_conj(token)\n",
    "            # # Store result\n",
    "            # result += cross_product_tuple(subjects, abilities)\n",
    "            # Storage final result\n",
    "            sentence_location = get_sentence_location(sentence_points, idx)\n",
    "            # storage[sentence_location] += cross_product_tuple(subjects, abilities)\n",
    "\n",
    "            # THIS IS NEW OUTPUT STORAGE SHOULD BE: UNCOMMENT AFTER SUBJECT PASSIVE AND INPERFECT RULES ALREADY ADJUSTED\n",
    "            storage[sentence_location] += cross_product_flatten(subjects, abilities)\n",
    "\n",
    "    # Storing final result\n",
    "    # Make storage unique only\n",
    "    if storage:\n",
    "        for key, value in storage.items():\n",
    "            storage[key] = list(set(value))\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b38bee4-4bcb-4df6-a22d-905e5040e5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ea2505-8cf2-40a5-913e-3a849255651c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155855d-5909-4ee1-ad40-e16d4d2bde02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae028897-7f68-41e7-8974-e0aa82cc989a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e40287-7ca6-4073-8ee8-a41f841326ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01971f72-589c-4528-856a-d9d5fa263725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ab0b4-9a7c-4339-8a98-cb87eb8d69d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b96cf-4d13-4dc2-b58d-88ac47ef2406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "3144e136-d10f-4c01-9e0d-53b24ee97aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "620adcac-39b8-4f09-b33b-8888a1e64313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bing Liu's opinion word dictionary\n",
    "bing_liu_opinion_words = set()  # Add the actual list of opinion words here\n",
    "\n",
    "# Function to load opinion words from Bing Liu lexicon\n",
    "def load_opinion_words(filepath):\n",
    "    global bing_liu_opinion_words\n",
    "    temp = pd.read_table(filepath, comment=';', header=None)[0].to_list()\n",
    "    bing_liu_opinion_words = bing_liu_opinion_words.union(set(temp))\n",
    "\n",
    "\n",
    "# Load opinion words\n",
    "current_dir = os.getcwd()\n",
    "load_opinion_words(os.path.join(current_dir, 'util/opinion-lexicon-English/negative-words.txt'))\n",
    "load_opinion_words(os.path.join(current_dir, 'util/opinion-lexicon-English/positive-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "856a7bc6-0472-4c9b-95fe-d5d7d53e187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intransitive rate verb\n",
    "\n",
    "data_verb = pd.read_csv('verb_transitivity.tsv', sep='\\t')\n",
    "\n",
    "map_verb_intrans = data_verb[['verb', 'percent_intrans']].set_index('verb').to_dict()['percent_intrans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "eb91066e-c17f-446e-9cdb-deb92bdf650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nlp model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "0c42b932-345d-4f3f-a766-0dff46278928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  99 non-null     object\n",
      "dtypes: object(1)\n",
      "memory usage: 924.0+ bytes\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The staff were incredibly helpful and patient,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I had a great experience purchasing my phone h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Their selection of phones is amazing, and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I appreciate how the staff walked me through s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great customer service, I left with the phone ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  The staff were incredibly helpful and patient,...\n",
       "1  I had a great experience purchasing my phone h...\n",
       "2  Their selection of phones is amazing, and the ...\n",
       "3  I appreciate how the staff walked me through s...\n",
       "4  Great customer service, I left with the phone ..."
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "positive_reviews = [\n",
    "    \"The staff were incredibly helpful and patient, helping me find the perfect phone!\",\n",
    "    \"I had a great experience purchasing my phone here, the process was smooth and quick.\",\n",
    "    \"Their selection of phones is amazing, and the prices are very competitive!\",\n",
    "    \"I appreciate how the staff walked me through setting up my new device.\",\n",
    "    \"Great customer service, I left with the phone I wanted and all my questions answered.\",\n",
    "    \"They offer amazing deals on phones, I couldnt resist upgrading.\",\n",
    "    \"The technician fixed my phones issue faster than I expected. Highly recommend!\",\n",
    "    \"Fantastic experience, the staff really know their stuff!\",\n",
    "    \"I found the perfect phone case here, and the variety was impressive.\",\n",
    "    \"Upgrading my phone was a breeze thanks to their professional service.\",\n",
    "    \"Staff was knowledgeable and made sure I knew everything about my new phone.\",\n",
    "    \"Prices were reasonable and the staff very courteous!\",\n",
    "    \"Very happy with my purchase, the staff really went the extra mile.\",\n",
    "    \"Excellent service! They helped me find exactly what I was looking for.\",\n",
    "    \"Great deals on accessories, and the staff was super friendly!\",\n",
    "    \"I love this store! Always a smooth experience buying or fixing my phone.\",\n",
    "    \"I got a really good trade-in deal on my old phone.\",\n",
    "    \"Their repair services are quick and reliable.\",\n",
    "    \"The staff was extremely helpful in setting up my phone and transferring all my data.\",\n",
    "    \"Very professional and friendly service, Im super satisfied!\",\n",
    "    \"Great variety of phones, and the staff was very patient with my questions.\",\n",
    "    \"The process was super simple, and Im thrilled with my new phone.\",\n",
    "    \"They helped me choose a phone within my budget, which I really appreciated.\",\n",
    "    \"My phone was fixed in less than 30 minutes, such fast service!\",\n",
    "    \"Im a loyal customer because their customer service is always outstanding.\",\n",
    "    \"Best phone store in town, hands down!\",\n",
    "    \"The staff made sure I was completely comfortable with my purchase.\",\n",
    "    \"I found exactly what I needed, and they helped me get a great deal.\",\n",
    "    \"This store has a fantastic warranty service!\",\n",
    "    \"The staff was very informative, I learned a lot about phone features I didnt know about.\",\n",
    "    \"Excellent store for buying phone accessories, so much variety!\",\n",
    "    \"The phone I bought here is working perfectly, couldnt be happier.\",\n",
    "    \"They were super quick in setting up my phone, I was out of there in no time.\",\n",
    "    \"Always come here for upgrades, they never disappoint!\",\n",
    "    \"The store layout is easy to navigate and staff are always ready to help.\",\n",
    "    \"Best pricing for phone plans, they helped me save a lot!\",\n",
    "    \"Ive been to many phone stores, but this one by far provides the best service.\",\n",
    "    \"Customer service here is top-notch, they always resolve my issues quickly.\",\n",
    "    \"I always recommend this store to friends and family, they never fail to impress.\",\n",
    "    \"The staff took the time to show me all my options, no pressure sales.\",\n",
    "    \"Amazing place to buy the latest phones at great prices!\",\n",
    "    \"Their warranty plan is worth every penny, such a relief!\",\n",
    "    \"I appreciate how they were able to fix my phone on the same day.\",\n",
    "    \"Got a great deal on my new phone and an awesome case as well!\",\n",
    "    \"The staff was very accommodating when I had questions about phone features.\",\n",
    "    \"I had a great experience with their trade-in program.\",\n",
    "    \"Service was quick and efficient, I was in and out within 15 minutes!\",\n",
    "    \"They even helped me transfer all my contacts and data without extra charge.\",\n",
    "    \"My phone has been working flawlessly since I bought it from here.\",\n",
    "    \"They fixed my screen perfectly and even gave me a discount on the repair.\",\n",
    "    \"This is my go-to store for any phone issues, always reliable.\",\n",
    "    \"They offer fantastic promotions and discounts!\",\n",
    "    \"Great phone selection and even better customer service.\",\n",
    "    \"They resolved my issue very quickly and professionally.\",\n",
    "    \"I love how organized the store is and how fast they attend to customers.\",\n",
    "    \"Highly recommend this store if youre looking for good deals on phones!\",\n",
    "    \"I always leave this store feeling like I made the right purchase.\",\n",
    "    \"I received excellent advice from the sales team, they really know their products.\",\n",
    "    \"Very happy with the repair service here, my phone looks brand new!\"\n",
    "]\n",
    "\n",
    "\n",
    "negative_reviews = [\n",
    "    \"I had to wait over an hour to be helped, and the staff wasnt apologetic at all.\",\n",
    "    \"Bought a phone here that stopped working within a week, very disappointing.\",\n",
    "    \"Their prices are too high, and the selection is limited.\",\n",
    "    \"Customer service is poor, no one seemed interested in helping me.\",\n",
    "    \"I had a terrible experience, the phone they sold me was defective.\",\n",
    "    \"The staff was rude and unhelpful, Im never coming back.\",\n",
    "    \"They charged me extra for services I didnt need, felt like a scam.\",\n",
    "    \"Phone repairs took way too long, I had to come back multiple times.\",\n",
    "    \"I bought a phone, but they didnt inform me of all the hidden fees.\",\n",
    "    \"Staff seemed untrained and gave me incorrect information about the phone plan.\",\n",
    "    \"Their warranty is useless, they refused to fix my phone under it.\",\n",
    "    \"I had to return a faulty phone twice before they finally gave me a refund.\",\n",
    "    \"Very disorganized, I waited forever just to get a simple issue resolved.\",\n",
    "    \"The phone I purchased here was overpriced compared to other stores.\",\n",
    "    \"They refused to honor the promotion I came in for, very misleading.\",\n",
    "    \"I felt pressured to buy accessories I didnt need.\",\n",
    "    \"The repair was done poorly, and my phone broke again within a week.\",\n",
    "    \"Customer service was extremely slow, they need to hire more staff.\",\n",
    "    \"They didnt even check if my phone was working after the repair.\",\n",
    "    \"Terrible experience, my phone still has the same issue after getting it 'fixed'.\",\n",
    "    \"They upsold me on a phone plan I didnt need, very deceptive.\",\n",
    "    \"The staff was unprofessional and seemed like they didnt want to be there.\",\n",
    "    \"Their return policy is awful, I couldnt exchange my phone despite its defects.\",\n",
    "    \"They didnt apply the discount I was promised.\",\n",
    "    \"The store was messy and understaffed.\",\n",
    "    \"My phone broke down just after the warranty expired, very frustrating.\",\n",
    "    \"They kept trying to sell me more expensive phones when I clearly stated my budget.\",\n",
    "    \"The repair job was incomplete, and they refused to refund me.\",\n",
    "    \"Their customer service representatives were extremely rude on the phone.\",\n",
    "    \"I had to call multiple times just to get a response, very unprofessional.\",\n",
    "    \"They didnt explain anything clearly and rushed me through the purchase.\",\n",
    "    \"I regret buying from here, their post-purchase support is non-existent.\",\n",
    "    \"Phone stopped working just outside the return window, terrible quality.\",\n",
    "    \"The store was chaotic, with long lines and unhelpful staff.\",\n",
    "    \"They didnt even have the phone I wanted in stock after promising me it was available.\",\n",
    "    \"Terrible follow-up, they lost my repair order, and I had to start over.\",\n",
    "    \"I felt overcharged for a simple screen repair.\",\n",
    "    \"Bought a refurbished phone that had several issues they didnt disclose.\",\n",
    "    \"The technician damaged my phone during the repair, and they didnt take responsibility.\",\n",
    "    \"Im extremely disappointed, will not be coming back here again.\"\n",
    "]\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'review': positive_reviews + negative_reviews})\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "fcbd1c83-8c4e-4279-9868-f85a7870afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_words(corpus, thres_tfidf=75, thres_idf=25):\n",
    "    # Define local variables\n",
    "    storage_idf = set()\n",
    "    # storage_tfidf = set()\n",
    "    storage_tfidf = {}\n",
    "\n",
    "    # Define the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Preprocessing text\n",
    "    def preprocessing(text):\n",
    "        text = remove_extra_spaces(text)\n",
    "        text = expand_contractions(text)\n",
    "        text = remove_non_ascii(text)\n",
    "\n",
    "        # Get token of words\n",
    "        doc = nlp(text)\n",
    "        result = []\n",
    "        for token in doc:\n",
    "            t = token.lemma_.lower()\n",
    "\n",
    "            # If only contains special characters or numbers and length less than 3\n",
    "            if re.match(r'^[0-9\\W]+$', t) or len(t) < 3 or t in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                result.append(t)\n",
    "        return result\n",
    "\n",
    "    ##========= GENERATE MODEL =========##\n",
    "    # Create texts\n",
    "    texts = [preprocessing(document) for document in corpus]\n",
    "\n",
    "    # Create dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    # Convert documents into Bag-of-words format\n",
    "    corpus_bow = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Train the TF-IDF model\n",
    "    tfidf_model = gensim.models.TfidfModel(corpus_bow)\n",
    "\n",
    "    ##============ EXTRACT IMPORTANT VALUES =========##\n",
    "    # Get the idf values\n",
    "    idf_values = tfidf_model.idfs # Return (word_id: idf_values)\n",
    "    scores_idf = np.array(list(idf_values.values()))\n",
    "    \n",
    "    idf_dict = {}\n",
    "    for id, value in idf_values.items():\n",
    "        word = dictionary[id]\n",
    "        idf_dict[word] = value\n",
    "        \n",
    "\n",
    "    # Apply the model to the corpus (get corpus tfidf)\n",
    "    corpus_tfidf = tfidf_model[corpus_bow]\n",
    "\n",
    "    # Get dictionary of tfidf values and scores\n",
    "    scores_tfidf = []\n",
    "    tfidf_dict = {}\n",
    "    for doc_idx, doc in enumerate(corpus_tfidf):\n",
    "\n",
    "        dict_doc = {}\n",
    "        for word_id, score in doc:\n",
    "            word = dictionary[word_id]\n",
    "            dict_doc[word] = score\n",
    "            scores_tfidf.append(score)\n",
    "\n",
    "        tfidf_dict[doc_idx] = dict_doc\n",
    "    \n",
    "    ##=========== Get the threshold =========##\n",
    "    threshold_idf = np.percentile(scores_idf, thres_idf)\n",
    "    threshold_tfidf = np.percentile(scores_tfidf, thres_tfidf)\n",
    "\n",
    "\n",
    "    ##========== Get Words =============##\n",
    "    # IDF\n",
    "    for key, value in idf_dict.items():\n",
    "        if value <= threshold_idf:\n",
    "            storage_idf.add(key)\n",
    "\n",
    "    # TF IDF\n",
    "    # for idx_doc, dict_words in tfidf_dict.items():\n",
    "    #     for key, value in dict_words.items():\n",
    "    #         if value >= threshold_tfidf:\n",
    "    #             storage_tfidf.add(key)\n",
    "\n",
    "    for idx_doc, dict_words in tfidf_dict.items():\n",
    "        temp = set()\n",
    "        for key, value in dict_words.items():\n",
    "            if value >= threshold_tfidf:\n",
    "                temp.add(key)\n",
    "            \n",
    "        storage_tfidf[idx_doc] = temp\n",
    "\n",
    "    return storage_idf, storage_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "b727df68-409e-4e3e-8f15-8c527807ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_filter(data, id, mapper_idf=None, mapper_tfidf=None):\n",
    "    # Define the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocessing(text):\n",
    "        text = remove_extra_spaces(text)\n",
    "        text = expand_contractions(text)\n",
    "        text = remove_non_ascii(text)\n",
    "\n",
    "        # Get token of words\n",
    "        doc = nlp(text)\n",
    "        result_obj = []\n",
    "        result_verb_adj = []\n",
    "        for token in doc:\n",
    "            t = token.lemma_.lower()\n",
    "            # If only contains special characters or numbers and length less than 3\n",
    "            if re.match(r'^[0-9\\W]+$', t) or len(t) < 3 or t in stop_words:\n",
    "                continue\n",
    "            # If the token is adjective, noun, propn, or verb\n",
    "            if token.pos_ in ['NOUN', 'PROPN']:\n",
    "                result_obj.append(t)\n",
    "            elif token.pos_ in ['ADJ', 'VERB']:\n",
    "                result_verb_adj.append(t)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        return result_obj, result_verb_adj\n",
    "        \n",
    "    if not mapper_idf and not mapper_tfidf:\n",
    "        return data\n",
    "\n",
    "    mapper = mapper_idf | mapper_tfidf[id]\n",
    "    temp = {}\n",
    "    for idx, element in data.items():\n",
    "        temp[idx] = []\n",
    "        for d in element:\n",
    "            text = ' '.join(d)\n",
    "            compare_obj, compare_verb_adj = preprocessing(text)\n",
    "\n",
    "            is_object_pass = False\n",
    "            is_verb_adj_pass = False\n",
    "\n",
    "            # Handling object\n",
    "            if len(compare_obj) == 0:\n",
    "                is_object_pass = True\n",
    "            else:\n",
    "                for w in compare_obj:\n",
    "                    if w in mapper:\n",
    "                        is_object_pass = True\n",
    "                        break\n",
    "            \n",
    "            # Handling verb ajective\n",
    "            if not is_verb_adj_pass:\n",
    "                for w in compare_verb_adj:\n",
    "                    if w in mapper:\n",
    "                        is_verb_adj_pass = True\n",
    "                        break\n",
    "\n",
    "            if is_object_pass and is_verb_adj_pass:\n",
    "                temp[idx].append(d)\n",
    "\n",
    "            # If object passed and verb-adj passed ==> True\n",
    "            # If object passed but verb-adj not passed ==> False\n",
    "            # If object not passed ==> False\n",
    "    return temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "88ab018e-218e-48f4-9753-b38271a4b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_filter_aspect(data, id, mapper_idf=None, mapper_tfidf=None):\n",
    "\n",
    "    if not mapper_idf and not mapper_tfidf:\n",
    "        return data\n",
    "\n",
    "    mapper = mapper_idf | mapper_tfidf[id]\n",
    "    temp = []\n",
    "    for d in data:\n",
    "        doc = nlp(d)\n",
    "        for token in doc:\n",
    "            if token.text.lower() in mapper:\n",
    "                temp.append(d)\n",
    "                break\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "cf4d8192-c356-49ef-8360-4274700a7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_words_aspect(dict_doc, idx_doc, mapper_1=None, mapper_2=None):\n",
    "    # If mapper_1 and mapper_2 is None, do not filter it.\n",
    "    if not mapper_1 and not mapper_2:\n",
    "        return dict_doc\n",
    "\n",
    "    # Copy dictionary\n",
    "    dictionary = dict_doc.copy()\n",
    "    \n",
    "    # Get mapper based on its document.\n",
    "    if mapper_2 :\n",
    "        mapper_2 = mapper_2[idx_doc]\n",
    "\n",
    "    for key, value in dictionary.items():\n",
    "        temp = []\n",
    "        for v1 in value:\n",
    "            # Since it could be multiple word, we must check one by one\n",
    "            for v in v1.split():\n",
    "                # If aspect is in mapper_1 or mapper_2 then keep it\n",
    "                if v in mapper_1 or v in mapper_2:\n",
    "                    # Append full value\n",
    "                    temp.append(v1)\n",
    "                    break\n",
    "\n",
    "        # Update list of string\n",
    "        dictionary[key] = temp\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "04d81284-6cd5-43ce-b5f0-dcb357fda3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function for preprocessing data\n",
    "\n",
    "# Function to check if a specific word exists in text\n",
    "def word_exists(word, text):\n",
    "    # Create the regex pattern with word boundaries\n",
    "    pattern = r'\\b' + re.escape(word) + r'\\b'\n",
    "    \n",
    "    # Search for the word in the text\n",
    "    if re.search(pattern, text, re.IGNORECASE):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def flatten_data(x):\n",
    "    return [item for sublist in x.values() for item in sublist]\n",
    "\n",
    "def contraction(x):\n",
    "    flatten = flatten_data(x)\n",
    "\n",
    "    if len(flatten) > 0:\n",
    "        temp = [item[0] + ' ' + item[1] for item in flatten]\n",
    "        return '. '.join(temp) + '.'\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "b14d9646-27dc-434a-bbc5-ae1e9ac1a72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     {0: [('staff', 'were helpful', ('helpful',), '...\n",
       "1     {0: [('process', 'was smooth', ('smooth',), 'A...\n",
       "2     {0: [('selection', 'is amazing', ('amazing',),...\n",
       "3     {0: [('I', 'appreciate walked me', ('appreciat...\n",
       "4     {0: [('I', 'left with phone', ('leave',), 'VER...\n",
       "5     {0: [('I', 'could not resist upgrading', ('res...\n",
       "6     {0: [('I', 'expected', ('expect',), 'VERB'), (...\n",
       "7     {0: [('experience', 'really know stuff', ('kno...\n",
       "8     {0: [('variety', 'was impressive', ('impressiv...\n",
       "9                                               {0: []}\n",
       "10    {0: [('staff', 'was knowledgeable', ('knowledg...\n",
       "11    {0: [('price', 'were reasonable', ('reasonable...\n",
       "12    {0: [('staff', 'really went mile', ('go',), 'V...\n",
       "13    {0: [('service', 'be excellent', ('excellent',...\n",
       "14    {0: [('deal', 'was friendly', ('friendly',), '...\n",
       "15    {0: [('I', 'love store', ('love',), 'VERB')], ...\n",
       "16    {0: [('I', 'got good deal on old phone', ('get...\n",
       "17    {0: [('service', 'are quick', ('quick',), 'ADJ...\n",
       "18    {0: [('staff', 'was helpful in setting phone',...\n",
       "19          {0: [('I', 'm satisfied', ('m',), 'VERB')]}\n",
       "20    {0: [('staff', 'was patient with questions', (...\n",
       "21    {0: [('process', 'was simple', ('simple',), 'A...\n",
       "22    {0: [('they', 'helped choose within budget', (...\n",
       "23    {0: [('phone', 'was fixed in minutes', ('fix',...\n",
       "24    {0: [('I', 'm', ('m',), 'VERB'), ('service', '...\n",
       "25         {0: [('store', 'hands', ('hand',), 'VERB')]}\n",
       "26    {0: [('I', 'was comfortable with purchase', ('...\n",
       "27    {0: [('I', 'found needed what', ('find', 'need...\n",
       "28    {0: [('store', 'has fantastic warranty service...\n",
       "29    {0: [('I', 'learned lot about phone features',...\n",
       "30    {0: [('variety', 'be excellent', ('excellent',...\n",
       "31                                              {0: []}\n",
       "32    {0: [('they', 'were quick in setting phone', (...\n",
       "33                                              {0: []}\n",
       "34    {0: [('layout', 'is easy', ('easy',), 'ADJ'), ...\n",
       "35    {0: [('they', 'helped save lot', ('help', 'sav...\n",
       "36    {0: [('one', 'provides best service', ('provid...\n",
       "37    {0: [('they', 'always resolve is notch', ('res...\n",
       "38    {0: [('they', 'not fail recommend to friends',...\n",
       "39     {0: [('staff', 'took time', ('take',), 'VERB')]}\n",
       "40    {0: [('place', 'be amazing', ('amazing',), 'AD...\n",
       "41       {0: [('plan', 'is worth', ('worth',), 'ADJ')]}\n",
       "42    {0: [('I', 'appreciate were able', ('appreciat...\n",
       "43                                              {0: []}\n",
       "44    {0: [('staff', 'was accommodating', ('accommod...\n",
       "45    {0: [('I', 'had great experience with program'...\n",
       "46    {0: [('service', 'was quick', ('quick',), 'ADJ...\n",
       "47    {0: [('they', 'even helped transfer without ex...\n",
       "48    {0: [('I', 'bought it', ('buy',), 'VERB'), ('p...\n",
       "49    {0: [('they', 'fixed perfectly', ('fix',), 'VE...\n",
       "50    {0: [('this', 'is reliable', ('reliable',), 'A...\n",
       "51    {0: [('they', 'offer discounts', ('offer',), '...\n",
       "52    {0: [('service', 'be phone', ('phone',), 'OTHE...\n",
       "53    {0: [('they', 'resolved professionally', ('res...\n",
       "54    {0: [('I', 'love is organized', ('love', 'be')...\n",
       "55    {0: [('you', 're looking for good deals', ('lo...\n",
       "56    {0: [('I', 'made right purchase', ('make',), '...\n",
       "57    {0: [('they', 'really know received excellent ...\n",
       "58     {0: [('phone', 'looks new', ('look',), 'VERB')]}\n",
       "59    {0: [('I', 'had to wait over hour', ('have', '...\n",
       "60                                              {0: []}\n",
       "61    {0: [('selection', 'is limited', ('limited',),...\n",
       "62    {0: [('one', 'seemed is poor', ('seem', 'be'),...\n",
       "63    {0: [('phone', 'was defective', ('defective',)...\n",
       "64    {0: [('staff', 'was rude', ('rude',), 'ADJ'), ...\n",
       "65    {0: [('they', 'felt like scam', ('feel',), 'VE...\n",
       "66    {0: [('I', 'had took too long', ('have', 'take...\n",
       "67    {0: [('they', 'did not inform me', ('inform',)...\n",
       "68    {0: [('staff', 'seemed', ('seem',), 'VERB'), (...\n",
       "69    {0: [('they', 'refused is useless', ('refuse',...\n",
       "70    {0: [('they', 'twice gave refund', ('give',), ...\n",
       "71    {0: [('I', 'very disorganized waited forever',...\n",
       "72                                              {0: []}\n",
       "73    {0: [('they', 'refused to honor promotion', ('...\n",
       "74    {0: [('I', 'felt pressured', ('feel',), 'VERB')]}\n",
       "75    {0: [('repair', 'was done poorly', ('do',), 'V...\n",
       "76    {0: [('service', 'was slow', ('slow',), 'ADJ')...\n",
       "77    {0: [('phone', 'was working after repair', ('w...\n",
       "78    {0: [('phone', 'still has same issue after get...\n",
       "79    {0: [('they', 'upsold on phone plan', ('upsold...\n",
       "80    {0: [('staff', 'was unprofessional', ('unprofe...\n",
       "81    {0: [('I', 'could not exchange phone', ('excha...\n",
       "82    {0: [('they', 'did not apply discount', ('appl...\n",
       "83    {0: [('store', 'was messy', ('messy',), 'ADJ')...\n",
       "84    {0: [('warranty', 'just expired', ('expire',),...\n",
       "85    {0: [('they', 'kept trying', ('keep', 'try'), ...\n",
       "86    {0: [('job', 'was incomplete', ('incomplete',)...\n",
       "87    {0: [('representative', 'were rude on phone', ...\n",
       "88    {0: [('I', 'had to call multiple times', ('hav...\n",
       "89    {0: [('they', 'rushed me', ('rush',), 'VERB'),...\n",
       "90        {0: [('support', 'is non', ('non',), 'ADJ')]}\n",
       "91    {0: [('phone', 'stopped working outside return...\n",
       "92    {0: [('store', 'was chaotic', ('chaotic',), 'A...\n",
       "93    {0: [('it', 'was available', ('available',), '...\n",
       "94    {0: [('they', 'lost repair order', ('lose',), ...\n",
       "95    {0: [('I', 'felt overcharged', ('feel',), 'VER...\n",
       "96                                              {0: []}\n",
       "97    {0: [('technician', 'damaged during repair', (...\n",
       "98                                              {0: []}\n",
       "Name: ability, dtype: object"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply extraction\n",
    "\n",
    "def process_ability(x):\n",
    "    # Prepare sentence\n",
    "    texts = remove_extra_spaces(x)\n",
    "    texts = expand_contractions(x)\n",
    "    texts = remove_non_ascii(x)\n",
    "\n",
    "    # Get aspect\n",
    "    doc = nlp(texts)\n",
    "    mapper_pron_ant = get_mapper_pron_ant(doc)\n",
    "    result = get_raw_abilities(doc)\n",
    "    \n",
    "    return result\n",
    "\n",
    "df['ability'] = df['review'].apply(process_ability)\n",
    "df['ability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "e2e2c44b-7325-4a47-9a97-110358c6e386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['staff were helpful. staff were patient.',\n",
       "       'process was smooth. process was quick.',\n",
       "       'selection is amazing. price are competitive.',\n",
       "       'I appreciate walked me. I appreciate walked through setting new device.',\n",
       "       'I left with phone. question answered.',\n",
       "       'I could not resist upgrading. I could not resist offer amazing deals on phones.',\n",
       "       'I expected. technician fixed phones issue. technician fixed faster.',\n",
       "       'experience really know stuff.',\n",
       "       'variety was impressive. I found perfect phone case.', '',\n",
       "       'staff was knowledgeable.', 'price were reasonable.',\n",
       "       'staff really went mile.',\n",
       "       'service be excellent. they helped find.',\n",
       "       'deal was friendly. staff was friendly.',\n",
       "       'I love store. buying be experience. fixing be experience.',\n",
       "       'I got good deal on old phone.',\n",
       "       'service are quick. service are reliable.',\n",
       "       'staff was helpful in setting phone.', 'I m satisfied.',\n",
       "       'staff was patient with questions. variety was patient with questions.',\n",
       "       'process was simple. I m thrilled.',\n",
       "       'they helped choose within budget. they helped choose phone.',\n",
       "       'phone was fixed in minutes.', 'I m. service is outstanding.',\n",
       "       'store hands.', 'I was comfortable with purchase. staff made.',\n",
       "       'I found needed what. they helped get great deal.',\n",
       "       'store has fantastic warranty service.',\n",
       "       'I learned lot about phone features. I learned was informative. staff was informative.',\n",
       "       'variety be excellent. store be excellent.', '',\n",
       "       'they were quick in setting phone.', '',\n",
       "       'layout is easy. staff are ready.', 'they helped save lot.',\n",
       "       'one provides best service. I been to many phone stores.',\n",
       "       'they always resolve is notch. service is notch. service is notch. they always resolve issues. they always resolve quickly.',\n",
       "       'they not fail recommend to friends. they not fail recommend store. they not fail recommend to family.',\n",
       "       'staff took time.', 'place be amazing.', 'plan is worth.',\n",
       "       'I appreciate were able. they were able.', '',\n",
       "       'staff was accommodating.', 'I had great experience with program.',\n",
       "       'service was quick. service was efficient.',\n",
       "       'they even helped transfer without extra charge. they even helped transfer data. they even helped transfer contacts.',\n",
       "       'I bought it. phone has working flawlessly. I bought from here.',\n",
       "       'they fixed perfectly. they fixed screen. they even gave discount on repair.',\n",
       "       'this is reliable.',\n",
       "       'they offer discounts. they offer fantastic promotions.',\n",
       "       'service be phone. selection be phone. service be great. selection be great.',\n",
       "       'they resolved professionally. they resolved very quickly. they resolved issue.',\n",
       "       'I love is organized. they fast attend to customers. store is organized.',\n",
       "       'you re looking for good deals.',\n",
       "       'I made right purchase. I always leave store.',\n",
       "       'they really know received excellent advice from sales team. they really know products.',\n",
       "       'phone looks new.',\n",
       "       'I had to wait over hour. staff was not apologetic.', '',\n",
       "       'selection is limited. price are high.',\n",
       "       'one seemed is poor. service is poor.', 'phone was defective.',\n",
       "       'staff was rude. I m not coming back. staff was unhelpful. I m not coming was rude.',\n",
       "       'they felt like scam. they charged me. they charged extra for services.',\n",
       "       'I had took too long. I had took way.',\n",
       "       'they did not inform me. they did not inform of hidden fees. I bought phone.',\n",
       "       'staff seemed. staff gave incorrect information about phone plan.',\n",
       "       'they refused is useless. warranty is useless.',\n",
       "       'they twice gave refund. I had to return faulty phone.',\n",
       "       'I very disorganized waited forever.', '',\n",
       "       'they refused to honor promotion.', 'I felt pressured.',\n",
       "       'repair was done poorly. phone broke within week. phone broke again.',\n",
       "       'service was slow. they need was slow.',\n",
       "       'phone was working after repair.',\n",
       "       'phone still has same issue after getting.',\n",
       "       'they upsold on phone plan. they upsold me.',\n",
       "       'staff was unprofessional. they did not want to be there.',\n",
       "       'I could not exchange phone. I could not exchange is awful. policy is awful. I could not exchange despite defects.',\n",
       "       'they did not apply discount.',\n",
       "       'store was messy. store was understaffed.',\n",
       "       'warranty just expired. phone broke.', 'they kept trying.',\n",
       "       'job was incomplete. they refused to refund me.',\n",
       "       'representative were rude on phone.',\n",
       "       'I had to call multiple times.',\n",
       "       'they rushed me. they did not explain anything. they did not explain clearly. they rushed through purchase.',\n",
       "       'support is non.',\n",
       "       'phone stopped working outside return window. phone stopped quality.',\n",
       "       'store was chaotic. store was with unhelpful staff. store was with long lines.',\n",
       "       'it was available. they did not even have after promising me. they did not even have phone.',\n",
       "       'they lost repair order.', 'I felt overcharged.', '',\n",
       "       'technician damaged during repair. technician damaged phone. they did not take responsibility.',\n",
       "       ''], dtype=object)"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df['ability'].apply(contraction).values\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "06a6eb43-f9bb-48ef-9914-ba5ef0a61170",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = corpus\n",
    "\n",
    "mapper_1, mapper_2 = get_words(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "2ae1235f-97ab-4f04-a0c7-c3d710063960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always',\n",
       " 'amazing',\n",
       " 'appreciate',\n",
       " 'break',\n",
       " 'buy',\n",
       " 'charge',\n",
       " 'could',\n",
       " 'deal',\n",
       " 'discount',\n",
       " 'even',\n",
       " 'excellent',\n",
       " 'experience',\n",
       " 'extra',\n",
       " 'fantastic',\n",
       " 'fast',\n",
       " 'feel',\n",
       " 'find',\n",
       " 'fix',\n",
       " 'get',\n",
       " 'give',\n",
       " 'good',\n",
       " 'great',\n",
       " 'help',\n",
       " 'helpful',\n",
       " 'issue',\n",
       " 'know',\n",
       " 'leave',\n",
       " 'long',\n",
       " 'look',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'make',\n",
       " 'need',\n",
       " 'new',\n",
       " 'offer',\n",
       " 'one',\n",
       " 'patient',\n",
       " 'phone',\n",
       " 'plan',\n",
       " 'price',\n",
       " 'process',\n",
       " 'promotion',\n",
       " 'purchase',\n",
       " 'question',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'really',\n",
       " 'refund',\n",
       " 'refuse',\n",
       " 'reliable',\n",
       " 'repair',\n",
       " 'resolve',\n",
       " 'return',\n",
       " 'rude',\n",
       " 'seem',\n",
       " 'selection',\n",
       " 'service',\n",
       " 'set',\n",
       " 'staff',\n",
       " 'store',\n",
       " 'take',\n",
       " 'technician',\n",
       " 'time',\n",
       " 'unhelpful',\n",
       " 'variety',\n",
       " 'wait',\n",
       " 'warranty',\n",
       " 'within',\n",
       " 'work'}"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "8faf5158-196f-44f8-b093-1b956428d5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: set(),\n",
       " 1: {'process'},\n",
       " 2: set(),\n",
       " 3: {'walk'},\n",
       " 4: {'answer'},\n",
       " 5: {'resist'},\n",
       " 6: {'technician'},\n",
       " 7: set(),\n",
       " 8: set(),\n",
       " 9: set(),\n",
       " 10: {'knowledgeable'},\n",
       " 11: {'reasonable'},\n",
       " 12: {'mile'},\n",
       " 13: set(),\n",
       " 14: {'friendly'},\n",
       " 15: {'experience'},\n",
       " 16: {'old'},\n",
       " 17: {'service'},\n",
       " 18: {'helpful', 'set'},\n",
       " 19: {'satisfied'},\n",
       " 20: {'patient', 'question'},\n",
       " 21: {'simple', 'thrilled'},\n",
       " 22: {'choose'},\n",
       " 23: {'minute'},\n",
       " 24: {'outstanding'},\n",
       " 25: {'hand'},\n",
       " 26: {'comfortable'},\n",
       " 27: set(),\n",
       " 28: {'fantastic'},\n",
       " 29: {'informative', 'learn'},\n",
       " 30: {'excellent'},\n",
       " 31: set(),\n",
       " 32: {'quick', 'set'},\n",
       " 33: set(),\n",
       " 34: set(),\n",
       " 35: {'save'},\n",
       " 36: set(),\n",
       " 37: {'notch'},\n",
       " 38: {'fail', 'recommend'},\n",
       " 39: {'take', 'time'},\n",
       " 40: {'place'},\n",
       " 41: {'worth'},\n",
       " 42: {'able'},\n",
       " 43: set(),\n",
       " 44: {'accommodate'},\n",
       " 45: {'program'},\n",
       " 46: {'efficient', 'service'},\n",
       " 47: {'transfer'},\n",
       " 48: {'buy'},\n",
       " 49: set(),\n",
       " 50: {'reliable'},\n",
       " 51: {'offer'},\n",
       " 52: {'great', 'selection'},\n",
       " 53: {'resolve'},\n",
       " 54: {'organize'},\n",
       " 55: {'look'},\n",
       " 56: set(),\n",
       " 57: set(),\n",
       " 58: {'look', 'new'},\n",
       " 59: set(),\n",
       " 60: set(),\n",
       " 61: set(),\n",
       " 62: {'poor'},\n",
       " 63: {'defective'},\n",
       " 64: {'come'},\n",
       " 65: {'charge'},\n",
       " 66: {'take'},\n",
       " 67: {'inform'},\n",
       " 68: set(),\n",
       " 69: {'useless'},\n",
       " 70: set(),\n",
       " 71: {'disorganized', 'forever'},\n",
       " 72: set(),\n",
       " 73: {'honor'},\n",
       " 74: {'pressured'},\n",
       " 75: {'break'},\n",
       " 76: {'slow'},\n",
       " 77: {'repair', 'work'},\n",
       " 78: {'still'},\n",
       " 79: {'upsold'},\n",
       " 80: {'unprofessional', 'want'},\n",
       " 81: {'exchange'},\n",
       " 82: {'apply'},\n",
       " 83: set(),\n",
       " 84: {'expire'},\n",
       " 85: {'keep', 'try'},\n",
       " 86: set(),\n",
       " 87: {'representative', 'rude'},\n",
       " 88: {'call', 'multiple'},\n",
       " 89: {'explain', 'rush'},\n",
       " 90: {'non', 'support'},\n",
       " 91: {'stop'},\n",
       " 92: {'store'},\n",
       " 93: {'even'},\n",
       " 94: {'lose', 'order'},\n",
       " 95: {'overcharge'},\n",
       " 96: set(),\n",
       " 97: {'damage'},\n",
       " 98: set()}"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982e8f4-1a3b-471e-bd24-9f97df3ceb9d",
   "metadata": {},
   "source": [
    "**Important Sentence Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "ba6a041b-f714-4b58-90af-9694447cab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: FILTER FOR SENTENCES ==> CHANGE WITH LSI SUMMARIZATION.\n",
    "\n",
    "# df['ability_filtered'] = [weighted_filter(data, id=id, mapper_idf=mapper_1, mapper_tfidf=mapper_2) for id, data in enumerate(df['ability'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de237bad-28d6-4162-94be-0abfa9b7d13a",
   "metadata": {},
   "source": [
    "**Aspect Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "e7189e4f-ba67-4d5c-90ba-8dffa99aa4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_matched_patterns(input_string, patterns):\n",
    "#     # Create a regex pattern by joining the list of patterns using the '|' (OR) operator\n",
    "#     regex_pattern = '|'.join(map(re.escape, patterns))\n",
    "    \n",
    "#     # Use re.findall to find all matches in the input string\n",
    "#     matches = re.findall(regex_pattern, input_string)\n",
    "    \n",
    "#     return matches\n",
    "\n",
    "# # Example usage\n",
    "# input_string = \"This is an apple, melon dragon, and orange\"\n",
    "# patterns = ['apple', 'an apple', 'melon dragon', 'melon', 'dragon', 'manggoo', 'pink']\n",
    "\n",
    "# matched_patterns = find_matched_patterns(input_string, patterns)\n",
    "# print(matched_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "f1ce402e-322d-4fdb-a959-539ba5d2ec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'staff': ['staff were helpful.', 'staff were patient.']},\n",
       " {'process': ['process was smooth.', 'process was quick.']},\n",
       " {'selection': ['selection is amazing.'], 'price': ['price are competitive.']},\n",
       " {},\n",
       " {'question': ['question answered.']},\n",
       " {},\n",
       " {'phones issue': ['technician fixed phones issue.'],\n",
       "  'technician': ['technician fixed phones issue.',\n",
       "   'technician fixed faster.']},\n",
       " {'experience': ['experience really know stuff.']},\n",
       " {'variety': ['variety was impressive.']},\n",
       " {},\n",
       " {'staff': ['staff was knowledgeable.']},\n",
       " {'price': ['price were reasonable.']},\n",
       " {'staff': ['staff really went mile.']},\n",
       " {'service': ['service be excellent.']},\n",
       " {'deal': ['deal was friendly.'], 'staff': ['staff was friendly.']},\n",
       " {'buying': ['buying be experience.'], 'fixing': ['fixing be experience.']},\n",
       " {},\n",
       " {'service': ['service are quick.', 'service are reliable.']},\n",
       " {'phone': ['staff was helpful in setting phone.'],\n",
       "  'staff': ['staff was helpful in setting phone.']},\n",
       " {},\n",
       " {'staff': ['staff was patient with questions.'],\n",
       "  'variety': ['variety was patient with questions.']},\n",
       " {'process': ['process was simple.']},\n",
       " {},\n",
       " {'phone': ['phone was fixed in minutes.']},\n",
       " {'service': ['service is outstanding.']},\n",
       " {'store': ['store hands.']},\n",
       " {'staff': ['staff made.']},\n",
       " {},\n",
       " {'warranty service': ['store has fantastic warranty service.'],\n",
       "  'store': ['store has fantastic warranty service.']},\n",
       " {'staff': ['staff was informative.']},\n",
       " {'variety': ['variety be excellent.'], 'store': ['store be excellent.']},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {'layout': ['layout is easy.'], 'staff': ['staff are ready.']},\n",
       " {},\n",
       " {'service': ['one provides best service.'],\n",
       "  'one': ['one provides best service.']},\n",
       " {'service': ['service is notch.', 'service is notch.']},\n",
       " {},\n",
       " {'time': ['staff took time.'], 'staff': ['staff took time.']},\n",
       " {'place': ['place be amazing.']},\n",
       " {'plan': ['plan is worth.']},\n",
       " {},\n",
       " {},\n",
       " {'staff': ['staff was accommodating.']},\n",
       " {},\n",
       " {'service': ['service was quick.', 'service was efficient.']},\n",
       " {},\n",
       " {'phone': ['phone has working flawlessly.']},\n",
       " {},\n",
       " {'this': ['this is reliable.']},\n",
       " {},\n",
       " {'service': ['service be phone.', 'service be great.'],\n",
       "  'selection': ['selection be phone.', 'selection be great.']},\n",
       " {},\n",
       " {'store': ['store is organized.']},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {'phone': ['phone looks new.']},\n",
       " {'staff': ['staff was not apologetic.']},\n",
       " {},\n",
       " {'selection': ['selection is limited.'], 'price': ['price are high.']},\n",
       " {'one': ['one seemed is poor.'], 'service': ['service is poor.']},\n",
       " {'phone': ['phone was defective.']},\n",
       " {'staff': ['staff was rude.', 'staff was unhelpful.']},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {'staff': ['staff seemed.',\n",
       "   'staff gave incorrect information about phone plan.'],\n",
       "  'phone plan': ['staff gave incorrect information about phone plan.']},\n",
       " {'warranty': ['warranty is useless.']},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {'repair': ['repair was done poorly.'],\n",
       "  'phone': ['phone broke within week.', 'phone broke again.']},\n",
       " {'service': ['service was slow.']},\n",
       " {'phone': ['phone was working after repair.']},\n",
       " {'same issue': ['phone still has same issue after getting.'],\n",
       "  'phone': ['phone still has same issue after getting.']},\n",
       " {},\n",
       " {'staff': ['staff was unprofessional.']},\n",
       " {'policy': ['policy is awful.']},\n",
       " {},\n",
       " {'store': ['store was messy.', 'store was understaffed.']},\n",
       " {'warranty': ['warranty just expired.'], 'phone': ['phone broke.']},\n",
       " {},\n",
       " {'job': ['job was incomplete.']},\n",
       " {'representative': ['representative were rude on phone.']},\n",
       " {},\n",
       " {},\n",
       " {'support': ['support is non.']},\n",
       " {'return window': ['phone stopped working outside return window.'],\n",
       "  'phone': ['phone stopped working outside return window.',\n",
       "   'phone stopped quality.']},\n",
       " {'store': ['store was chaotic.',\n",
       "   'store was with unhelpful staff.',\n",
       "   'store was with long lines.'],\n",
       "  'staff': ['store was with unhelpful staff.'],\n",
       "  'long lines': ['store was with long lines.']},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {'technician': ['technician damaged during repair.',\n",
       "   'technician damaged phone.']},\n",
       " {}]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aspect extraction rules\n",
    "\n",
    "def find_matched_patterns(input_string, patterns):\n",
    "    # Create a regex pattern by joining the list of patterns using the '|' (OR) operator\n",
    "    regex_pattern = '|'.join(map(re.escape, patterns))\n",
    "    \n",
    "    # Use re.findall to find all matches in the input string\n",
    "    matches = re.findall(regex_pattern, input_string)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def process_aspect_rules(data, id):\n",
    "    subject_pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they']\n",
    "    \n",
    "    # Data is converted into flatten\n",
    "    data = flatten_data(data)\n",
    "    # Get text and subject of data\n",
    "    if len(data) > 0:\n",
    "        text, subjects, keywords, labels = zip(*[(item[0] + ' ' + item[1] + '.', item[0], item[2], item[3]) for item in data])\n",
    "    else: \n",
    "        text, subjects, keywords, labels = ('', '', '', '')\n",
    "\n",
    "\n",
    "    \n",
    "    # Convert text into string    \n",
    "    doc = nlp('. '.join(text))\n",
    "\n",
    "    # Extract from rules ==> Weighted pattern extraction\n",
    "    result = get_raw_aspects(doc)\n",
    "    result = prunning_aspect(result, doc)\n",
    "    # Get aspects\n",
    "    aspects = list(result.keys())\n",
    "    # Weighted filter\n",
    "    aspects = weighted_filter_aspect(aspects, id, mapper_idf=mapper_1, mapper_tfidf=mapper_2)\n",
    "\n",
    "    # update_storage = {}\n",
    "    storage = {}\n",
    "        \n",
    "    for i in range(len(data)):\n",
    "        s, t, k, l = subjects[i].lower(), text[i], keywords[i], labels[i]\n",
    "        \n",
    "        # If subject is pron continue\n",
    "        if s in subject_pronouns:\n",
    "            continue\n",
    "\n",
    "        # Temporary storage\n",
    "        temp = {'ADJ': [],\n",
    "               'VERB': [],\n",
    "               'OTHER': [],}\n",
    "\n",
    "        # Add keywords and text\n",
    "        temp[l].append((k, t))\n",
    "        \n",
    "        # Get matched aspects and Filtering\n",
    "        matched = []\n",
    "        if len(aspects) > 0:\n",
    "            matched += find_matched_patterns(t, aspects)\n",
    "            matched = [a for a in matched if not word_exists(s, a)]\n",
    "        matched.append(s)\n",
    "            \n",
    "        # Storing\n",
    "        for a in matched:\n",
    "            storage[a] = temp\n",
    "            # if not storage.get(a):\n",
    "            #     storage[a] = [t]\n",
    "            # else:\n",
    "            #     storage[a].append(t)\n",
    "                    \n",
    "    return storage\n",
    "\n",
    "aspects = [process_aspect_rules(data, id) for id, data in enumerate(df['ability'].values)]\n",
    "\n",
    "print(len(aspects))\n",
    "aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "89182656-1a06-44a9-aca5-acd3e861d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   review   99 non-null     object\n",
      " 1   ability  99 non-null     object\n",
      " 2   aspects  99 non-null     object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.4+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>ability</th>\n",
       "      <th>aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The staff were incredibly helpful and patient,...</td>\n",
       "      <td>{0: [('staff', 'were helpful', ('helpful',), '...</td>\n",
       "      <td>{'staff': ['staff were helpful.', 'staff were ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I had a great experience purchasing my phone h...</td>\n",
       "      <td>{0: [('process', 'was smooth', ('smooth',), 'A...</td>\n",
       "      <td>{'process': ['process was smooth.', 'process w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Their selection of phones is amazing, and the ...</td>\n",
       "      <td>{0: [('selection', 'is amazing', ('amazing',),...</td>\n",
       "      <td>{'selection': ['selection is amazing.'], 'pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I appreciate how the staff walked me through s...</td>\n",
       "      <td>{0: [('I', 'appreciate walked me', ('appreciat...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great customer service, I left with the phone ...</td>\n",
       "      <td>{0: [('I', 'left with phone', ('leave',), 'VER...</td>\n",
       "      <td>{'question': ['question answered.']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review                                            ability                                            aspects\n",
       "0  The staff were incredibly helpful and patient,...  {0: [('staff', 'were helpful', ('helpful',), '...  {'staff': ['staff were helpful.', 'staff were ...\n",
       "1  I had a great experience purchasing my phone h...  {0: [('process', 'was smooth', ('smooth',), 'A...  {'process': ['process was smooth.', 'process w...\n",
       "2  Their selection of phones is amazing, and the ...  {0: [('selection', 'is amazing', ('amazing',),...  {'selection': ['selection is amazing.'], 'pric...\n",
       "3  I appreciate how the staff walked me through s...  {0: [('I', 'appreciate walked me', ('appreciat...                                                 {}\n",
       "4  Great customer service, I left with the phone ...  {0: [('I', 'left with phone', ('leave',), 'VER...               {'question': ['question answered.']}"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['aspects'] = aspects\n",
    "\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "2e92b293-6ae3-4dd5-9522-773194772c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>ability</th>\n",
       "      <th>aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I appreciate how the staff walked me through s...</td>\n",
       "      <td>{0: [('I', 'appreciate walked me', ('appreciat...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>They offer amazing deals on phones, I couldnt...</td>\n",
       "      <td>{0: [('I', 'could not resist upgrading', ('res...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Upgrading my phone was a breeze thanks to thei...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I got a really good trade-in deal on my old ph...</td>\n",
       "      <td>{0: [('I', 'got good deal on old phone', ('get...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Very professional and friendly service, Im su...</td>\n",
       "      <td>{0: [('I', 'm satisfied', ('m',), 'VERB')]}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>They helped me choose a phone within my budget...</td>\n",
       "      <td>{0: [('they', 'helped choose within budget', (...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I found exactly what I needed, and they helped...</td>\n",
       "      <td>{0: [('I', 'found needed what', ('find', 'need...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>The phone I bought here is working perfectly, ...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>They were super quick in setting up my phone, ...</td>\n",
       "      <td>{0: [('they', 'were quick in setting phone', (...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Always come here for upgrades, they never disa...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Best pricing for phone plans, they helped me s...</td>\n",
       "      <td>{0: [('they', 'helped save lot', ('help', 'sav...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>I always recommend this store to friends and f...</td>\n",
       "      <td>{0: [('they', 'not fail recommend to friends',...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>I appreciate how they were able to fix my phon...</td>\n",
       "      <td>{0: [('I', 'appreciate were able', ('appreciat...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Got a great deal on my new phone and an awesom...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>I had a great experience with their trade-in p...</td>\n",
       "      <td>{0: [('I', 'had great experience with program'...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>They even helped me transfer all my contacts a...</td>\n",
       "      <td>{0: [('they', 'even helped transfer without ex...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>They fixed my screen perfectly and even gave m...</td>\n",
       "      <td>{0: [('they', 'fixed perfectly', ('fix',), 'VE...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>They offer fantastic promotions and discounts!</td>\n",
       "      <td>{0: [('they', 'offer discounts', ('offer',), '...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>They resolved my issue very quickly and profes...</td>\n",
       "      <td>{0: [('they', 'resolved professionally', ('res...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Highly recommend this store if youre looking ...</td>\n",
       "      <td>{0: [('you', 're looking for good deals', ('lo...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>I always leave this store feeling like I made ...</td>\n",
       "      <td>{0: [('I', 'made right purchase', ('make',), '...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>I received excellent advice from the sales tea...</td>\n",
       "      <td>{0: [('they', 'really know received excellent ...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Bought a phone here that stopped working withi...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>They charged me extra for services I didnt ne...</td>\n",
       "      <td>{0: [('they', 'felt like scam', ('feel',), 'VE...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Phone repairs took way too long, I had to come...</td>\n",
       "      <td>{0: [('I', 'had took too long', ('have', 'take...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>I bought a phone, but they didnt inform me of...</td>\n",
       "      <td>{0: [('they', 'did not inform me', ('inform',)...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>I had to return a faulty phone twice before th...</td>\n",
       "      <td>{0: [('they', 'twice gave refund', ('give',), ...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Very disorganized, I waited forever just to ge...</td>\n",
       "      <td>{0: [('I', 'very disorganized waited forever',...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>The phone I purchased here was overpriced comp...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>They refused to honor the promotion I came in ...</td>\n",
       "      <td>{0: [('they', 'refused to honor promotion', ('...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>I felt pressured to buy accessories I didnt n...</td>\n",
       "      <td>{0: [('I', 'felt pressured', ('feel',), 'VERB')]}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>They upsold me on a phone plan I didnt need, ...</td>\n",
       "      <td>{0: [('they', 'upsold on phone plan', ('upsold...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>They didnt apply the discount I was promised.</td>\n",
       "      <td>{0: [('they', 'did not apply discount', ('appl...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>They kept trying to sell me more expensive pho...</td>\n",
       "      <td>{0: [('they', 'kept trying', ('keep', 'try'), ...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>I had to call multiple times just to get a res...</td>\n",
       "      <td>{0: [('I', 'had to call multiple times', ('hav...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>They didnt explain anything clearly and rushe...</td>\n",
       "      <td>{0: [('they', 'rushed me', ('rush',), 'VERB'),...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>They didnt even have the phone I wanted in st...</td>\n",
       "      <td>{0: [('it', 'was available', ('available',), '...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Terrible follow-up, they lost my repair order,...</td>\n",
       "      <td>{0: [('they', 'lost repair order', ('lose',), ...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>I felt overcharged for a simple screen repair.</td>\n",
       "      <td>{0: [('I', 'felt overcharged', ('feel',), 'VER...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Bought a refurbished phone that had several is...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Im extremely disappointed, will not be coming...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review                                            ability aspects\n",
       "3   I appreciate how the staff walked me through s...  {0: [('I', 'appreciate walked me', ('appreciat...      {}\n",
       "5   They offer amazing deals on phones, I couldnt...  {0: [('I', 'could not resist upgrading', ('res...      {}\n",
       "9   Upgrading my phone was a breeze thanks to thei...                                            {0: []}      {}\n",
       "16  I got a really good trade-in deal on my old ph...  {0: [('I', 'got good deal on old phone', ('get...      {}\n",
       "19  Very professional and friendly service, Im su...        {0: [('I', 'm satisfied', ('m',), 'VERB')]}      {}\n",
       "22  They helped me choose a phone within my budget...  {0: [('they', 'helped choose within budget', (...      {}\n",
       "27  I found exactly what I needed, and they helped...  {0: [('I', 'found needed what', ('find', 'need...      {}\n",
       "31  The phone I bought here is working perfectly, ...                                            {0: []}      {}\n",
       "32  They were super quick in setting up my phone, ...  {0: [('they', 'were quick in setting phone', (...      {}\n",
       "33  Always come here for upgrades, they never disa...                                            {0: []}      {}\n",
       "35  Best pricing for phone plans, they helped me s...  {0: [('they', 'helped save lot', ('help', 'sav...      {}\n",
       "38  I always recommend this store to friends and f...  {0: [('they', 'not fail recommend to friends',...      {}\n",
       "42  I appreciate how they were able to fix my phon...  {0: [('I', 'appreciate were able', ('appreciat...      {}\n",
       "43  Got a great deal on my new phone and an awesom...                                            {0: []}      {}\n",
       "45  I had a great experience with their trade-in p...  {0: [('I', 'had great experience with program'...      {}\n",
       "47  They even helped me transfer all my contacts a...  {0: [('they', 'even helped transfer without ex...      {}\n",
       "49  They fixed my screen perfectly and even gave m...  {0: [('they', 'fixed perfectly', ('fix',), 'VE...      {}\n",
       "51     They offer fantastic promotions and discounts!  {0: [('they', 'offer discounts', ('offer',), '...      {}\n",
       "53  They resolved my issue very quickly and profes...  {0: [('they', 'resolved professionally', ('res...      {}\n",
       "55  Highly recommend this store if youre looking ...  {0: [('you', 're looking for good deals', ('lo...      {}\n",
       "56  I always leave this store feeling like I made ...  {0: [('I', 'made right purchase', ('make',), '...      {}\n",
       "57  I received excellent advice from the sales tea...  {0: [('they', 'really know received excellent ...      {}\n",
       "60  Bought a phone here that stopped working withi...                                            {0: []}      {}\n",
       "65  They charged me extra for services I didnt ne...  {0: [('they', 'felt like scam', ('feel',), 'VE...      {}\n",
       "66  Phone repairs took way too long, I had to come...  {0: [('I', 'had took too long', ('have', 'take...      {}\n",
       "67  I bought a phone, but they didnt inform me of...  {0: [('they', 'did not inform me', ('inform',)...      {}\n",
       "70  I had to return a faulty phone twice before th...  {0: [('they', 'twice gave refund', ('give',), ...      {}\n",
       "71  Very disorganized, I waited forever just to ge...  {0: [('I', 'very disorganized waited forever',...      {}\n",
       "72  The phone I purchased here was overpriced comp...                                            {0: []}      {}\n",
       "73  They refused to honor the promotion I came in ...  {0: [('they', 'refused to honor promotion', ('...      {}\n",
       "74  I felt pressured to buy accessories I didnt n...  {0: [('I', 'felt pressured', ('feel',), 'VERB')]}      {}\n",
       "79  They upsold me on a phone plan I didnt need, ...  {0: [('they', 'upsold on phone plan', ('upsold...      {}\n",
       "82     They didnt apply the discount I was promised.  {0: [('they', 'did not apply discount', ('appl...      {}\n",
       "85  They kept trying to sell me more expensive pho...  {0: [('they', 'kept trying', ('keep', 'try'), ...      {}\n",
       "88  I had to call multiple times just to get a res...  {0: [('I', 'had to call multiple times', ('hav...      {}\n",
       "89  They didnt explain anything clearly and rushe...  {0: [('they', 'rushed me', ('rush',), 'VERB'),...      {}\n",
       "93  They didnt even have the phone I wanted in st...  {0: [('it', 'was available', ('available',), '...      {}\n",
       "94  Terrible follow-up, they lost my repair order,...  {0: [('they', 'lost repair order', ('lose',), ...      {}\n",
       "95     I felt overcharged for a simple screen repair.  {0: [('I', 'felt overcharged', ('feel',), 'VER...      {}\n",
       "96  Bought a refurbished phone that had several is...                                            {0: []}      {}\n",
       "98  Im extremely disappointed, will not be coming...                                            {0: []}      {}"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom handling aspect\n",
    "\n",
    "fail = df[df.apply(lambda x: len(x['aspects']) == 0, axis=1)]\n",
    "print(len(fail))\n",
    "fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08017eef-0f25-432a-ab69-4eb3dfd8c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect extraction rules\n",
    "\n",
    "def find_matched_patterns(input_string, patterns):\n",
    "    # Create a regex pattern by joining the list of patterns using the '|' (OR) operator\n",
    "    regex_pattern = '|'.join(map(re.escape, patterns))\n",
    "    \n",
    "    # Use re.findall to find all matches in the input string\n",
    "    matches = re.findall(regex_pattern, input_string)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def process_aspect_rules(data, id):\n",
    "    subject_pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they']\n",
    "    \n",
    "    # Data is converted into flatten\n",
    "    data = flatten_data(data)\n",
    "    # Get text and subject of data\n",
    "    if len(data) > 0:\n",
    "        text, subjects, keywords, labels = zip(*[(item[0] + ' ' + item[1] + '.', item[0], item[2], item[3]) for item in data])\n",
    "    else: \n",
    "        text, subjects, keywords, labels = ('', '', '', '')\n",
    "\n",
    "\n",
    "    \n",
    "    # Convert text into string    \n",
    "    doc = nlp('. '.join(text))\n",
    "\n",
    "    # Extract from rules ==> Weighted pattern extraction\n",
    "    result = get_raw_aspects(doc)\n",
    "    result = prunning_aspect(result, doc)\n",
    "    # Get aspects\n",
    "    aspects = list(result.keys())\n",
    "    # Weighted filter\n",
    "    aspects = weighted_filter_aspect(aspects, id, mapper_idf=mapper_1, mapper_tfidf=mapper_2)\n",
    "\n",
    "    # update_storage = {}\n",
    "    storage = {}\n",
    "        \n",
    "    for i in range(len(data)):\n",
    "        s, t, k, l = subjects[i].lower(), text[i], keywords[i], labels[i]\n",
    "        \n",
    "        # If subject is pron continue\n",
    "        if s in subject_pronouns:\n",
    "            continue\n",
    "\n",
    "        # Temporary storage\n",
    "        temp = {'ADJ': [],\n",
    "               'VERB': [],\n",
    "               'OTHER': [],}\n",
    "\n",
    "        # Add keywords and text\n",
    "        temp[l].append((k, t))\n",
    "        \n",
    "        # Get matched aspects and Filtering\n",
    "        matched = []\n",
    "        if len(aspects) > 0:\n",
    "            matched += find_matched_patterns(t, aspects)\n",
    "            matched = [a for a in matched if not word_exists(s, a)]\n",
    "        matched.append(s)\n",
    "            \n",
    "        # Storing\n",
    "        for a in matched:\n",
    "            storage[a] = temp\n",
    "            # if not storage.get(a):\n",
    "            #     storage[a] = [t]\n",
    "            # else:\n",
    "            #     storage[a].append(t)\n",
    "                    \n",
    "    return storage\n",
    "\n",
    "aspects = [process_aspect_rules(data, id) for id, data in enumerate(df['ability'].values)]\n",
    "\n",
    "print(len(aspects))\n",
    "aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "dc88137d-1761-455d-8f1a-6cccc8b111bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('staff', 'were helpful', ('helpful',), 'ADJ'), ('staff', 'were patient', ('patient',), 'ADJ')]\n",
      "[('process', 'was smooth', ('smooth',), 'ADJ'), ('process', 'was quick', ('quick',), 'ADJ')]\n",
      "[('selection', 'is amazing', ('amazing',), 'ADJ'), ('price', 'are competitive', ('competitive',), 'ADJ')]\n",
      "[('I', 'appreciate walked me', ('appreciate', 'walk'), 'VERB'), ('I', 'appreciate walked through setting new device', ('appreciate', 'walk'), 'VERB')]\n",
      "[('I', 'left with phone', ('leave',), 'VERB'), ('question', 'answered', ('answer',), 'VERB')]\n",
      "[('I', 'could not resist upgrading', ('resist',), 'VERB'), ('I', 'could not resist offer amazing deals on phones', ('resist', 'offer'), 'VERB')]\n",
      "[('I', 'expected', ('expect',), 'VERB'), ('technician', 'fixed phones issue', ('fix',), 'VERB'), ('technician', 'fixed faster', ('fix',), 'VERB')]\n",
      "[('experience', 'really know stuff', ('know',), 'VERB')]\n",
      "[('variety', 'was impressive', ('impressive',), 'ADJ'), ('I', 'found perfect phone case', ('find',), 'VERB')]\n",
      "[]\n",
      "[('staff', 'was knowledgeable', ('knowledgeable',), 'ADJ')]\n",
      "[('price', 'were reasonable', ('reasonable',), 'ADJ')]\n",
      "[('staff', 'really went mile', ('go',), 'VERB')]\n",
      "[('service', 'be excellent', ('excellent',), 'ADJ'), ('they', 'helped find', ('help', 'find'), 'VERB')]\n",
      "[('deal', 'was friendly', ('friendly',), 'ADJ'), ('staff', 'was friendly', ('friendly',), 'ADJ')]\n",
      "[('I', 'love store', ('love',), 'VERB'), ('buying', 'be experience', ('experience',), 'OTHER'), ('fixing', 'be experience', ('experience',), 'OTHER')]\n",
      "[('I', 'got good deal on old phone', ('get',), 'VERB')]\n",
      "[('service', 'are quick', ('quick',), 'ADJ'), ('service', 'are reliable', ('reliable',), 'ADJ')]\n",
      "[('staff', 'was helpful in setting phone', ('helpful',), 'ADJ')]\n",
      "[('I', 'm satisfied', ('m',), 'VERB')]\n",
      "[('staff', 'was patient with questions', ('patient',), 'ADJ'), ('variety', 'was patient with questions', ('patient',), 'ADJ')]\n",
      "[('process', 'was simple', ('simple',), 'ADJ'), ('I', 'm thrilled', ('m',), 'VERB')]\n",
      "[('they', 'helped choose within budget', ('help', 'choose'), 'VERB'), ('they', 'helped choose phone', ('help', 'choose'), 'VERB')]\n",
      "[('phone', 'was fixed in minutes', ('fix',), 'VERB')]\n",
      "[('I', 'm', ('m',), 'VERB'), ('service', 'is outstanding', ('outstanding',), 'ADJ')]\n",
      "[('store', 'hands', ('hand',), 'VERB')]\n",
      "[('I', 'was comfortable with purchase', ('comfortable',), 'ADJ'), ('staff', 'made', ('make',), 'VERB')]\n",
      "[('I', 'found needed what', ('find', 'need'), 'VERB'), ('they', 'helped get great deal', ('help', 'get'), 'VERB')]\n",
      "[('store', 'has fantastic warranty service', ('have',), 'VERB')]\n",
      "[('I', 'learned lot about phone features', ('learn',), 'VERB'), ('I', 'learned was informative', ('learn', 'be'), 'VERB'), ('staff', 'was informative', ('informative',), 'ADJ')]\n",
      "[('variety', 'be excellent', ('excellent',), 'ADJ'), ('store', 'be excellent', ('excellent',), 'ADJ')]\n",
      "[]\n",
      "[('they', 'were quick in setting phone', ('quick',), 'ADJ')]\n",
      "[]\n",
      "[('layout', 'is easy', ('easy',), 'ADJ'), ('staff', 'are ready', ('ready',), 'ADJ')]\n",
      "[('they', 'helped save lot', ('help', 'save'), 'VERB')]\n",
      "[('one', 'provides best service', ('provide',), 'VERB'), ('I', 'been to many phone stores', ('stores',), 'OTHER')]\n",
      "[('they', 'always resolve is notch', ('resolve', 'be'), 'VERB'), ('service', 'is notch', ('notch',), 'ADJ'), ('service', 'is notch', ('notch',), 'OTHER'), ('they', 'always resolve issues', ('resolve',), 'VERB'), ('they', 'always resolve quickly', ('resolve',), 'VERB')]\n",
      "[('they', 'not fail recommend to friends', ('fail', 'recommend'), 'VERB'), ('they', 'not fail recommend store', ('fail', 'recommend'), 'VERB'), ('they', 'not fail recommend to family', ('fail', 'recommend'), 'VERB')]\n",
      "[('staff', 'took time', ('take',), 'VERB')]\n",
      "[('place', 'be amazing', ('amazing',), 'ADJ')]\n",
      "[('plan', 'is worth', ('worth',), 'ADJ')]\n",
      "[('I', 'appreciate were able', ('appreciate', 'be'), 'VERB'), ('they', 'were able', ('able',), 'ADJ')]\n",
      "[]\n",
      "[('staff', 'was accommodating', ('accommodating',), 'ADJ')]\n",
      "[('I', 'had great experience with program', ('have',), 'VERB')]\n",
      "[('service', 'was quick', ('quick',), 'ADJ'), ('service', 'was efficient', ('efficient',), 'ADJ')]\n",
      "[('they', 'even helped transfer without extra charge', ('help', 'transfer'), 'VERB'), ('they', 'even helped transfer data', ('help', 'transfer'), 'VERB'), ('they', 'even helped transfer contacts', ('help', 'transfer'), 'VERB')]\n",
      "[('I', 'bought it', ('buy',), 'VERB'), ('phone', 'has working flawlessly', ('work',), 'VERB'), ('I', 'bought from here', ('buy',), 'VERB')]\n",
      "[('they', 'fixed perfectly', ('fix',), 'VERB'), ('they', 'fixed screen', ('fix',), 'VERB'), ('they', 'even gave discount on repair', ('give',), 'VERB')]\n",
      "[('this', 'is reliable', ('reliable',), 'ADJ')]\n",
      "[('they', 'offer discounts', ('offer',), 'VERB'), ('they', 'offer fantastic promotions', ('offer',), 'VERB')]\n",
      "[('service', 'be phone', ('phone',), 'OTHER'), ('selection', 'be phone', ('phone',), 'OTHER'), ('service', 'be great', ('great',), 'ADJ'), ('selection', 'be great', ('great',), 'ADJ')]\n",
      "[('they', 'resolved professionally', ('resolve',), 'VERB'), ('they', 'resolved very quickly', ('resolve',), 'VERB'), ('they', 'resolved issue', ('resolve',), 'VERB')]\n",
      "[('I', 'love is organized', ('love', 'be'), 'VERB'), ('they', 'fast attend to customers', ('attend',), 'VERB'), ('store', 'is organized', ('organize',), 'ADJ')]\n",
      "[('you', 're looking for good deals', ('look',), 'VERB')]\n",
      "[('I', 'made right purchase', ('make',), 'VERB'), ('I', 'always leave store', ('leave',), 'VERB')]\n",
      "[('they', 'really know received excellent advice from sales team', ('know', 'receive'), 'VERB'), ('they', 'really know products', ('know',), 'VERB')]\n",
      "[('phone', 'looks new', ('look',), 'VERB')]\n",
      "[('I', 'had to wait over hour', ('have', 'wait'), 'VERB'), ('staff', 'was not apologetic', ('apologetic',), 'ADJ')]\n",
      "[]\n",
      "[('selection', 'is limited', ('limited',), 'ADJ'), ('price', 'are high', ('high',), 'ADJ')]\n",
      "[('one', 'seemed is poor', ('seem', 'be'), 'VERB'), ('service', 'is poor', ('poor',), 'ADJ')]\n",
      "[('phone', 'was defective', ('defective',), 'ADJ')]\n",
      "[('staff', 'was rude', ('rude',), 'ADJ'), ('I', 'm not coming back', ('come',), 'VERB'), ('staff', 'was unhelpful', ('unhelpful',), 'ADJ'), ('I', 'm not coming was rude', ('come', 'be'), 'VERB')]\n",
      "[('they', 'felt like scam', ('feel',), 'VERB'), ('they', 'charged me', ('charge',), 'VERB'), ('they', 'charged extra for services', ('charge',), 'VERB')]\n",
      "[('I', 'had took too long', ('have', 'take'), 'VERB'), ('I', 'had took way', ('have', 'take'), 'VERB')]\n",
      "[('they', 'did not inform me', ('inform',), 'VERB'), ('they', 'did not inform of hidden fees', ('inform',), 'VERB'), ('I', 'bought phone', ('buy',), 'VERB')]\n",
      "[('staff', 'seemed', ('seem',), 'VERB'), ('staff', 'gave incorrect information about phone plan', ('give',), 'VERB')]\n",
      "[('they', 'refused is useless', ('refuse', 'be'), 'VERB'), ('warranty', 'is useless', ('useless',), 'ADJ')]\n",
      "[('they', 'twice gave refund', ('give',), 'VERB'), ('I', 'had to return faulty phone', ('have', 'return'), 'VERB')]\n",
      "[('I', 'very disorganized waited forever', ('wait',), 'VERB')]\n",
      "[]\n",
      "[('they', 'refused to honor promotion', ('refuse', 'honor'), 'VERB')]\n",
      "[('I', 'felt pressured', ('feel',), 'VERB')]\n",
      "[('repair', 'was done poorly', ('do',), 'VERB'), ('phone', 'broke within week', ('break',), 'VERB'), ('phone', 'broke again', ('break',), 'VERB')]\n",
      "[('service', 'was slow', ('slow',), 'ADJ'), ('they', 'need was slow', ('need', 'be'), 'VERB')]\n",
      "[('phone', 'was working after repair', ('work',), 'VERB')]\n",
      "[('phone', 'still has same issue after getting', ('have',), 'VERB')]\n",
      "[('they', 'upsold on phone plan', ('upsold',), 'VERB'), ('they', 'upsold me', ('upsold',), 'VERB')]\n",
      "[('staff', 'was unprofessional', ('unprofessional',), 'ADJ'), ('they', 'did not want to be there', ('want', 'be'), 'VERB')]\n",
      "[('I', 'could not exchange phone', ('exchange',), 'VERB'), ('I', 'could not exchange is awful', ('exchange', 'be'), 'VERB'), ('policy', 'is awful', ('awful',), 'ADJ'), ('I', 'could not exchange despite defects', ('exchange',), 'VERB')]\n",
      "[('they', 'did not apply discount', ('apply',), 'VERB')]\n",
      "[('store', 'was messy', ('messy',), 'ADJ'), ('store', 'was understaffed', ('understaffed',), 'ADJ')]\n",
      "[('warranty', 'just expired', ('expire',), 'VERB'), ('phone', 'broke', ('break',), 'VERB')]\n",
      "[('they', 'kept trying', ('keep', 'try'), 'VERB')]\n",
      "[('job', 'was incomplete', ('incomplete',), 'ADJ'), ('they', 'refused to refund me', ('refuse', 'refund'), 'VERB')]\n",
      "[('representative', 'were rude on phone', ('rude',), 'ADJ')]\n",
      "[('I', 'had to call multiple times', ('have', 'call'), 'VERB')]\n",
      "[('they', 'rushed me', ('rush',), 'VERB'), ('they', 'did not explain anything', ('explain',), 'VERB'), ('they', 'did not explain clearly', ('explain',), 'VERB'), ('they', 'rushed through purchase', ('rush',), 'VERB')]\n",
      "[('support', 'is non', ('non',), 'ADJ')]\n",
      "[('phone', 'stopped working outside return window', ('stop', 'work'), 'VERB'), ('phone', 'stopped quality', ('stop',), 'VERB')]\n",
      "[('store', 'was chaotic', ('chaotic',), 'ADJ'), ('store', 'was with unhelpful staff', ('staff',), 'OTHER'), ('store', 'was with long lines', ('lines',), 'OTHER')]\n",
      "[('it', 'was available', ('available',), 'ADJ'), ('they', 'did not even have after promising me', ('have',), 'VERB'), ('they', 'did not even have phone', ('have',), 'VERB')]\n",
      "[('they', 'lost repair order', ('lose',), 'VERB')]\n",
      "[('I', 'felt overcharged', ('feel',), 'VERB')]\n",
      "[]\n",
      "[('technician', 'damaged during repair', ('damage',), 'VERB'), ('technician', 'damaged phone', ('damage',), 'VERB'), ('they', 'did not take responsibility', ('take',), 'VERB')]\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>ability</th>\n",
       "      <th>aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The staff were incredibly helpful and patient,...</td>\n",
       "      <td>{0: [('staff', 'were helpful', ('helpful',), '...</td>\n",
       "      <td>{'staff': ['staff were helpful.', 'staff were ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I had a great experience purchasing my phone h...</td>\n",
       "      <td>{0: [('process', 'was smooth', ('smooth',), 'A...</td>\n",
       "      <td>{'process': ['process was smooth.', 'process w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Their selection of phones is amazing, and the ...</td>\n",
       "      <td>{0: [('selection', 'is amazing', ('amazing',),...</td>\n",
       "      <td>{'selection': ['selection is amazing.'], 'pric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I appreciate how the staff walked me through s...</td>\n",
       "      <td>{0: [('I', 'appreciate walked me', ('appreciat...</td>\n",
       "      <td>{'staff': {'ADJ': [], 'VERB': [(('appreciate',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great customer service, I left with the phone ...</td>\n",
       "      <td>{0: [('I', 'left with phone', ('leave',), 'VER...</td>\n",
       "      <td>{'question': ['question answered.']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review                                            ability                                            aspects\n",
       "0  The staff were incredibly helpful and patient,...  {0: [('staff', 'were helpful', ('helpful',), '...  {'staff': ['staff were helpful.', 'staff were ...\n",
       "1  I had a great experience purchasing my phone h...  {0: [('process', 'was smooth', ('smooth',), 'A...  {'process': ['process was smooth.', 'process w...\n",
       "2  Their selection of phones is amazing, and the ...  {0: [('selection', 'is amazing', ('amazing',),...  {'selection': ['selection is amazing.'], 'pric...\n",
       "3  I appreciate how the staff walked me through s...  {0: [('I', 'appreciate walked me', ('appreciat...  {'staff': {'ADJ': [], 'VERB': [(('appreciate',...\n",
       "4  Great customer service, I left with the phone ...  {0: [('I', 'left with phone', ('leave',), 'VER...               {'question': ['question answered.']}"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FROM ANALYSIS PATTERN MANUAL: SOON NEED TO COREFERENCE RESOLUTION.\n",
    "\n",
    "# SET IT AS EMPTY STRING BY DEFAULT\n",
    "# def process_aspect_custom(text):\n",
    "#     return []\n",
    "    \n",
    "def process_aspect_custom(data):\n",
    "    # Data is converted into flatten\n",
    "    data = flatten_data(data)\n",
    "\n",
    "    # Get text and subject of data\n",
    "    if len(data) > 0:\n",
    "        text, subjects, keywords, labels = zip(*[(item[0] + ' ' + item[1] + '.', item[0], item[2], item[3]) for item in data])\n",
    "    else: \n",
    "        text, subjects, keywords, labels = ('', '', '', '')\n",
    "\n",
    "\n",
    "    storage = {}\n",
    "    for i in range(len(data)):\n",
    "        s, t, k, l = subjects[i].lower(), text[i], keywords[i], labels[i]\n",
    "        \n",
    "        # Temporary storage\n",
    "        temp = {'ADJ': [],\n",
    "               'VERB': [],\n",
    "               'OTHER': [],}\n",
    "\n",
    "        # Add keywords and text\n",
    "        temp[l].append((k, t))\n",
    "        \n",
    "\n",
    "        if s in ['i', 'this']:\n",
    "            s = 'staff'\n",
    "            storage[s] = temp\n",
    "            # if not storage.get(s):\n",
    "            #     storage[s] = [t]\n",
    "            # else:\n",
    "            #     storage[s].append(t)\n",
    "                \n",
    "        elif s in ['they', 'you']:\n",
    "            s = 'store'\n",
    "            storage[s] = temp\n",
    "            # if not storage.get(s):\n",
    "            #     storage[s] = [t]\n",
    "            # else:\n",
    "            #     storage[s].append(t)\n",
    "                    \n",
    "    \n",
    "    return storage\n",
    "\n",
    "def process_aspect(data, id):\n",
    "    result = process_aspect_rules(data, id)\n",
    "    if len(result) == 0:\n",
    "        result = process_aspect_custom(data)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# aspects = [process_aspect(text, id) for id, text in enumerate(corpus)]\n",
    "aspects = [process_aspect(data, id) for id, data in enumerate(df['ability'].values)]\n",
    "df['aspects'] = aspects\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "715d6f79-d109-43e5-81a4-aaa245ff4c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>ability</th>\n",
       "      <th>aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Upgrading my phone was a breeze thanks to thei...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>The phone I bought here is working perfectly, ...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Always come here for upgrades, they never disa...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Got a great deal on my new phone and an awesom...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Bought a phone here that stopped working withi...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>The phone I purchased here was overpriced comp...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Bought a refurbished phone that had several is...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Im extremely disappointed, will not be coming...</td>\n",
       "      <td>{0: []}</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review  ability aspects\n",
       "9   Upgrading my phone was a breeze thanks to thei...  {0: []}      {}\n",
       "31  The phone I bought here is working perfectly, ...  {0: []}      {}\n",
       "33  Always come here for upgrades, they never disa...  {0: []}      {}\n",
       "43  Got a great deal on my new phone and an awesom...  {0: []}      {}\n",
       "60  Bought a phone here that stopped working withi...  {0: []}      {}\n",
       "72  The phone I purchased here was overpriced comp...  {0: []}      {}\n",
       "96  Bought a refurbished phone that had several is...  {0: []}      {}\n",
       "98  Im extremely disappointed, will not be coming...  {0: []}      {}"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail = df[df.apply(lambda x: len(x['aspects']) == 0, axis=1)]\n",
    "print(len(fail))\n",
    "fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "d58d3f84-8313-49fc-8814-e8f7bb28ed3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [('selection', 'is amazing', ('amazing',), 'ADJ'),\n",
       "  ('price', 'are competitive', ('competitive',), 'ADJ')]}"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2]['ability']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273bff6-8341-4d9f-bc14-874491dfe4a9",
   "metadata": {},
   "source": [
    "**Create Pipeline Meta-Data Aspect based Sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "b5db18a6-0118-4cde-9840-5852745225c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'staff': ['staff were helpful', 'staff were patient']},\n",
       " 1: {'process': ['process was smooth', 'process was quick']},\n",
       " 2: {'selection': ['selection is amazing'],\n",
       "  'price': ['price are competitive']},\n",
       " 3: {'new device': ['I appreciate walked through setting new device']},\n",
       " 4: {'question': ['question answered']},\n",
       " 5: {'staff': ['I could not resist upgrading',\n",
       "   'I could not resist offer amazing deals on phones']},\n",
       " 6: {'phones issue': ['technician fixed phones issue.'],\n",
       "  'technician': ['technician fixed phones issue', 'technician fixed faster']},\n",
       " 7: {'experience': ['experience really know stuff']},\n",
       " 8: {'phone case': ['I found perfect phone case'],\n",
       "  'variety': ['variety was impressive']},\n",
       " 9: {},\n",
       " 10: {'staff': ['staff was knowledgeable']},\n",
       " 11: {'price': ['price were reasonable']},\n",
       " 12: {'staff': ['staff really went mile']},\n",
       " 13: {'service': ['service be excellent']},\n",
       " 14: {'deal': ['deal was friendly'], 'staff': ['staff was friendly']},\n",
       " 15: {'store': ['I love store.'],\n",
       "  'experience': ['buying be experience.'],\n",
       "  'buying': ['buying be experience'],\n",
       "  'fixing': ['fixing be experience']},\n",
       " 16: {'deal': ['I got good deal on old phone'],\n",
       "  'old phone': ['I got good deal on old phone']},\n",
       " 17: {'service': ['service are quick', 'service are reliable']},\n",
       " 18: {'phone': ['staff was helpful in setting phone'],\n",
       "  'staff': ['staff was helpful in setting phone']},\n",
       " 19: {'staff': ['I m satisfied']},\n",
       " 20: {'staff': ['staff was patient with questions'],\n",
       "  'variety': ['variety was patient with questions']},\n",
       " 21: {'process': ['process was simple']},\n",
       " 22: {'phone': ['they helped choose phone']},\n",
       " 23: {'phone': ['phone was fixed in minutes']},\n",
       " 24: {'service': ['service is outstanding']},\n",
       " 25: {'store': ['store hands']},\n",
       " 26: {'staff': ['staff made']},\n",
       " 27: {'deal': ['they helped get great deal']},\n",
       " 28: {'warranty service': ['store has fantastic warranty service'],\n",
       "  'store': ['store has fantastic warranty service']},\n",
       " 29: {'lot': ['I learned lot about phone features.'],\n",
       "  'phone features': ['I learned lot about phone features.'],\n",
       "  'staff': ['staff was informative']},\n",
       " 30: {'variety': ['variety be excellent'], 'store': ['store be excellent']},\n",
       " 31: {},\n",
       " 32: {'phone': ['they were quick in setting phone']},\n",
       " 33: {},\n",
       " 34: {'layout': ['layout is easy'], 'staff': ['staff are ready']},\n",
       " 35: {'lot': ['they helped save lot']},\n",
       " 36: {'service': ['one provides best service.'],\n",
       "  'many phone stores': ['I been to many phone stores'],\n",
       "  'one': ['one provides best service']},\n",
       " 37: {'service': ['service is notch', 'service is notch']},\n",
       " 38: {'store': ['they not fail recommend store.']},\n",
       " 39: {'time': ['staff took time'], 'staff': ['staff took time']},\n",
       " 40: {'place': ['place be amazing']},\n",
       " 41: {'plan': ['plan is worth']},\n",
       " 42: {'staff': ['I appreciate were able'], 'store': ['they were able']},\n",
       " 43: {},\n",
       " 44: {'staff': ['staff was accommodating']},\n",
       " 45: {'experience': ['I had great experience with program']},\n",
       " 46: {'service': ['service was quick', 'service was efficient']},\n",
       " 47: {'extra charge': ['they even helped transfer without extra charge.']},\n",
       " 48: {'phone': ['phone has working flawlessly']},\n",
       " 49: {'discount': ['they even gave discount on repair']},\n",
       " 50: {'this': ['this is reliable']},\n",
       " 51: {'store': ['they offer discounts', 'they offer fantastic promotions']},\n",
       " 52: {'service': ['service be phone', 'service be great'],\n",
       "  'selection': ['selection be phone', 'selection be great']},\n",
       " 53: {'issue': ['they resolved issue']},\n",
       " 54: {'store': ['store is organized']},\n",
       " 55: {'store': ['you re looking for good deals']},\n",
       " 56: {'purchase': ['I made right purchase.'],\n",
       "  'store': ['I always leave store']},\n",
       " 57: {'store': ['they really know received excellent advice from sales team',\n",
       "   'they really know products']},\n",
       " 58: {'phone': ['phone looks new']},\n",
       " 59: {'staff': ['staff was not apologetic']},\n",
       " 60: {},\n",
       " 61: {'selection': ['selection is limited'], 'price': ['price are high']},\n",
       " 62: {'one': ['one seemed is poor'], 'service': ['service is poor']},\n",
       " 63: {'phone': ['phone was defective']},\n",
       " 64: {'staff': ['staff was rude', 'staff was unhelpful']},\n",
       " 65: {'store': ['they felt like scam',\n",
       "   'they charged me',\n",
       "   'they charged extra for services']},\n",
       " 66: {'staff': ['I had took too long', 'I had took way']},\n",
       " 67: {'phone': ['I bought phone']},\n",
       " 68: {'phone plan': ['staff gave incorrect information about phone plan'],\n",
       "  'staff': ['staff seemed',\n",
       "   'staff gave incorrect information about phone plan']},\n",
       " 69: {'warranty': ['warranty is useless']},\n",
       " 70: {'refund': ['they twice gave refund.'],\n",
       "  'phone': ['I had to return faulty phone']},\n",
       " 71: {'staff': ['I very disorganized waited forever']},\n",
       " 72: {},\n",
       " 73: {'promotion': ['they refused to honor promotion']},\n",
       " 74: {'staff': ['I felt pressured']},\n",
       " 75: {'repair': ['repair was done poorly'],\n",
       "  'phone': ['phone broke within week', 'phone broke again']},\n",
       " 76: {'service': ['service was slow']},\n",
       " 77: {'phone': ['phone was working after repair']},\n",
       " 78: {'same issue': ['phone still has same issue after getting'],\n",
       "  'phone': ['phone still has same issue after getting']},\n",
       " 79: {'phone plan': ['they upsold on phone plan.']},\n",
       " 80: {'staff': ['staff was unprofessional']},\n",
       " 81: {'phone': ['I could not exchange phone.'], 'policy': ['policy is awful']},\n",
       " 82: {'discount': ['they did not apply discount']},\n",
       " 83: {'store': ['store was messy', 'store was understaffed']},\n",
       " 84: {'warranty': ['warranty just expired'], 'phone': ['phone broke']},\n",
       " 85: {'store': ['they kept trying']},\n",
       " 86: {'job': ['job was incomplete']},\n",
       " 87: {'representative': ['representative were rude on phone']},\n",
       " 88: {'multiple times': ['I had to call multiple times']},\n",
       " 89: {'store': ['they rushed me',\n",
       "   'they did not explain anything',\n",
       "   'they did not explain clearly',\n",
       "   'they rushed through purchase']},\n",
       " 90: {'support': ['support is non']},\n",
       " 91: {'return window': ['phone stopped working outside return window.'],\n",
       "  'phone': ['phone stopped working outside return window',\n",
       "   'phone stopped quality']},\n",
       " 92: {'staff': ['store was with unhelpful staff.'],\n",
       "  'long lines': ['store was with long lines'],\n",
       "  'store': ['store was chaotic',\n",
       "   'store was with unhelpful staff',\n",
       "   'store was with long lines']},\n",
       " 93: {'phone': ['they did not even have phone']},\n",
       " 94: {'repair order': ['they lost repair order']},\n",
       " 95: {'staff': ['I felt overcharged']},\n",
       " 96: {},\n",
       " 97: {'technician': ['technician damaged during repair',\n",
       "   'technician damaged phone']},\n",
       " 98: {}}"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate code\n",
    "\n",
    "def pipeline_meta_data(input_):\n",
    "    result = {}\n",
    "    for id, data in enumerate(input_):\n",
    "        temp = {}\n",
    "        items = process_aspect(data, id)\n",
    "        if len(items) > 0:\n",
    "            for aspect, data in items.items():\n",
    "                if not temp.get(aspect):\n",
    "                    temp[aspect] = data\n",
    "                else:\n",
    "                    temp[aspect] += data\n",
    "        result[id] = temp\n",
    "\n",
    "    return result\n",
    "\n",
    "meta_data = pipeline_meta_data(df['ability'].values)\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "7bf079d1-f1bd-4eaf-97d9-73dde8f22756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 91 entries, 0 to 90\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   index    91 non-null     int64 \n",
      " 1   review   91 non-null     object\n",
      " 2   ability  91 non-null     object\n",
      " 3   aspects  91 non-null     object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 3.0+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review</th>\n",
       "      <th>ability</th>\n",
       "      <th>aspects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The staff were incredibly helpful and patient,...</td>\n",
       "      <td>{0: [('staff', 'were helpful', ('helpful',), '...</td>\n",
       "      <td>{'staff': ['staff were helpful', 'staff were p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I had a great experience purchasing my phone h...</td>\n",
       "      <td>{0: [('process', 'was smooth', ('smooth',), 'A...</td>\n",
       "      <td>{'process': ['process was smooth', 'process wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Their selection of phones is amazing, and the ...</td>\n",
       "      <td>{0: [('selection', 'is amazing', ('amazing',),...</td>\n",
       "      <td>{'selection': ['selection is amazing'], 'price...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I appreciate how the staff walked me through s...</td>\n",
       "      <td>{0: [('I', 'appreciate walked me', ('appreciat...</td>\n",
       "      <td>{'new device': ['I appreciate walked through s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Great customer service, I left with the phone ...</td>\n",
       "      <td>{0: [('I', 'left with phone', ('leave',), 'VER...</td>\n",
       "      <td>{'question': ['question answered']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                             review  \\\n",
       "0      0  The staff were incredibly helpful and patient,...   \n",
       "1      1  I had a great experience purchasing my phone h...   \n",
       "2      2  Their selection of phones is amazing, and the ...   \n",
       "3      3  I appreciate how the staff walked me through s...   \n",
       "4      4  Great customer service, I left with the phone ...   \n",
       "\n",
       "                                             ability  \\\n",
       "0  {0: [('staff', 'were helpful', ('helpful',), '...   \n",
       "1  {0: [('process', 'was smooth', ('smooth',), 'A...   \n",
       "2  {0: [('selection', 'is amazing', ('amazing',),...   \n",
       "3  {0: [('I', 'appreciate walked me', ('appreciat...   \n",
       "4  {0: [('I', 'left with phone', ('leave',), 'VER...   \n",
       "\n",
       "                                             aspects  \n",
       "0  {'staff': ['staff were helpful', 'staff were p...  \n",
       "1  {'process': ['process was smooth', 'process wa...  \n",
       "2  {'selection': ['selection is amazing'], 'price...  \n",
       "3  {'new device': ['I appreciate walked through s...  \n",
       "4                {'question': ['question answered']}  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.apply(lambda x: len(x['aspects']) != 0, axis=1)].copy().reset_index()\n",
    "\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "ee0ae1b8-4b4b-417a-ae72-fb0f32a8b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save dictionary to a JSON file\n",
    "# with open('data-1.json', 'w') as json_file:\n",
    "#     json.dump(meta_data, json_file, indent=4)  # 'indent=4' makes the JSON pretty-printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "ea8475d9-6105-482f-bc59-653576a0ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel(\"example.xlsx\", index=False)\n",
    "# df.to_csv(\"example.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
