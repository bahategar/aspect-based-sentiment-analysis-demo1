{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "ad554876-b6f1-4d92-ba0a-f7e185db1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "24604080-dd71-46c7-b4c7-3e8eb769b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "67c8d9e3-ef2e-4317-ac3e-a63e53aaece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   reviewer_id  100 non-null    int64 \n",
      " 1   review_time  100 non-null    object\n",
      " 2   review       100 non-null    object\n",
      " 3   rating       100 non-null    int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 3.3+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3 months ago</td>\n",
       "      <td>Why does it look like someone spit on my food?...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>It'd McDonalds. It is what it is as far as the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Made a mobile order got to the speaker and che...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>a month ago</td>\n",
       "      <td>My mc. Crispy chicken sandwich was ÃÂ¯ÃÂ¿ÃÂ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>I repeat my order 3 times in the drive thru, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewer_id   review_time  \\\n",
       "0            1  3 months ago   \n",
       "1            2    5 days ago   \n",
       "2            3    5 days ago   \n",
       "3            4   a month ago   \n",
       "4            5  2 months ago   \n",
       "\n",
       "                                              review  rating  \n",
       "0  Why does it look like someone spit on my food?...       1  \n",
       "1  It'd McDonalds. It is what it is as far as the...       4  \n",
       "2  Made a mobile order got to the speaker and che...       1  \n",
       "3  My mc. Crispy chicken sandwich was ÃÂ¯ÃÂ¿ÃÂ...       5  \n",
       "4  I repeat my order 3 times in the drive thru, a...       1  "
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./sample-mcd.csv', encoding='latin1')\n",
    "df = df[['reviewer_id', 'review_time', 'review', 'rating']]\n",
    "\n",
    "df['rating'] = df['rating'].apply(lambda x: int(x.split(\" \")[0]))\n",
    "\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "9c459340-037e-4d5b-956d-57d5ef23a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bing Liu's opinion word dictionary\n",
    "bing_liu_opinion_words = set()  # Add the actual list of opinion words here\n",
    "\n",
    "# Function to load opinion words from Bing Liu lexicon\n",
    "def load_opinion_words(filepath):\n",
    "    global bing_liu_opinion_words\n",
    "    temp = pd.read_table(filepath, comment=';', header=None)[0].to_list()\n",
    "    bing_liu_opinion_words = bing_liu_opinion_words.union(set(temp))\n",
    "\n",
    "\n",
    "# Load opinion words\n",
    "current_dir = os.getcwd()\n",
    "load_opinion_words(os.path.join(current_dir, 'util/opinion-lexicon-English/negative-words.txt'))\n",
    "load_opinion_words(os.path.join(current_dir, 'util/opinion-lexicon-English/positive-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "ebd2f99e-753a-422b-a912-b41e73c404b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "##========== PREPARATION TEXT ===========##\n",
    "\n",
    "# Contraction\n",
    "def expand_contractions(sentence, contraction_mapping=CONTRACTION_MAP):\n",
    "    \"\"\"\n",
    "    Expand the contractions in a sentence. For example don't => do not.\n",
    "    \n",
    "    Paramters:\n",
    "    sentence (str): The input sentence to clean.\n",
    "    contraction_mapping (dict): A dictionary for mapping contractions.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    str: The expanded contraction sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expanded_match(contraction):\n",
    "        \"\"\"\n",
    "        Filter for expanding the matched contraction.\n",
    "        \n",
    "        Parameters:\n",
    "        contraction (str): The input of contraction\n",
    "        \n",
    "        Returns:\n",
    "        str: The expanded contraction.\n",
    "        \"\"\"\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n",
    "        \n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_sentence = contractions_pattern.sub(expanded_match, sentence)\n",
    "    return expanded_sentence\n",
    "\n",
    "\n",
    "def remove_extra_spaces(sentence):\n",
    "    # Use regex to replace multiple spaces with a single space\n",
    "    return re.sub(r'\\s+', ' ', sentence).strip()\n",
    "\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"\n",
    "    Remove all non-ASCII characters from the text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to clean.\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned text with only ASCII characters.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return ''.join([char for char in text if ord(char) < 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "b3bac39d-1e11-4eb5-8d71-267cfb2ec94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##=========== EXTRACT ASPECT ============##\n",
    "# Cross product two lists\n",
    "def cross_product_str(first, second):\n",
    "    \"\"\"\n",
    "    Do cross product\n",
    "\n",
    "    parameters\n",
    "    -----------\n",
    "    first: list/string\n",
    "    second: list/string\n",
    "\n",
    "    return: list of string\n",
    "    \"\"\"\n",
    "    temp = []\n",
    "    if type(first) == str:\n",
    "        first = [first]\n",
    "    if type(second) == str:\n",
    "        second = [second]\n",
    "    for i in first:\n",
    "        for j in second:\n",
    "            text = (i + ' ' + j).strip()\n",
    "            temp.append(text)\n",
    "    return temp\n",
    "\n",
    "def cross_product_tuple(first, second):\n",
    "    \"\"\"\n",
    "    Do cross product\n",
    "\n",
    "    parameters\n",
    "    -----------\n",
    "    first: list/string\n",
    "    second: list/string\n",
    "\n",
    "    return: list of tuple\n",
    "    \"\"\"\n",
    "    temp = []\n",
    "    if type(first) == str:\n",
    "        first = [first]\n",
    "    if type(second) == str:\n",
    "        second = [second]\n",
    "    for i in first:\n",
    "        for j in second:\n",
    "            temp.append((i, j))\n",
    "    return temp\n",
    "\n",
    "# Get neglection text\n",
    "def get_neglect(token):\n",
    "    for t in token.children:\n",
    "        if (t.dep_ == 'neg') or (t.dep_ == 'det' and t.text.lower() == 'no'):\n",
    "            return 'not'\n",
    "    return ''\n",
    "\n",
    "# Get token specific pos tag\n",
    "def get_token_pos(token, pos):\n",
    "    if type(pos) == str:\n",
    "        pos = [pos]\n",
    "    for t in token.children:\n",
    "        if t.pos_ in pos:\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_token_pos_left(token, pos):\n",
    "    if type(pos) == str:\n",
    "        pos = [pos]\n",
    "    for t in token.children:\n",
    "        if (t.pos_ in pos) and (t.i < token.i):\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_token_pos_right(token, pos):\n",
    "    if type(pos) == str:\n",
    "        pos = [pos]\n",
    "    for t in token.children:\n",
    "        if (t.pos_ in pos) and (t.i > token.i):\n",
    "            return t\n",
    "    return None\n",
    "    \n",
    "# Get token spcific dependency\n",
    "def get_token_dep(token, dep):\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if t.dep_ in dep:\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_token_dep_left(token, dep):\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if (t.dep_ in dep) and (t.i < token.i):\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_token_dep_right(token, dep):\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if (t.dep_ in dep) and (t.i > token.i):\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_all_token_dep(token, dep):\n",
    "    result = []\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if t.dep_ in dep:\n",
    "            result.append(t)\n",
    "    return result\n",
    "\n",
    "def get_all_token_dep_right(token, dep):\n",
    "    result = []\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if t.dep_ in dep and t.i > token.i:\n",
    "            result.append(t)\n",
    "    return result\n",
    "\n",
    "def get_all_token_dep_left(token, dep):\n",
    "    result = []\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if t.dep_ in dep and t.i < token.i:\n",
    "            result.append(t)\n",
    "    return result\n",
    "\n",
    "# Get token coordinate conjugation\n",
    "# def get_token_cc(token):\n",
    "#     for t in token.children:\n",
    "#         if t.dep_ == 'cc':\n",
    "#             return t\n",
    "#     return None\n",
    "\n",
    "# Crawling all possibile conjunct\n",
    "def extract_conj(token, neglect=False, lemma=False):\n",
    "    result = []\n",
    "    current = get_token_dep(token, dep='conj')\n",
    "    while current:\n",
    "        if neglect:\n",
    "            neg = get_neglect(current)\n",
    "            # If lemma\n",
    "            if lemma:\n",
    "                text = (neg + ' ' + current.lemma_).strip()\n",
    "            else:\n",
    "                text = (neg + ' ' + current.text).strip()\n",
    "                    \n",
    "            result.append(text)\n",
    "        else:\n",
    "            result.append(current.text)\n",
    "        current = get_token_dep(current, dep='conj')\n",
    "\n",
    "    return result\n",
    "\n",
    "# Get sentences that include coordinating conjunction and its conjunct\n",
    "def get_text_conj(token):\n",
    "    # Get all sentence of series include the conjugation\n",
    "    tokens = [token]\n",
    "    # Get all token\n",
    "    tokens += extract_conj(token, all_token=True)\n",
    "\n",
    "    text = ''\n",
    "    for i, t in enumerate(tokens):\n",
    "        text = text + t.text\n",
    "        if i < len(tokens) - 1:\n",
    "            if t.dep_ == 'cc':\n",
    "                text += ' '\n",
    "            else:\n",
    "                text += ', '\n",
    "\n",
    "    # text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Crawling all possibile adjective pre-modifier\n",
    "def extract_pre_adj(token, lemma=False):\n",
    "    result = []\n",
    "    current_idx = token.i\n",
    "    for child in token.children:\n",
    "        if child.pos_ == 'ADJ' and child.i < current_idx:\n",
    "            if lemma:\n",
    "                result.append((child.lemma_, child.i))\n",
    "            else:\n",
    "                result.append((child.text, child.i))\n",
    "\n",
    "    # Sort by its index\n",
    "    result = sorted(result, key=lambda x: x[1])\n",
    "\n",
    "    # Return only list of string\n",
    "    result = [item[0] for item in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "# Crawling all possible adjective post-modifier\n",
    "def extract_post_adj(token, lemma=False):\n",
    "    result = []\n",
    "    current_idx = token.i\n",
    "    for child in token.children:\n",
    "        if child.pos_ == 'ADJ' and child.i > current_idx:\n",
    "            if lemma:\n",
    "                result.append((child.lemma_, child.i))\n",
    "            else:\n",
    "                result.append((child.text, child.i))\n",
    "\n",
    "    # Sort by its index\n",
    "    result = sorted(result, key=lambda x: x[1])\n",
    "\n",
    "    # Return only list of string\n",
    "    result = [item[0] for item in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "# Crawling all possible adverb\n",
    "def extract_adv(token, lemma=True):\n",
    "    result_pre = []\n",
    "    result_post = []\n",
    "    current_idx = token.i\n",
    "    for child in token.children:\n",
    "        # If pre-position adverb\n",
    "        if child.pos_ == 'ADV' and child.i < current_idx:\n",
    "            if lemma:\n",
    "                result_pre.append((child.lemma_, child.i))\n",
    "            else:\n",
    "                result_pre.append((child.text, child.i))\n",
    "\n",
    "        # If post-position adverb\n",
    "        if child.pos_ == 'ADV' and child.i > current_idx:\n",
    "            if lemma:\n",
    "                result_post.append((child.lemma_, child.i))\n",
    "            else:\n",
    "                result_post.append((child.text, child.i))\n",
    "\n",
    "    # Sort by its index\n",
    "    result_pre = sorted(result_pre, key=lambda x: x[1])\n",
    "    result_post = sorted(result_post, key=lambda x: x[1])\n",
    "\n",
    "    # Return only list of string\n",
    "    result_pre = [item[0] for item in result_pre]\n",
    "    result_post = [item[0] for item in result_post]\n",
    "\n",
    "    return result_pre, result_post\n",
    "\n",
    "# Crawling preposition phrase after particullar token\n",
    "def crawling_after_token_prep_phrase(token, neglect=False):\n",
    "    result = []\n",
    "    basis_idx = token.i\n",
    "    prep = get_all_token_dep(token, dep='prep')\n",
    "    if prep:\n",
    "        # If contain children: dep pcomp dep VERB pos tag; Until reach dobj or pobj\n",
    "        for p in prep:\n",
    "            prep_idx = p.i\n",
    "            # If the preposition on the left basis token index, continue\n",
    "            if basis_idx > prep_idx:\n",
    "                continue\n",
    "                \n",
    "            current = get_token_dep(p, dep=['pcomp', 'dobj', 'pobj'])\n",
    "            # Store objects\n",
    "            obj = []\n",
    "            # Store complement\n",
    "            comp = [p.text]\n",
    "            while current:\n",
    "                text = current.text\n",
    "                # If current token is object, get the pre-modifier adjective\n",
    "                if current.dep_ in ['dobj', 'pobj']:\n",
    "                    pre_adj = ' '.join(extract_pre_adj(current))\n",
    "                    obj += cross_product_str(pre_adj, text)\n",
    "\n",
    "                    # Extract conjunct object\n",
    "                    obj_conj = extract_conj(current, neglect=neglect)\n",
    "                    if len(obj_conj) > 0:\n",
    "                        obj += obj_conj\n",
    "                else:\n",
    "                    comp = cross_product_str(comp, text)\n",
    "                    \n",
    "                current = get_token_dep(current, dep=['pcomp', 'dobj', 'pobj'])\n",
    "\n",
    "            result += cross_product_str(comp, obj)\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "# # Get the sentence point mapper\n",
    "# def get_sentence_mapper():\n",
    "#     sentence_point = {}\n",
    "#     for i, s in enumerate(doc.sents):\n",
    "#         sentence_point[i] = (s.start, s.end)\n",
    "#     return sentence_point\n",
    "    \n",
    "# # Get location sentence\n",
    "# sentence_mapper = get_sentence_mapper(doc)\n",
    "\n",
    "def get_sentence_location(mapper, position):\n",
    "    for s in mapper.keys():\n",
    "        interval = mapper[s]\n",
    "        if position >= interval[0] and position < interval[1]:\n",
    "            return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "1a20cb71-973a-44d5-8e09-79b2d69bdc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all raw aspects\n",
    "def get_raw_aspects(doc):\n",
    "    # Define global variables\n",
    "    global bing_liu_opinion_words\n",
    "    \n",
    "    # Define local variables\n",
    "    storage = []\n",
    "\n",
    "    # Define helper function\n",
    "    def is_abnormal_noun(text):\n",
    "        \"\"\"\n",
    "            If text only contains special character/number/both OR total length less than 3 it specified as abnormal.\n",
    "        \"\"\"\n",
    "        if re.match(r'^[0-9\\W]+$', token.text) or len(token.text) < 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Going through all token\n",
    "    for idx, token in enumerate(doc):\n",
    "        # Make sure the text is not abnormal\n",
    "        if is_abnormal_noun(token.text):\n",
    "            continue\n",
    "\n",
    "        # If the word is noun and preceded by an adjective\n",
    "        if idx != 0 and (token.pos_ == 'NOUN' and doc[idx - 1].pos_ == 'ADJ'):\n",
    "            # If the adjective is an opinion\n",
    "            if doc[idx - 1].text not in bing_liu_opinion_words:\n",
    "                # Concatenate adj + word then add to storage\n",
    "                text = doc[idx - 1].text + ' ' + token.text\n",
    "                storage.append((text, idx - 1, idx + 1))\n",
    "            else:\n",
    "                # Else, add noun only\n",
    "                text = token.text\n",
    "                storage.append((text, idx, idx + 1))\n",
    "            continue\n",
    "            \n",
    "        # If the word is noun and preceded by another noun\n",
    "        if idx != 0 and (token.pos_ == 'NOUN' and doc[idx - 1].pos_ == 'NOUN'):\n",
    "            text = doc[idx - 1].text + ' ' + token.text\n",
    "            storage.append((text, idx - 1, idx + 1))\n",
    "            continue\n",
    "\n",
    "        # If the word is noun and direct object\n",
    "        if token.pos_ == 'NOUN' and (token.dep_ == 'dobj'):\n",
    "            text = token.text\n",
    "            storage.append((text, idx, idx + 1))\n",
    "            continue\n",
    "    \n",
    "        # If the word is noun and a subject of sentence\n",
    "        if token.pos_ == 'NOUN' and token.dep_ == 'nsubj':\n",
    "            text = token.text\n",
    "            storage.append((text, idx, idx + 1))\n",
    "            continue\n",
    "    \n",
    "        # If the word is noun and a conj of another noun\n",
    "        if (token.pos_ == 'NOUN' and token.dep_ == 'conj') and (token.head.pos_ == 'NOUN'):\n",
    "            text = token.text\n",
    "            storage.append((text, idx, idx + 1))\n",
    "            continue\n",
    "    \n",
    "        # # If the sentence contains SUBJECT VERB, then makes it true\n",
    "        # if token.dep_ == 'nsubj' and token.head.pos_ == 'VERB':\n",
    "        #     is_contain_subject_verb = True\n",
    "    \n",
    "        # # If token is word that contain pre-modifier\n",
    "        # if (token.dep_ == 'amod' and token.head.pos_ == 'NOUN'):\n",
    "        #     if token.head.i != idx + 1:\n",
    "        #         continue\n",
    "        #     text = token.text + ' ' + token.head.text\n",
    "        #     storage.append((text, idx, token.head.i + 1))\n",
    "    \n",
    "        # # If token is word that contain post-modifier\n",
    "        # if (token.dep_ == 'pobj' and token.pos_ == 'NOUN'):\n",
    "        #     if token.head.dep_ == 'prep' and token.head.head.pos_ == 'NOUN':\n",
    "        #         text = token.head.head.text + ' ' + token.head.text + ' ' + token.text\n",
    "        #         start = token.head.head.i\n",
    "        #         storage.append((text, start, idx + 1))\n",
    "            \n",
    "        \n",
    "        # If token is adverb modifier and its head is NOUN then store it.\n",
    "        if (token.dep_ == 'advmod' and token.head.pos_ == 'NOUN'):\n",
    "            text = token.head.text + ' ' + token.text\n",
    "            storage.append((text, token.head.i, idx + 1))\n",
    "            # adv_adj_mod.append((text, idx, idx + 1))\n",
    "\n",
    "    # Sort storage\n",
    "    storage = list(set(storage))\n",
    "    storage = sorted(storage, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "    return storage\n",
    "\n",
    "# Prunning raw aspect\n",
    "def prunning_aspect(list_, doc):\n",
    "    # Define local variables\n",
    "    drop_idx = []\n",
    "    storage = {}\n",
    "    \n",
    "    # Get sentence mapper and prepare storage\n",
    "    sentence_points = {}\n",
    "    for i, s in enumerate(doc.sents):\n",
    "        sentence_points[i] = (s.start, s.end)\n",
    "        storage[i] = []\n",
    "\n",
    "    for idx, item in enumerate(list_):\n",
    "        # As long as current idx does not more than maximum list_ index\n",
    "        if idx != len(list_) - 1:\n",
    "            # Get the next item\n",
    "            next_item = list_[idx + 1]\n",
    "            # If current item start position and next item end position are overlapping\n",
    "            if item[-1] - 1 == next_item[1]:\n",
    "                # We merge the text based on last text in current item and first text in next item\n",
    "                append_text = ' '.join(next_item[0].split()[1:])\n",
    "                # Update next item values\n",
    "                new_text = item[0] + ' ' + append_text\n",
    "                new_start = item[1]\n",
    "                new_end = next_item[-1]\n",
    "                list_[idx + 1] = (new_text, new_start, new_end)\n",
    "\n",
    "                # Add current index into dropped index list\n",
    "                drop_idx.append(idx)\n",
    "            \n",
    "            # If current item start position = next item end position (They are next to each other)\n",
    "            if item[-1] == next_item[1]:\n",
    "                # Update the next value (do not have to merge the text based on specific text).\n",
    "                new_text = item[0] + ' ' + next_item[0]\n",
    "                new_start = item[1]\n",
    "                new_end = next_item[-1]\n",
    "                list_[idx + 1] = (new_text, new_start, new_end)\n",
    "\n",
    "                # Add current index into dropped index list\n",
    "                drop_idx.append(idx)\n",
    "                \n",
    "    list_ = [list_[i] for i in range(len(list_)) if i not in drop_idx]\n",
    "\n",
    "    # Create return as mapper\n",
    "    for i, s in enumerate(list_):\n",
    "        text, start, end = s\n",
    "        sentence_location = get_sentence_location(sentence_points, start)\n",
    "        # Update value and store text as lowercase\n",
    "        storage[sentence_location].append(text.lower())\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "c1620efd-e9df-4ee0-a48f-5a8c47783170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mapper pronouns-antecedents (subject only)\n",
    "def get_mapper_pron_ant(doc):\n",
    "    \n",
    "    def locate_subject_ant_pron(_doc):\n",
    "        # Locate potential antecedents and pronouns (subject only)\n",
    "    \n",
    "        # Define local variables\n",
    "        antecedents = []\n",
    "        pron = []\n",
    "        prohibit_pronouns = [ 'i', 'you', 'me', 'my', 'mine']\n",
    "    \n",
    "        # Get sentence mapper\n",
    "        sentence_points = {}\n",
    "        for i, s in enumerate(_doc.sents):\n",
    "            sentence_points[i] = (s.start, s.end)\n",
    "        \n",
    "        for token in _doc:\n",
    "            # Condition potential antecedents\n",
    "            # If the token is not pronouns and it's a subject\n",
    "            if (token.pos_ in ['NOUN', 'PROPN']) and (token.dep_ == 'nsubj'):\n",
    "                start = token.i\n",
    "                end = start + 1\n",
    "                location_sentence = get_sentence_location(sentence_points, start)\n",
    "                antecedents.append((token, start, location_sentence))\n",
    "                # Check is there any conj\n",
    "                # antecedents += extract_conj(token, only_token=True)\n",
    "        \n",
    "            # if (token.pos_ != 'PRON') and (token.dep_ == 'dobj' or token.dep_ == 'pobj'):\n",
    "            #     start = token.i\n",
    "            #     end = start + 1\n",
    "            #     location_sentence = get_sentence_location(sentence_points, start)\n",
    "            #     antecedents.append((token, start, location_sentence))\n",
    "            #     # Check is there any conj\n",
    "            #     # antecedents += extract_conj(token, only_token=True)    \n",
    "        \n",
    "            # Condition potential pronouns\n",
    "            # Rule 1\n",
    "            # If pron is subject (it could be same sentence or previously)\n",
    "            if (token.pos_ == 'PRON' and token.text.lower() not in prohibit_pronouns) and (token.dep_ == 'nsubj'):\n",
    "                # start = est_loc - len(token.text)\n",
    "                # end = est_loc\n",
    "                # start = ex.index(token.text)\n",
    "                # end = start + len(token.text)\n",
    "                start = token.i\n",
    "                end = start + 1\n",
    "                location_sentence = get_sentence_location(sentence_points, start)\n",
    "                pron.append((token, start, location_sentence))\n",
    "                \n",
    "            # Rule 2\n",
    "            # If pron is possesion (ant is subject in the same sentence)\n",
    "            if (token.pos_ == 'PRON' and token.text.lower() not in prohibit_pronouns) and (token.dep_ == 'poss'):\n",
    "                # start = est_loc - len(token.text)\n",
    "                # end = est_loc\n",
    "                start = token.i\n",
    "                end = start + 1\n",
    "                location_sentence = get_sentence_location(sentence_points, start)\n",
    "                pron.append((token, start, location_sentence))\n",
    "        \n",
    "            # Rule 3\n",
    "            # If pron is object\n",
    "            # if (token.pos_ == 'PRON') and (token.dep_ == 'dobj' or token.dep_ == 'pobj'):\n",
    "            #     start = token.i\n",
    "            #     end = start + 1\n",
    "            #     location_sentence = get_sentence_location(sentence_points, start)\n",
    "            #     pron.append((token, start, location_sentence))\n",
    "        \n",
    "        \n",
    "        return (antecedents, pron)\n",
    "\n",
    "    # Filter sentence\n",
    "    def filter_sentence(_list, location):\n",
    "        temp = []\n",
    "        for e in _list:\n",
    "            if e[-1] == location:\n",
    "                temp.append(e)\n",
    "        return temp\n",
    "\n",
    "    # Define local variable\n",
    "    mapper = {}\n",
    "    result = None\n",
    "\n",
    "    antecedents, pronouns = locate_subject_ant_pron(doc)\n",
    "    \n",
    "    if len(pronouns) > 0:\n",
    "        for p in pronouns:\n",
    "            # Current status\n",
    "            is_success = False\n",
    "\n",
    "            # Get current text, index token, and location sentence token\n",
    "            token_pron, index_pron, sent_pron = p\n",
    "            current_sentence = sent_pron\n",
    "            \n",
    "            while current_sentence > -1:\n",
    "                # Get the antecedents\n",
    "                filter_antecedents = filter_sentence(antecedents, current_sentence)\n",
    "\n",
    "                # If the filter antecedents exist\n",
    "                if len(filter_antecedents) > 0:\n",
    "                    for ant in filter_antecedents:\n",
    "                        token_ant, index_ant, sent_ant = ant\n",
    "                        # If antecedent is subject and pronouns is subject or possession and antecedent on the left of pronoun\n",
    "                        if ('subj' in token_ant.dep_) and ('subj' in token_pron.dep_ or 'poss' in token_pron.dep_) and (index_ant < index_pron):\n",
    "                            mapper[index_pron] = index_ant\n",
    "                            is_success = True\n",
    "                            break\n",
    "                        # if ('obj' in token_ant.dep_ and 'obj' in token_pron.dep_) and (index_ant < index_pron):\n",
    "                        #     mapper[index_pron] = index_ant\n",
    "                        #     is_success = True\n",
    "                        #     break\n",
    "                \n",
    "                # If already success, break it.\n",
    "                if is_success:\n",
    "                    break\n",
    "                    \n",
    "                current_sentence -= 1\n",
    "\n",
    "    return mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "3331957f-9c75-4326-a91b-04183d65357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_abilities(doc):\n",
    "    storage = {}\n",
    "    first_person_pronouns = [ 'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
    "\n",
    "    # Get sentence mapper and prepare storage\n",
    "    sentence_points = {}\n",
    "    for i, s in enumerate(doc.sents):\n",
    "        sentence_points[i] = (s.start, s.end)\n",
    "        storage[i] = []\n",
    "\n",
    "    # Get mapper pronoun and antecedents\n",
    "    mapper_pron_ant = get_mapper_pron_ant(doc)\n",
    "        \n",
    "    for idx, token in enumerate(doc):\n",
    "        subjects = []\n",
    "        abilities = []\n",
    "    \n",
    "        # If the token is verb\n",
    "        if token.pos_ == 'VERB':\n",
    "            \n",
    "            for t in token.children:\n",
    "                # Check if the token children contain subject.\n",
    "                if t.dep_ == 'nsubj':\n",
    "                    # Get current child index and text\n",
    "                    current_idx = t.i\n",
    "                    text = t.lemma_\n",
    "                    # If the current child is first person pronoun\n",
    "                    if t.pos_ == 'PRON' and t.text.lower() in first_person_pronouns:\n",
    "                        # The token is reference to \"the user\"\n",
    "                        text = 'the user'\n",
    "                    # If the current child is pronoun and its current_idx in mapper_pron_ant\n",
    "                    elif t.pos_ == 'PRON' and current_idx in mapper_pron_ant.keys():\n",
    "                        # Change current token\n",
    "                        idx_map = mapper_pron_ant[current_idx]\n",
    "                        t = doc[idx_map]\n",
    "                        text = t.lemma_\n",
    "                    # If the current child is pronoun (but not in mapper_pron_ant keys), or only contains special characters or numbers, or\n",
    "                    #   length text less than 3\n",
    "                    elif (t.pos_ == 'PRON') or (re.match(r'^[0-9\\W]+$', t.text)) or (len(t.text) < 3):\n",
    "                        continue\n",
    "                    subjects.append(text)\n",
    "                    # Looping through the children of subject.\n",
    "                    subjects += extract_conj(t, lemma=True)\n",
    "\n",
    "            if len(subjects) > 0:\n",
    "                # Make sure the subject is unique\n",
    "                subjects = list(set(subjects))\n",
    "                \n",
    "                # Store the result\n",
    "                result = cross_product_tuple(subjects, token.lemma_)\n",
    "                # Result should be lowercase\n",
    "                result = [(s.lower(), a.lower()) for s, a in result]\n",
    "                sentence_location = get_sentence_location(sentence_points, idx)\n",
    "                storage[sentence_location] += result\n",
    "    \n",
    "    \n",
    "        # If the token is aux\n",
    "        elif token.pos_ == 'AUX':\n",
    "            \n",
    "            # Looping through children\n",
    "            for t in token.children:\n",
    "                if t.dep_ == 'nsubj':\n",
    "                    # Get current child index and text\n",
    "                    current_idx = t.i\n",
    "                    text = t.lemma_\n",
    "                    # If the current child is pronoun I, me, my\n",
    "                    if t.pos_ == 'PRON' and t.text.lower() in first_person_pronouns:\n",
    "                        # The token is reference to \"the user\"\n",
    "                        text = 'the user'\n",
    "                    # If the current child is pronoun and its current_idx in mapper_pron_ant\n",
    "                    elif t.pos_ == 'PRON' and current_idx in mapper_pron_ant.keys():\n",
    "                        # Change current token\n",
    "                        idx_map = mapper_pron_ant[current_idx]\n",
    "                        t = doc[idx_map]\n",
    "                        text = t.lemma_\n",
    "                    # If the current child is pronoun (but not in mapper_pron_ant keys), or only contains special characters or numbers, or\n",
    "                    #   length text less than 3\n",
    "                    elif (t.pos_ == 'PRON') or (re.match(r'^[0-9\\W]+$', t.text)) or (len(t.text) < 3):\n",
    "                        continue\n",
    "                        \n",
    "                    subjects.append(text)\n",
    "                    # Looping through the children of subject.\n",
    "                    subjects += extract_conj(t, lemma=True)\n",
    "\n",
    "                # Check if the neglect exist and depend on token aux\n",
    "                neg = get_neglect(token)\n",
    "                if t.dep_ == 'acomp':\n",
    "                    # If neglection does not exist after aux, then check if it exist at first adj/verb\n",
    "                    if not neg:\n",
    "                        neg = get_neglect(t)\n",
    "                    abilities.append(t.lemma_)\n",
    "                    # Looping through the children of subject\n",
    "                    # If neglection does not appear in after aux or before first subject.\n",
    "                    #  Then check all neglection in first conjugation.\n",
    "                    if not neg:\n",
    "                        abilities += extract_conj(t, neglect=True, lemma=True)\n",
    "                    else:\n",
    "                        abilities += extract_conj(t, lemma=True)\n",
    "                        abilities = cross_product_str(neg, abilities)\n",
    "    \n",
    "            if len(subjects) > 0 and len(abilities) > 0 :\n",
    "                # Make sure the subject is unique\n",
    "                subjects = list(set(subjects))\n",
    "\n",
    "                # Store the result\n",
    "                result = cross_product_tuple(subjects, abilities)\n",
    "                # Result should be lowercase\n",
    "                result = [(s.lower(), a.lower()) for s, a in result]\n",
    "                sentence_location = get_sentence_location(sentence_points, idx)\n",
    "                storage[sentence_location] += result\n",
    "            \n",
    "        # If the token is noun\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            # If only contains special characters or numbers, or length text less than 3\n",
    "            if re.match(r'^[0-9\\W]+$', token.text) or len(token.text) < 3:\n",
    "                continue\n",
    "            for t in token.lefts:\n",
    "                if t.pos_ == 'ADJ':\n",
    "                    neg = get_neglect(t)\n",
    "                    if neg:\n",
    "                        abilities.append(neg + ' ' + t.lemma_)\n",
    "                    else:\n",
    "                        abilities.append(t.lemma_)\n",
    "    \n",
    "            # If the token contain abilities, then we check is there any conjugation\n",
    "            if len(abilities) > 0:\n",
    "                subjects.append(token.lemma_)\n",
    "                subjects += extract_conj(token, lemma=True)\n",
    "                # Make sure the subject is unique\n",
    "                subjects = list(set(subjects))\n",
    "\n",
    "\n",
    "                # Store the result\n",
    "                result = cross_product_tuple(subjects, abilities)\n",
    "                # Result should be lowercase\n",
    "                result = [(s.lower(), a.lower()) for s, a in result]\n",
    "                sentence_location = get_sentence_location(sentence_points, idx)\n",
    "                storage[sentence_location] += result\n",
    "\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "16c6c993-2983-43be-a83b-691931eba33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_active_rules(token):\n",
    "    abilities = []\n",
    "    # Go to its head\n",
    "    head = token.head\n",
    "\n",
    "    # Get neglect; If there is no neglect, return empty text.\n",
    "    neg = get_neglect(head)\n",
    "\n",
    "    # If head is Verb.\n",
    "    if head.pos_ == 'VERB':\n",
    "\n",
    "        # Get all conjunct (except adjective with dependency acomp)\n",
    "        if neg:\n",
    "            verb_conjunct = extract_conj(head)\n",
    "        else:\n",
    "            # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "            verb_conjunct = extract_conj(head, neglect=True)\n",
    "\n",
    "        # If the verb is posession\n",
    "        if head.lemma_ == 'have':\n",
    "\n",
    "            # Get direct object\n",
    "            obj = get_token_dep(head, dep='dobj')\n",
    "\n",
    "            # If direct object exist\n",
    "            if obj:\n",
    "                print(\"Part one\")\n",
    "                # Extract all possible pre-modifier adjectives\n",
    "                pre_adj_text = ' '.join(extract_pre_adj(obj))\n",
    "\n",
    "                # Concatenate components into: Adj (optional) + Direct object\n",
    "                ability = cross_product_str(pre_adj_text, obj.text)\n",
    "                # Concatenate components into: Verb (Have) + Adj (optional) + Direct object\n",
    "                ability = cross_product_str(head.lemma_, ability)\n",
    "                # Concatenate components into: not (optional) + Verb (Have) + Adj (optional) + Direct object\n",
    "                ability = cross_product_str(neg, ability)\n",
    "\n",
    "                # Add the ability into abilities\n",
    "                abilities += ability\n",
    "                # EXPECTED PATTERN: Subject + not (optional) + Verb (Have) + Adj (optional) + Direct object\n",
    "\n",
    "                # If the object has conjunct\n",
    "                obj_conjunct = extract_conj(obj)\n",
    "                if len(obj_conjunct) > 0:\n",
    "                    print(\"Part two\")\n",
    "                    # Concatenate components into: Verb (Have) + Conjunct\n",
    "                    ability = cross_product_str(head.lemma_, obj_conjunct)\n",
    "                    # Concatenate components into: not (optional) + Verb (Have) + Conjunct\n",
    "                    ability = cross_product_str(neg, ability)\n",
    "\n",
    "                    # Add the ability into abilities\n",
    "                    abilities += ability\n",
    "                    # EXPECTED PATTERN: Subject + not (optional) + Verb (Have) + Direct object\n",
    "                    # Note: Since normaly, If direct object is noun/propn/pron the conjuncts are noun/propn/pron too.\n",
    "                    #        This rule follow this concept. In somehow, the conjunct could be adjective or another verb.\n",
    "\n",
    "            # If the verb is posession but do not have direct object (object that the subject posessed) return empty list\n",
    "            else:\n",
    "                return []\n",
    "        else:\n",
    "            # Get aux\n",
    "            aux = get_token_dep(head, dep='aux')\n",
    "\n",
    "            # Get Pre and post adv\n",
    "            pre_adv, post_adv = extract_adv(head)\n",
    "            # Convert pre and post adverb into string\n",
    "            pre_adv = (' '.join(pre_adv)).strip()\n",
    "            post_adv = (' '.join(post_adv)).strip()\n",
    "\n",
    "            # Get Preposition after verb\n",
    "            prep_after_verb = crawling_after_token_prep_phrase(head)\n",
    "\n",
    "            # Get intransitive rate score\n",
    "            int_rate = map_verb_intrans.get(head.text) or map_verb_intrans.get(head.lemma_)\n",
    "            # If the verb is not in the mapper ( we assume it is transitive verb )\n",
    "            if not int_rate:\n",
    "                int_rate = 0\n",
    "\n",
    "            # Concatenate components into: adv (optional) + verb\n",
    "            ability = cross_product_str(pre_adv, head.text)\n",
    "            # Concatenate components into: not (optional) + adv (optional) + verb\n",
    "            ability = cross_product_str(neg, ability)\n",
    "            # Concatenate components into: not (optional) + adv (optional) + verb + adv (optional)\n",
    "            ability = cross_product_str(ability, post_adv)\n",
    "\n",
    "            # If aux exist, add the text into ability_text\n",
    "            if aux:\n",
    "                # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + adv (optional)\n",
    "                ability = cross_product_str(aux.text, ability)\n",
    "\n",
    "            # If the verb is intransitive or Adverb after verb exist\n",
    "            if (int_rate >= 0.5) or (post_adv):\n",
    "                print(\"Part three\")\n",
    "                # EXPECTED PATTERN: Subject + aux (optional) + not (optional) + adv (optional) + verb + adv\n",
    "                abilities += ability\n",
    "\n",
    "            # If the verb is transitive\n",
    "            else:\n",
    "                # Get the direct object\n",
    "                obj = get_token_dep(head, dep='dobj')\n",
    "                # If the direct object exist\n",
    "                if obj:\n",
    "                    print(\"Part four\")\n",
    "                    # Get pre adjective modifier direct object and convert in into string\n",
    "                    pre_adj = ' '.join(extract_pre_adj(obj))\n",
    "                    # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + adj (optional)\n",
    "                    ability = cross_product_str(ability, pre_adj)\n",
    "                    # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + adj (optional) + Direct object\n",
    "                    ability = cross_product_str(ability, obj.text)\n",
    "\n",
    "                    # EXPECTED PATTERN: Subject + aux (optional) + not (optional) + adv (optional) + verb + adj (optional) + Direct object\n",
    "                    abilities += ability\n",
    "\n",
    "            # If the preposition phrase exists after verb\n",
    "            if prep_after_verb:\n",
    "                print(\"Part five\")\n",
    "                # Concatenate components into: adv (optional) + verb\n",
    "                ability = cross_product_str(pre_adv, head.text)\n",
    "                # Concatenate components into: not (optional) + adv (optional) + verb\n",
    "                ability = cross_product_str(neg, ability)\n",
    "                # If auxiliary token exist\n",
    "                if aux:\n",
    "                    # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb\n",
    "                    ability = cross_product_str(aux.text, ability)\n",
    "\n",
    "                # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + preposition phrase\n",
    "                ability = cross_product_str(ability, prep_after_verb)\n",
    "\n",
    "                # EXPECTED PATTERN: Subject + aux (optional) + not (optional) + adv (optional) + verb + preposition phrase\n",
    "                abilities += ability\n",
    "\n",
    "            if len(verb_conjunct) > 0:\n",
    "                print(\"Part six\")\n",
    "                # Extract ability\n",
    "                ability = cross_product_str(neg, verb_conjunct)\n",
    "                if aux:\n",
    "                    ability = cross_product_str(aux.text, ability)\n",
    "                abilities += ability\n",
    "\n",
    "    # If head is aux\n",
    "    elif head.pos_ == 'AUX':\n",
    "        # Get the token\n",
    "        # NOTE: if 'AUX' is root, only have one adjective with dependency acomp.\n",
    "        adj_token = get_token_dep(head, dep='acomp')\n",
    "        noun_token = get_token_pos_right(head, pos=['NOUN', 'PROPN'])\n",
    "\n",
    "        if adj_token:\n",
    "            print(\"Part seven\")\n",
    "            # Get Preposition after adjective\n",
    "            prep_after_adj = ' '.join(crawling_after_token_prep_phrase(adj_token))\n",
    "\n",
    "            # Concatenate components into: aux + not (optional)\n",
    "            ability = cross_product_str(head.text, neg)\n",
    "            # Concatenate components into: aux + not (optional) + adj\n",
    "            ability = cross_product_str(ability, adj_token.text)\n",
    "            # Concatenate components into: aux + not (optional) + adj + preposition phrase (optional)\n",
    "            ability = cross_product_str(ability, prep_after_adj)\n",
    "\n",
    "            # EXPECTED PATTERN: Subject + aux + not (optional) + adj\n",
    "            abilities += ability\n",
    "\n",
    "            # Get all conjunct (except adjective with dependency acomp)\n",
    "            if neg:\n",
    "                adj_conjunct = extract_conj(adj_token)\n",
    "            else:\n",
    "                # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "                adj_conjunct = extract_conj(adj_token, neglect=True)\n",
    "\n",
    "            if len(adj_conjunct) > 0:\n",
    "                print(\"Part eight\")\n",
    "                # Concatenate components into: aux + not (optional)\n",
    "                ability = cross_product_str(head.text, neg)\n",
    "                # Concatenate components into: aux + not (optional) + adj\n",
    "                ability = cross_product_str(ability, adj_conjunct)\n",
    "                # EXPECTED PATTERN: Subject + aux + not (optional) + adj\n",
    "                abilities += ability\n",
    "\n",
    "        elif noun_token:\n",
    "            print(\"Part nine\")\n",
    "            # Get Pre-modifier adjective\n",
    "            pre_adj_noun = ' '.join(extract_pre_adj(noun_token))\n",
    "            # Concatenate components into: aux + not (optional)\n",
    "            ability = cross_product_str(head.text, neg)\n",
    "            # Concatenate components into: aux + not (optional) + pre-modifier adjective (optional)\n",
    "            ability = cross_product_str(ability, pre_adj_noun)\n",
    "            # Concatenate components into: aux + not (optional) + pre-modifier adjective (optional) + noun\n",
    "            ability = cross_product_str(ability, noun_token.text)\n",
    "\n",
    "            # EXPECTED PATTERN: Subject + aux + not (optional) + pre-modifier adjective (optional) + noun\n",
    "            abilities += ability\n",
    "\n",
    "            # Get all conjunct\n",
    "            if neg:\n",
    "                noun_conjunct = extract_conj(noun_token)\n",
    "            else:\n",
    "                # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "                noun_conjunct = extract_conj(noun_token, neglect=True)\n",
    "\n",
    "            if len(noun_conjunct) > 0:\n",
    "                print(\"Part ten\")\n",
    "                # Concatenate components into: aux + not (optional)\n",
    "                ability = cross_product_str(head.text, neg)\n",
    "                # Concatenate components into: aux + not (optional) + noun\n",
    "                ability = cross_product_str(ability, noun_conjunct)\n",
    "                # EXPECTED PATTERN: Subject + aux + not (optional) + noun\n",
    "                abilities += ability\n",
    "    return abilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "550f2f1a-fcc5-4ac9-abdb-6ed4e6538510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_passive_rules(token):\n",
    "    abilities = []\n",
    "    # Get the token head (verb). Since passive form at least form: Subject + auxpass + verb \n",
    "    head = token.head\n",
    "    if head.pos_ != 'VERB':\n",
    "        return []\n",
    "\n",
    "    ##================= GET ALL POSSIBLE COMPONENTS ===============##\n",
    "    # Get neglect; If there is no neglect, return empty text.\n",
    "    neg = get_neglect(head)\n",
    "\n",
    "    # Get auxpass token\n",
    "    auxpass = get_token_dep(head, dep='auxpass')\n",
    "    if not auxpass:\n",
    "        # Since passive sentence must have auxpass in its component.\n",
    "        return []\n",
    "\n",
    "    # Get aux token\n",
    "    aux = get_token_dep(head, dep='aux')\n",
    "\n",
    "    # Get advmod after verb token\n",
    "    advmod_main = get_token_dep_right(head, dep=['advmod', 'npadvmod'])\n",
    "    if advmod_main:\n",
    "        pre_advmod_main, post_advmod_main = extract_adv(advmod_main)\n",
    "        pre_advmod_main = ' '.join(pre_advmod_main)\n",
    "        post_advmod_main = ' '.join(post_advmod_main)\n",
    "\n",
    "    # Get the agent token\n",
    "    agent = get_token_dep_right(head, dep='agent')\n",
    "    obj_agent = None\n",
    "    # If the agent token exist\n",
    "    if agent:\n",
    "        # Get the object that refers to 'agent' token\n",
    "        obj_agent = get_token_dep(agent, dep=['pobj', 'dobj'])\n",
    "\n",
    "    # Get prepositional phrase\n",
    "    if neg:\n",
    "        prep_after_verb = crawling_after_token_prep_phrase(head)\n",
    "    else:\n",
    "        prep_after_verb = crawling_after_token_prep_phrase(head, neglect=True)\n",
    "\n",
    "    # Get xcomp token\n",
    "    xcomp = get_token_dep(head, dep='xcomp')\n",
    "    # Initalize object and advmod of xcomp.\n",
    "    obj_xcomp = None\n",
    "    advmod_xcomp = None\n",
    "    if xcomp:\n",
    "        # Get the aux, adv, and obj of xcomp tokens.\n",
    "        aux_xcomp = get_token_dep(xcomp, dep='aux')\n",
    "        obj_xcomp = get_token_dep(xcomp, dep=['pobj', 'dobj'])\n",
    "        advmod_xcomp = get_token_dep_right(xcomp, dep='advmod')\n",
    "        # If adv modifier of xcomp exist\n",
    "        if advmod_xcomp:\n",
    "            # Get pre and post adverb of main adverb modifier xcomp.\n",
    "            pre_advmod_xcomp, post_advmod_xcomp = extract_adv(advmod_xcomp)\n",
    "            pre_advmod_xcomp = ' '.join(pre_advmod_xcomp)\n",
    "            post_advmod_xcomp = ' '.join(post_advmod_xcomp)\n",
    "\n",
    "    ##================= STORING ABILITIES ===============##     \n",
    "    # Store ability: If adverb modifier exist\n",
    "    if advmod_main:\n",
    "        # If aux exist\n",
    "        if aux:\n",
    "            # Concatenate components: aux (optional) + neg (optional)\n",
    "            ability = cross_product_str(aux.text, neg)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "            ability = cross_product_str(ability, auxpass.text)\n",
    "        else:\n",
    "            # Concatenate components: auxpass + neg (optional)\n",
    "            ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "        ability = cross_product_str(ability, head.text)\n",
    "\n",
    "        # If pre adverb exist\n",
    "        if pre_advmod_main:\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + pre-advmod (optional)\n",
    "            ability = cross_product_str(ability, pre_advmod_main)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + pre-advmod (optional) + advmod\n",
    "        ability = cross_product_str(ability, advmod_main.text)\n",
    "\n",
    "        # Get prepositional phrase after adverb\n",
    "        prep_after_adv = crawling_after_token_prep_phrase(advmod_main)\n",
    "        if prep_after_adv:\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + pre-advmod (optional)\n",
    "            #                            + advmod + preposition phrase (optional)\n",
    "            ability = cross_product_str(ability, prep_after_adv)\n",
    "\n",
    "        # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + pre-advmod (optional)\n",
    "        #                      + advmod + preposition phrase (optional)\n",
    "        abilities += ability\n",
    "\n",
    "        # Get all the adverb conjuncts\n",
    "        # If neglection exist\n",
    "        if neg:\n",
    "            # It assume that all conjuncts are neglection \n",
    "            advmod_main_conj = extract_conj(advmod_main)\n",
    "        else:\n",
    "            # If neglect do not come at first, then check neglection in front each conjunct\n",
    "            advmod_main_conj = extract_conj(advmod_main, neglect=True)\n",
    "\n",
    "        # If adverb has conjunct                \n",
    "        if len(advmod_main_conj) > 0:\n",
    "            # If contain aux\n",
    "            if aux:\n",
    "                # Concatenate components: aux (optional) + neg (optional)\n",
    "                ability = cross_product_str(aux.text, neg)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                ability = cross_product_str(ability, auxpass.text)\n",
    "            else:\n",
    "                # Concatenate components: auxpass + neg (optional)\n",
    "                ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "            ability = cross_product_str(ability, head.text)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + advmod\n",
    "            ability = cross_product_str(ability, advmod_main_conj)\n",
    "            # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass\n",
    "            abilities += ability\n",
    "\n",
    "    # Store ability: If agent and object agent token exist\n",
    "    if obj_agent and agent:\n",
    "        # If aux exist\n",
    "        if aux:\n",
    "            # Concatenate components: aux (optional) + neg (optional)\n",
    "            ability = cross_product_str(aux.text, neg)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "            ability = cross_product_str(ability, auxpass.text)\n",
    "        else:\n",
    "            # Concatenate components: auxpass + neg (optional)\n",
    "            ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb \n",
    "        ability = cross_product_str(ability, head.text)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + agent\n",
    "        ability = cross_product_str(ability, agent.text)\n",
    "\n",
    "        # Get the pre adjective of object\n",
    "        pre_adj = ' '.join(extract_pre_adj(obj_agent))\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + agent + adj (optional)\n",
    "        ability = cross_product_str(ability, pre_adj)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + agent + adj (optional) + object\n",
    "        ability = cross_product_str(ability, obj_agent.text)\n",
    "        # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + agent + adj (optional) + object\n",
    "        abilities += ability\n",
    "\n",
    "        # Get all object conjuncts\n",
    "        if neg:\n",
    "            obj_agent_conj = extract_conj(obj_agent)\n",
    "        else:\n",
    "            # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "            obj_agent_conj = extract_conj(obj_agent, neglection=True)\n",
    "        # If object conjuncts exist\n",
    "        if len(obj_agent_conj) > 0:\n",
    "            # If aux exist\n",
    "            if aux:\n",
    "                # Concatenate components: aux (optional) + neg (optional)\n",
    "                ability = cross_product_str(aux.text, neg)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                ability = cross_product_str(ability, auxpass.text)\n",
    "            else:\n",
    "                # Concatenate components: auxpass + neg (optional)\n",
    "                ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "            ability = cross_product_str(ability, head.text)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + object\n",
    "            ability = cross_product_str(ability, obj_agent_conj)\n",
    "            # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + object\n",
    "            abilities += ability\n",
    "\n",
    "    # Store ability: If preposition after verb exist\n",
    "    if prep_after_verb:\n",
    "        # If aux exist\n",
    "        if aux:\n",
    "            # Concatenate components: aux (optional) + neg (optional)\n",
    "            ability = cross_product_str(aux.text, neg)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "            ability = cross_product_str(ability, auxpass.text)\n",
    "        else:\n",
    "            # Concatenate components: auxpass + neg (optional)\n",
    "            ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "        ability = cross_product_str(ability, head.text)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + preposition phrase\n",
    "        ability = cross_product_str(ability, prep_after_verb)\n",
    "        # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + preposition phrase (optional)\n",
    "        abilities += ability\n",
    "        \n",
    "    # Store ability: If xcomp and object xcomp exist\n",
    "    if obj_xcomp and xcomp:\n",
    "        # If aux exist\n",
    "        if aux:\n",
    "            # Concatenate components: aux (optional) + neg (optional)\n",
    "            ability = cross_product_str(aux.text, neg)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "            ability = cross_product_str(ability, auxpass.text)\n",
    "        else:\n",
    "            # Concatenate components: auxpass + neg (optional)\n",
    "            ability = cross_product_str(auxpass.text, neg)        \n",
    "        \n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "        ability = cross_product_str(ability, head.text)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp\n",
    "        ability = cross_product_str(ability, aux_xcomp.text)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp\n",
    "        ability = cross_product_str(ability, xcomp.text)\n",
    "                \n",
    "        # Get the pre adjective of object\n",
    "        pre_adj = ' '.join(extract_pre_adj(obj_xcomp))\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + adj (optional)\n",
    "        ability = cross_product_str(ability, pre_adj)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + adj (optional) + object\n",
    "        ability = cross_product_str(ability, obj_xcomp.text)\n",
    "        # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + adj (optional) + object\n",
    "        abilities += ability\n",
    "                \n",
    "        # Get all object conjuncts\n",
    "        if neg:\n",
    "            obj_xcomp_conj = extract_conj(obj_xcomp)\n",
    "        else:\n",
    "            # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "            obj_xcomp_conj = extract_conj(obj_xcomp, neglect=True)\n",
    "        \n",
    "        # If object conjuncts exist\n",
    "        if len(obj_xcomp_conj) > 0:\n",
    "            # If aux exist\n",
    "            if aux:\n",
    "                # Concatenate components: aux (optional) + neg (optional)\n",
    "                ability = cross_product_str(aux.text, neg)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                ability = cross_product_str(ability, auxpass.text)\n",
    "            else:\n",
    "                # Concatenate components: auxpass + neg (optional)\n",
    "                ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "            ability = cross_product_str(ability, head.text)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp\n",
    "            ability = cross_product_str(ability, aux_xcomp.text)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp\n",
    "            ability = cross_product_str(ability, xcomp.text)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + object\n",
    "            ability = cross_product_str(ability, obj_xcomp_conj)\n",
    "            # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + object\n",
    "            abilities += ability\n",
    "\n",
    "        # Store ability: If xcomp and object xcomp exist\n",
    "        if advmod_xcomp and xcomp:\n",
    "            # If aux exist\n",
    "            if aux:\n",
    "                # Concatenate components: aux (optional) + neg (optional)\n",
    "                ability = cross_product_str(aux.text, neg)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                ability = cross_product_str(ability, auxpass.text)\n",
    "            else:\n",
    "                # Concatenate components: auxpass + neg (optional)\n",
    "                ability = cross_product_str(auxpass.text, neg)\n",
    "                    \n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "            ability = cross_product_str(ability, head.text)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xomp\n",
    "            ability = cross_product_str(ability, aux_xcomp.text)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xomp + xcomp\n",
    "            ability = cross_product_str(ability, xcomp.text)\n",
    "            # If pre advmod xcomp exist\n",
    "            if pre_advmod_xcomp:\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xomp + xcomp + pre-advmod (optional)\n",
    "                ability = cross_product_str(ability, pre_advmod_xcomp)\n",
    "                \n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xomp + xcomp + pre-advmod (optional) + advmod\n",
    "            ability = cross_product_str(ability, advmod_xcomp.text)\n",
    "            # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + aux-xomp + xcomp + pre-advmod (optional) + advmod\n",
    "            abilities += ability  \n",
    "\n",
    "            # Get all advmod conjuncts\n",
    "            if neg:\n",
    "                advmod_xcomp_conj = extract_conj(advmod_xcomp)\n",
    "            else:\n",
    "                # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "                advmod_xcomp_conj = extract_conj(advmod_xcomp, neglect=True)\n",
    "            # If advmod conjuncts exist\n",
    "            if len(advmod_xcomp_conj) > 0:\n",
    "                if aux:\n",
    "                    # Concatenate components: aux (optional) + neg (optional)\n",
    "                    ability = cross_product_str(aux.text, neg)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                    ability = cross_product_str(ability, auxpass.text)\n",
    "                else:\n",
    "                    # Concatenate components: auxpass + neg (optional)\n",
    "                    ability = cross_product_str(auxpass.text, neg)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "                ability = cross_product_str(ability, head.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp\n",
    "                ability = cross_product_str(ability, aux_xcomp.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp\n",
    "                ability = cross_product_str(ability, xcomp.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + advmod\n",
    "                ability = cross_product_str(ability, advmod_xcomp_conj)\n",
    "                # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + advmod\n",
    "                abilities += ability\n",
    "                \n",
    "    return abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "b1c4f147-f354-4152-97d4-288c209b1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_abilities(doc):\n",
    "    # Define local variable.\n",
    "    storage = {}\n",
    "    first_person_pronouns = [ 'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
    "    pronouns = [\n",
    "    \"he\", \"she\", \"they\", \"it\", # Personal Pronouns (Subjective)\n",
    "    \"him\", \"her\", \"them\", \"it\", \"you\",  # Personal Pronouns (Objective)\n",
    "    \"his\", \"hers\", \"theirs\", \"its\", \"mine\", \"yours\", \"ours\",  # Possessive Pronouns\n",
    "    \"her\", \"their\", \"its\",  # Possessive Adjectives\n",
    "    \"himself\", \"herself\", \"themself\", \"themselves\", \"Itself\",  # Reflexive Pronouns,\n",
    "    \"this\", \"that\", \"these\", \"those\", # Demonstrative Pronouns\n",
    "    \"who\", \"whom\", \"whose\", \"which\", \"that\"  # Relative Pronouns\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Get sentence mapper and prepare storage\n",
    "    sentence_points = {}\n",
    "    for i, s in enumerate(doc.sents):\n",
    "        sentence_points[i] = (s.start, s.end)\n",
    "        storage[i] = []\n",
    "\n",
    "    # Get mapper pronoun and antecedents\n",
    "    mapper_pron_ant = get_mapper_pron_ant(doc)\n",
    "\n",
    "    # Define local variable.\n",
    "    result = []\n",
    "    for idx, token in enumerate(doc):\n",
    "        abilities = []\n",
    "\n",
    "        # Initial filter: If token only contains special characters or numbers, or length text less than 3\n",
    "        if (re.match(r'^[0-9\\W]+$', token.text)) or (len(token.text) < 3):\n",
    "            continue\n",
    "\n",
    "        ## ==================== SUBJECT ACTIVE SENTENCE =========================== ##\n",
    "        # If token is subject (should be nsubj and nsubjpass). This time only nsubj\n",
    "        # In case active sentence form\n",
    "        if token.dep_ == 'nsubj':\n",
    "            abilities += subject_active_rules(token)\n",
    "\n",
    "        ## ==================== SUBJECT PASSIVE SENTENCE =========================== ##\n",
    "        # If sentence is passive form.\n",
    "        if token.dep_ == 'nsubjpass':\n",
    "            abilities += subject_passive_rules(token)\n",
    "                \n",
    "        # Store final result\n",
    "        if len(abilities) > 0:\n",
    "            # Subject handling\n",
    "            text = token.lemma_\n",
    "            # current_idx = token.i\n",
    "            # If the subject is pronouns and first person pronouns\n",
    "            if token.pos_ == 'PRON' and token.text.lower() in first_person_pronouns:\n",
    "                print(\"PRON 1\")\n",
    "                subject = 'the user'\n",
    "            # If subject is pronouns and its token location in mapper_pron_ant\n",
    "            elif token.pos_ == 'PRON' and idx in mapper_pron_ant.keys():\n",
    "                print(\"PRON 2\")\n",
    "                # Get the antecedent index location\n",
    "                idx_map = mapper_pron_ant[idx]\n",
    "                # Change current token subject\n",
    "                token = doc[idx_map]\n",
    "                text = token.lemma_\n",
    "            # If the current child is pronoun (but not in mapper_pron_ant keys)\n",
    "            elif token.pos_ == 'PRON' and token.text.lower() in pronouns:\n",
    "                print(\"PRON 3\")\n",
    "                continue\n",
    "            \n",
    "            # Get all conj subject + current subject\n",
    "            subjects = [token.text] + extract_conj(token)\n",
    "            # # Store result\n",
    "            # result += cross_product_tuple(subjects, abilities)\n",
    "            # Storage final result\n",
    "            sentence_location = get_sentence_location(sentence_points, idx)\n",
    "            storage[sentence_location] += cross_product_tuple(subjects, abilities)\n",
    "            print(storage)\n",
    "\n",
    "    # Storing final result\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "dfdc86e0-4ab3-4dc0-bd01-752107eafcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"073b3408194444efbebda20f21d781da-0\" class=\"displacy\" width=\"7575\" height=\"574.5\" direction=\"ltr\" style=\"max-width: none; height: 574.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">It</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">'d</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">McDonalds.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">It</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">what</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">it</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">as</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">far</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">as</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">SCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">food</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">atmosphere</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">go.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">staff</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3200\">here</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3200\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3375\">does</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3375\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">make</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3725\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3725\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3900\">difference.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3900\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4075\">They</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4075\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4425\">all</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4425\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4600\">friendly,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4600\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4775\">accommodating</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4775\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4950\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4950\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5125\">always</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5125\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5300\">smiling.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5300\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5475\">Makes</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5475\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5650\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5650\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5825\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5825\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6000\">more</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6000\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6175\">pleasant</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6175\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6350\">experience</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6350\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6525\">than</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6525\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6700\">many</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6700\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6875\">other</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6875\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"7050\">fast</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"7050\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"7225\">food</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"7225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"484.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"7400\">places.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"7400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-0\" stroke-width=\"2px\" d=\"M70,439.5 C70,352.0 205.0,352.0 205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,441.5 L62,429.5 78,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-1\" stroke-width=\"2px\" d=\"M245,439.5 C245,352.0 380.0,352.0 380.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M380.0,441.5 L388.0,429.5 372.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-2\" stroke-width=\"2px\" d=\"M595,439.5 C595,352.0 730.0,352.0 730.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,441.5 L587,429.5 603,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-3\" stroke-width=\"2px\" d=\"M945,439.5 C945,264.5 1260.0,264.5 1260.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,441.5 L937,429.5 953,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-4\" stroke-width=\"2px\" d=\"M1120,439.5 C1120,352.0 1255.0,352.0 1255.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,441.5 L1112,429.5 1128,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-5\" stroke-width=\"2px\" d=\"M770,439.5 C770,177.0 1265.0,177.0 1265.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,441.5 L1273.0,429.5 1257.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-6\" stroke-width=\"2px\" d=\"M1470,439.5 C1470,352.0 1605.0,352.0 1605.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1470,441.5 L1462,429.5 1478,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-7\" stroke-width=\"2px\" d=\"M1295,439.5 C1295,264.5 1610.0,264.5 1610.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1610.0,441.5 L1618.0,429.5 1602.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-8\" stroke-width=\"2px\" d=\"M1820,439.5 C1820,89.5 2670.0,89.5 2670.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,441.5 L1812,429.5 1828,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-9\" stroke-width=\"2px\" d=\"M1995,439.5 C1995,352.0 2130.0,352.0 2130.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,441.5 L1987,429.5 2003,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-10\" stroke-width=\"2px\" d=\"M2170,439.5 C2170,177.0 2665.0,177.0 2665.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2170,441.5 L2162,429.5 2178,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-11\" stroke-width=\"2px\" d=\"M2170,439.5 C2170,352.0 2305.0,352.0 2305.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2305.0,441.5 L2313.0,429.5 2297.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-12\" stroke-width=\"2px\" d=\"M2170,439.5 C2170,264.5 2485.0,264.5 2485.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2485.0,441.5 L2493.0,429.5 2477.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-13\" stroke-width=\"2px\" d=\"M1645,439.5 C1645,2.0 2675.0,2.0 2675.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2675.0,441.5 L2683.0,429.5 2667.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-14\" stroke-width=\"2px\" d=\"M2870,439.5 C2870,352.0 3005.0,352.0 3005.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2870,441.5 L2862,429.5 2878,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-15\" stroke-width=\"2px\" d=\"M3045,439.5 C3045,264.5 3535.0,264.5 3535.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3045,441.5 L3037,429.5 3053,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-16\" stroke-width=\"2px\" d=\"M3045,439.5 C3045,352.0 3180.0,352.0 3180.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3180.0,441.5 L3188.0,429.5 3172.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-17\" stroke-width=\"2px\" d=\"M3395,439.5 C3395,352.0 3530.0,352.0 3530.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-17\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3395,441.5 L3387,429.5 3403,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-18\" stroke-width=\"2px\" d=\"M3745,439.5 C3745,352.0 3880.0,352.0 3880.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-18\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3745,441.5 L3737,429.5 3753,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-19\" stroke-width=\"2px\" d=\"M3570,439.5 C3570,264.5 3885.0,264.5 3885.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-19\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3885.0,441.5 L3893.0,429.5 3877.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-20\" stroke-width=\"2px\" d=\"M4095,439.5 C4095,352.0 4230.0,352.0 4230.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-20\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4095,441.5 L4087,429.5 4103,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-21\" stroke-width=\"2px\" d=\"M4270,439.5 C4270,352.0 4405.0,352.0 4405.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-21\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4405.0,441.5 L4413.0,429.5 4397.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-22\" stroke-width=\"2px\" d=\"M4270,439.5 C4270,264.5 4585.0,264.5 4585.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-22\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4585.0,441.5 L4593.0,429.5 4577.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-23\" stroke-width=\"2px\" d=\"M4620,439.5 C4620,352.0 4755.0,352.0 4755.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-23\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4755.0,441.5 L4763.0,429.5 4747.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-24\" stroke-width=\"2px\" d=\"M4795,439.5 C4795,352.0 4930.0,352.0 4930.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-24\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4930.0,441.5 L4938.0,429.5 4922.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-25\" stroke-width=\"2px\" d=\"M5145,439.5 C5145,352.0 5280.0,352.0 5280.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-25\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5145,441.5 L5137,429.5 5153,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-26\" stroke-width=\"2px\" d=\"M4795,439.5 C4795,264.5 5285.0,264.5 5285.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-26\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5285.0,441.5 L5293.0,429.5 5277.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-27\" stroke-width=\"2px\" d=\"M5495,439.5 C5495,352.0 5630.0,352.0 5630.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-27\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5630.0,441.5 L5638.0,429.5 5622.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-28\" stroke-width=\"2px\" d=\"M5845,439.5 C5845,264.5 6335.0,264.5 6335.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-28\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5845,441.5 L5837,429.5 5853,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-29\" stroke-width=\"2px\" d=\"M6020,439.5 C6020,352.0 6155.0,352.0 6155.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-29\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6020,441.5 L6012,429.5 6028,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-30\" stroke-width=\"2px\" d=\"M6195,439.5 C6195,352.0 6330.0,352.0 6330.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-30\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6195,441.5 L6187,429.5 6203,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-31\" stroke-width=\"2px\" d=\"M5670,439.5 C5670,177.0 6340.0,177.0 6340.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-31\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6340.0,441.5 L6348.0,429.5 6332.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-32\" stroke-width=\"2px\" d=\"M6370,439.5 C6370,352.0 6505.0,352.0 6505.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-32\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6505.0,441.5 L6513.0,429.5 6497.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-33\" stroke-width=\"2px\" d=\"M6720,439.5 C6720,177.0 7390.0,177.0 7390.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-33\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6720,441.5 L6712,429.5 6728,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-34\" stroke-width=\"2px\" d=\"M6895,439.5 C6895,264.5 7385.0,264.5 7385.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-34\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6895,441.5 L6887,429.5 6903,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-35\" stroke-width=\"2px\" d=\"M7070,439.5 C7070,352.0 7205.0,352.0 7205.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-35\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M7070,441.5 L7062,429.5 7078,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-36\" stroke-width=\"2px\" d=\"M7245,439.5 C7245,352.0 7380.0,352.0 7380.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-36\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M7245,441.5 L7237,429.5 7253,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-073b3408194444efbebda20f21d781da-0-37\" stroke-width=\"2px\" d=\"M6545,439.5 C6545,89.5 7395.0,89.5 7395.0,439.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-073b3408194444efbebda20f21d781da-0-37\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M7395.0,441.5 L7403.0,429.5 7387.0,429.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"Just spent 10 minutes waiting at this McDonald's .\" +\\\n",
    "        \"According to Google they're open 24/7. Finally we pull up to the window to see if anyone was there,\" +\\\n",
    "        \" sure enough one employee seated in the lobby and another at the window. She told me they were closed.\" +\\\n",
    "        \" Whoever these two are they need to be replaced. Get it together McDonald's. You're a corporate power house and you have a reputation to keep.\" +\\\n",
    "        \"One of the most unprofessional experiences I've ever had with fast food. 0/10 would not recomend this location.\"\n",
    "\n",
    "text = \"It'd McDonalds. It is what it is as far as the food and atmosphere go. The staff here does make a difference. They are all friendly, accommodating and always smiling. Makes for a more pleasant experience than many other fast food places.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, 'dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "83b1523a-4fed-41f0-8be4-4a9c9cdd87eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.  It'd McDonalds.\n",
      "1.  It is what it is as far as the food and atmosphere go.\n",
      "2.  The staff here does make a difference.\n",
      "3.  They are all friendly, accommodating and always smiling.\n",
      "4.  Makes for a more pleasant experience than many other fast food places.\n",
      "Part three\n",
      "{0: [], 1: [('food', 'go'), ('atmosphere', 'go')], 2: [], 3: [], 4: []}\n",
      "Part four\n",
      "{0: [], 1: [('food', 'go'), ('atmosphere', 'go')], 2: [('staff', 'does make difference')], 3: [], 4: []}\n",
      "Part seven\n",
      "Part eight\n",
      "PRON 2\n",
      "{0: [], 1: [('food', 'go'), ('atmosphere', 'go')], 2: [('staff', 'does make difference')], 3: [('staff', 'are friendly'), ('staff', 'are accommodating'), ('staff', 'are smiling')], 4: []}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [],\n",
       " 1: [('food', 'go'), ('atmosphere', 'go')],\n",
       " 2: [('staff', 'does make difference')],\n",
       " 3: [('staff', 'are friendly'),\n",
       "  ('staff', 'are accommodating'),\n",
       "  ('staff', 'are smiling')],\n",
       " 4: []}"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_ability(x):\n",
    "    # Prepare sentence\n",
    "    texts = remove_extra_spaces(x)\n",
    "    texts = expand_contractions(x)\n",
    "    texts = remove_non_ascii(x)\n",
    "\n",
    "    # Get aspect\n",
    "    doc = nlp(texts)\n",
    "    mapper_pron_ant = get_mapper_pron_ant(doc)\n",
    "    result = get_raw_abilities(doc)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# text = \"Just spent 10 minutes waiting at this McDonald's .\" +\\\n",
    "#         \"According to Google they're open 24/7. Finally we pull up to the window to see if anyone was there,\" +\\\n",
    "#         \" sure enough one employee seated in the lobby and another at the window. She told me they were closed.\" +\\\n",
    "#         \" Whoever these two are they need to be replaced. Get it together McDonald's. You're a corporate power house and you have a reputation to keep.\" +\\\n",
    "#         \"One of the most unprofessional experiences I've ever had with fast food. 0/10 would not recomend this location.\"\n",
    "# text = \"We hit it off pretty good in the beginning everything was great but that didn\" +\\\n",
    "#         \"ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯\" +\\\n",
    "#         \"ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯\" +\\\n",
    "#         \"ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½t last long. I mean the service is fast but with attitude an\"\n",
    "# text = \"Mcdonalds is great but they really need to hire people who understand both english and spanish and not just spanish speakers trying to take english orders. \"+\\\n",
    "#         \"The people are really nice but be smarter about who you put in the drive thru and lobby to take orders.\"\n",
    "for idx, sent in enumerate(doc.sents):\n",
    "    print(f\"{idx}. \", sent)\n",
    "sample_ability = process_ability(text)\n",
    "sample_ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "6b0b41cc-ecd3-438c-b507-944309aa29db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Just spent 10 minutes waiting at this McDonald's . According to Google they're open 24/7. Finally we pull up to the window to see if anyone was there, sure enough one employee seated in the lobby and another at the window. She told me they were closed. Whoever these two are they need to be replaced. Get it together McDonald's. You're a corporate power house and you have a reputation to keep. One of the most unprofessional experiences I've ever had with fast food. 0/10 would not recomend this location.\""
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].iloc[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "6e48a015-e37e-4d08-b1e3-0b8bb5050dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['minutes'],\n",
       " 1: [],\n",
       " 2: [],\n",
       " 3: [],\n",
       " 4: ['mcdonald'],\n",
       " 5: ['corporate power house', 'reputation'],\n",
       " 6: ['unprofessional experiences', 'food'],\n",
       " 7: ['location']}"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_aspect(x, mapper_1=None, mapper_2=None): \n",
    "    # Prepare sentence\n",
    "    texts = remove_extra_spaces(x)\n",
    "    texts = expand_contractions(x)\n",
    "    texts = remove_non_ascii(x)\n",
    "\n",
    "    # Get aspect\n",
    "    doc = nlp(texts)\n",
    "    result = get_raw_aspects(doc)\n",
    "    result = prunning_aspect(result, doc)\n",
    "\n",
    "    return result\n",
    "\n",
    "text = \"Just spent 10 minutes waiting at this McDonald's .\" +\\\n",
    "        \"According to Google they're open 24/7. Finally we pull up to the window to see if anyone was there,\" +\\\n",
    "        \" sure enough one employee seated in the lobby and another at the window. She told me they were closed.\" +\\\n",
    "        \" Whoever these two are they need to be replaced. Get it together McDonald's. You're a corporate power house and you have a reputation to keep.\" +\\\n",
    "        \"One of the most unprofessional experiences I've ever had with fast food. 0/10 would not recomend this location.\"\n",
    "# text = \"We hit it off pretty good in the beginning everything was great but that didn\" +\\\n",
    "#         \"ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯\" +\\\n",
    "#         \"ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯\" +\\\n",
    "#         \"ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½t last long. I mean the service is fast but with attitude an\"\n",
    "sample_asp = process_aspect(text)\n",
    "sample_asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde604de-68bf-43ac-8469-c8e6413f7238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "f3de86f2-dd41-426d-ba1d-8df7fcd7c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRY GENSHIM TF-IDF\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "from pprint import pprint\n",
    "\n",
    "# Determine selected word (only words) high TF-IDF (Original word) but low IDF (The most common exist in corpus)\n",
    "\n",
    "def get_words(corpus, thres_tfidf=90, thres_idf=10):\n",
    "    # Define local variables\n",
    "    storage_idf = set()\n",
    "    storage_tfidf = {}\n",
    "    \n",
    "    # Preprocessing text\n",
    "    def preprocessing(text):\n",
    "        text = remove_extra_spaces(text)\n",
    "        text = expand_contractions(text)\n",
    "        text = remove_non_ascii(text)\n",
    "\n",
    "        # Get token of words\n",
    "        doc = nlp(text)\n",
    "        result = []\n",
    "        for token in doc:\n",
    "            t = token.lemma_.lower()\n",
    "\n",
    "            # If only contains special characters or numbers and length less than 3\n",
    "            if re.match(r'^[0-9\\W]+$', t) or len(t) < 3:\n",
    "                continue\n",
    "            else:\n",
    "                result.append(t)\n",
    "        return result\n",
    "\n",
    "    ##========= GENERATE MODEL =========##\n",
    "    # Create texts\n",
    "    texts = [preprocessing(document) for document in corpus]\n",
    "\n",
    "    # Create dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    # Convert documents into Bag-of-words format\n",
    "    corpus_bow = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Train the TF-IDF model\n",
    "    tfidf_model = gensim.models.TfidfModel(corpus_bow)\n",
    "\n",
    "    ##============ EXTRACT IMPORTANT VALUES =========##\n",
    "    # Get the idf values\n",
    "    idf_values = tfidf_model.idfs # Return (word_id: idf_values)\n",
    "    scores_idf = np.array(list(idf_values.values()))\n",
    "    \n",
    "    idf_dict = {}\n",
    "    for id, value in idf_values.items():\n",
    "        word = dictionary[id]\n",
    "        idf_dict[word] = value\n",
    "        \n",
    "\n",
    "    # Apply the model to the corpus (get corpus tfidf)\n",
    "    corpus_tfidf = tfidf_model[corpus_bow]\n",
    "\n",
    "    # Get dictionary of tfidf values and scores\n",
    "    scores_tfidf = []\n",
    "    tfidf_dict = {}\n",
    "    for doc_idx, doc in enumerate(corpus_tfidf):\n",
    "\n",
    "        dict_doc = {}\n",
    "        for word_id, score in doc:\n",
    "            word = dictionary[word_id]\n",
    "            dict_doc[word] = score\n",
    "            scores_tfidf.append(score)\n",
    "\n",
    "        tfidf_dict[doc_idx] = dict_doc\n",
    "    \n",
    "    ##=========== Get the threshold =========##\n",
    "    threshold_idf = np.percentile(scores_idf, thres_idf)\n",
    "    threshold_tfidf = np.percentile(scores_tfidf, thres_tfidf)\n",
    "\n",
    "\n",
    "    ##========== Get Words =============##\n",
    "    # IDF\n",
    "    for key, value in idf_dict.items():\n",
    "        if value <= threshold_idf:\n",
    "            storage_idf.add(key)\n",
    "\n",
    "    # TF IDF\n",
    "    for idx_doc, dict_words in tfidf_dict.items():\n",
    "        temp = set()\n",
    "        for key, value in dict_words.items():\n",
    "            if value >= threshold_tfidf:\n",
    "                temp.add(key)\n",
    "            \n",
    "        storage_tfidf[idx_doc] = temp\n",
    "\n",
    "    return storage_idf, storage_tfidf\n",
    "\n",
    "documents = df['review'].values\n",
    "\n",
    "mapper_1, mapper_2 = get_words(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "742390b8-8d4c-4ed5-a72b-132d22500eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Just spent 10 minutes waiting at this McDonald's . According to Google \"\n",
      " \"they're open 24/7. Finally we pull up to the window to see if anyone was \"\n",
      " 'there, sure enough one employee seated in the lobby and another at the '\n",
      " 'window. She told me they were closed. Whoever these two are they need to be '\n",
      " \"replaced. Get it together McDonald's. You're a corporate power house and you \"\n",
      " \"have a reputation to keep. One of the most unprofessional experiences I've \"\n",
      " 'ever had with fast food. 0/10 would not recomend this location.')\n",
      "{0: ['minutes'],\n",
      " 1: [],\n",
      " 2: [],\n",
      " 3: [],\n",
      " 4: ['mcdonald'],\n",
      " 5: ['corporate power house', 'reputation'],\n",
      " 6: ['unprofessional experiences', 'food'],\n",
      " 7: ['location']}\n",
      "{0: [],\n",
      " 1: [],\n",
      " 2: [],\n",
      " 3: [],\n",
      " 4: ['mcdonald'],\n",
      " 5: [],\n",
      " 6: ['food'],\n",
      " 7: ['location']}\n"
     ]
    }
   ],
   "source": [
    "def important_words_aspect(dict_doc, idx_doc, mapper_1=None, mapper_2=None):\n",
    "    # If mapper_1 and mapper_2 is None, do not filter it.\n",
    "    if not mapper_1 and not mapper_2:\n",
    "        return dict_doc\n",
    "\n",
    "    # Copy dictionary\n",
    "    dictionary = dict_doc.copy()\n",
    "    \n",
    "    # Get mapper based on its document.\n",
    "    if mapper_2 :\n",
    "        mapper_2 = mapper_2[idx_doc]\n",
    "\n",
    "    for key, value in dictionary.items():\n",
    "        temp = []\n",
    "        for v1 in value:\n",
    "            # Since it could be multiple word, we must check one by one\n",
    "            for v in v1.split():\n",
    "                # If aspect is in mapper_1 or mapper_2 then keep it\n",
    "                if v in mapper_1 or v in mapper_2:\n",
    "                    # Append full value\n",
    "                    temp.append(v1)\n",
    "                    break\n",
    "\n",
    "        # Update list of string\n",
    "        dictionary[key] = temp\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "sample_asp = process_aspect(text)\n",
    "pprint(df['review'].iloc[33])\n",
    "pprint(sample_asp)\n",
    "pprint(important_words_aspect(sample_asp, 33, mapper_1=mapper_1, mapper_2=mapper_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "id": "89d0421a-6491-4847-ac1c-6f4b42469eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Just spent 10 minutes waiting at this McDonald's . According to Google \"\n",
      " \"they're open 24/7. Finally we pull up to the window to see if anyone was \"\n",
      " 'there, sure enough one employee seated in the lobby and another at the '\n",
      " 'window. She told me they were closed. Whoever these two are they need to be '\n",
      " \"replaced. Get it together McDonald's. You're a corporate power house and you \"\n",
      " \"have a reputation to keep. One of the most unprofessional experiences I've \"\n",
      " 'ever had with fast food. 0/10 would not recomend this location.')\n",
      "{0: [],\n",
      " 1: [('food', 'go'), ('atmosphere', 'go')],\n",
      " 2: [('staff', 'does make difference')],\n",
      " 3: [('staff', 'are friendly'),\n",
      "     ('staff', 'are accommodating'),\n",
      "     ('staff', 'are smiling')],\n",
      " 4: []}\n",
      "{0: [],\n",
      " 1: [('food', 'go')],\n",
      " 2: [('staff', 'does make difference')],\n",
      " 3: [('staff', 'are friendly'),\n",
      "     ('staff', 'are accommodating'),\n",
      "     ('staff', 'are smiling')],\n",
      " 4: []}\n"
     ]
    }
   ],
   "source": [
    "def important_words_ability(dict_doc, idx_doc, mapper_1=None, mapper_2=None):\n",
    "    # If mapper_1 and mapper_2 is None, do not filter it.\n",
    "    if not mapper_1 and not mapper_2:\n",
    "        return dict_doc\n",
    "\n",
    "    # Copy dictionary\n",
    "    dictionary = dict_doc.copy()\n",
    "    \n",
    "    # Get mapper based on its document.\n",
    "    if mapper_2 :\n",
    "        mapper_2 = mapper_2[idx_doc]\n",
    "\n",
    "    for key, value in dictionary.items():\n",
    "        temp = []\n",
    "        for s, a in value:\n",
    "            if s in mapper_1 or s in mapper_2:\n",
    "                # Append full value\n",
    "                temp.append((s, a))\n",
    "                    \n",
    "        # Update list of string\n",
    "        dictionary[key] = temp\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "pprint(df['review'].iloc[33])\n",
    "pprint(sample_ability)\n",
    "pprint(important_words_ability(sample_ability, 33, mapper_1=mapper_1, mapper_2=mapper_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "id": "645a2e72-d6d5-4aeb-8d24-da9e0d0898cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [],\n",
       " 1: [],\n",
       " 2: [],\n",
       " 3: [],\n",
       " 4: ['mcdonald'],\n",
       " 5: [],\n",
       " 6: ['food'],\n",
       " 7: ['location']}"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_aspect(x, idx, mapper_1=None, mapper_2=None): \n",
    "    # Prepare sentence\n",
    "    texts = remove_extra_spaces(x)\n",
    "    texts = expand_contractions(x)\n",
    "    texts = remove_non_ascii(x)\n",
    "\n",
    "    # Get aspect\n",
    "    doc = nlp(texts)\n",
    "    result = get_raw_aspects(doc)\n",
    "    result = prunning_aspect(result, doc)\n",
    "    result = important_words_aspect(result, idx_doc=idx, mapper_1=mapper_1, mapper_2=mapper_2)\n",
    "    return result\n",
    "\n",
    "text = \"Just spent 10 minutes waiting at this McDonald's .\" +\\\n",
    "        \"According to Google they're open 24/7. Finally we pull up to the window to see if anyone was there,\" +\\\n",
    "        \" sure enough one employee seated in the lobby and another at the window. She told me they were closed.\" +\\\n",
    "        \" Whoever these two are they need to be replaced. Get it together McDonald's. You're a corporate power house and you have a reputation to keep.\" +\\\n",
    "        \"One of the most unprofessional experiences I've ever had with fast food. 0/10 would not recomend this location.\"\n",
    "# text = \"We hit it off pretty good in the beginning everything was great but that didn\" +\\\n",
    "#         \"ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯\" +\\\n",
    "#         \"ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯\" +\\\n",
    "#         \"ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½t last long. I mean the service is fast but with attitude an\"\n",
    "process_aspect(text, 33, mapper_1, mapper_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "id": "d7fc5a2e-4afb-4cf0-a226-36a9de709a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part nine\n",
      "PRON 3\n",
      "Part four\n",
      "PRON 3\n",
      "Part seven\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part nine\n",
      "PRON 3\n",
      "Part one\n",
      "PRON 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_ability(x, idx, mapper_1=None, mapper_2=None):\n",
    "    # Prepare sentence\n",
    "    texts = remove_extra_spaces(x)\n",
    "    texts = expand_contractions(x)\n",
    "    texts = remove_non_ascii(x)\n",
    "\n",
    "    # Get aspect\n",
    "    doc = nlp(texts)\n",
    "    result = get_raw_abilities(doc)\n",
    "    result = important_words_ability(result, idx_doc=idx, mapper_1=mapper_1, mapper_2=mapper_2)\n",
    "    \n",
    "    return result\n",
    "\n",
    "text = \"Just spent 10 minutes waiting at this McDonald's .\" +\\\n",
    "        \"According to Google they're open 24/7. Finally we pull up to the window to see if anyone was there,\" +\\\n",
    "        \" sure enough one employee seated in the lobby and another at the window. She told me they were closed.\" +\\\n",
    "        \" Whoever these two are they need to be replaced. Get it together McDonald's. You're a corporate power house and you have a reputation to keep.\" +\\\n",
    "        \"One of the most unprofessional experiences I've ever had with fast food. 0/10 would not recomend this location.\"\n",
    "# text = \"We hit it off pretty good in the beginning everything was great but that didn\" +\\\n",
    "#         \"ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯\" +\\\n",
    "#         \"ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯\" +\\\n",
    "#         \"ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½t last long. I mean the service is fast but with attitude an\"\n",
    "# text = \"Mcdonalds is great but they really need to hire people who understand both english and spanish and not just spanish speakers trying to take english orders. \"+\\\n",
    "#         \"The people are really nice but be smarter about who you put in the drive thru and lobby to take orders.\"\n",
    "process_ability(text, 33, mapper_1, mapper_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "3b3ad815-98fd-4b95-aa6c-c03d890f73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process(x):\n",
    "#     # Prepare sentence\n",
    "#     texts = remove_extra_spaces(x)\n",
    "#     texts = expand_contractions(x)\n",
    "#     texts = remove_non_ascii(x)\n",
    "\n",
    "#     # Get aspect\n",
    "#     doc = nlp(texts)\n",
    "#     temp = []\n",
    "#     for token in doc:\n",
    "#         if token.pos_ == 'ADJ':\n",
    "#     result = get_raw_abilities(doc)\n",
    "#     result = important_words_ability(result, idx_doc=idx, mapper_1=mapper_1, mapper_2=mapper_2)\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "ab1e3412-2237-430e-a033-fd79f1927ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>aspect</th>\n",
       "      <th>ability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3 months ago</td>\n",
       "      <td>Why does it look like someone spit on my food?...</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: [], 1: ['normal transaction'], 2: ['substa...</td>\n",
       "      <td>{0: [], 1: [], 2: [], 3: []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>It'd McDonalds. It is what it is as far as the...</td>\n",
       "      <td>4</td>\n",
       "      <td>{0: [], 1: ['food', 'atmosphere'], 2: ['staff ...</td>\n",
       "      <td>{0: [], 1: [('food', 'go'), ('atmosphere', 'go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Made a mobile order got to the speaker and che...</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: ['mobile order'], 1: ['line'], 2: [], 3: [...</td>\n",
       "      <td>{0: [('order', 'got'), ('order', 'got to speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>a month ago</td>\n",
       "      <td>My mc. Crispy chicken sandwich was ÃÂ¯ÃÂ¿ÃÂ...</td>\n",
       "      <td>5</td>\n",
       "      <td>{0: [], 1: ['crispy chicken sandwich', 'custom...</td>\n",
       "      <td>{0: [], 1: [('service', 'was quick')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>I repeat my order 3 times in the drive thru, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: ['order', 'large meal double filet', 'larg...</td>\n",
       "      <td>{0: [], 1: []}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewer_id   review_time  \\\n",
       "0            1  3 months ago   \n",
       "1            2    5 days ago   \n",
       "2            3    5 days ago   \n",
       "3            4   a month ago   \n",
       "4            5  2 months ago   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  Why does it look like someone spit on my food?...       1   \n",
       "1  It'd McDonalds. It is what it is as far as the...       4   \n",
       "2  Made a mobile order got to the speaker and che...       1   \n",
       "3  My mc. Crispy chicken sandwich was ÃÂ¯ÃÂ¿ÃÂ...       5   \n",
       "4  I repeat my order 3 times in the drive thru, a...       1   \n",
       "\n",
       "                                              aspect  \\\n",
       "0  {0: [], 1: ['normal transaction'], 2: ['substa...   \n",
       "1  {0: [], 1: ['food', 'atmosphere'], 2: ['staff ...   \n",
       "2  {0: ['mobile order'], 1: ['line'], 2: [], 3: [...   \n",
       "3  {0: [], 1: ['crispy chicken sandwich', 'custom...   \n",
       "4  {0: ['order', 'large meal double filet', 'larg...   \n",
       "\n",
       "                                             ability  \n",
       "0                       {0: [], 1: [], 2: [], 3: []}  \n",
       "1  {0: [], 1: [('food', 'go'), ('atmosphere', 'go...  \n",
       "2  {0: [('order', 'got'), ('order', 'got to speak...  \n",
       "3             {0: [], 1: [('service', 'was quick')]}  \n",
       "4                                     {0: [], 1: []}  "
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['aspect'] = [process_aspect(text, i, mapper_1, mapper_2) for i, text in enumerate(df['review'].values)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "0a49a902-6845-44b8-a165-d9fd348bcbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part three\n",
      "Part five\n",
      "{0: [('someone', 'spit'), ('someone', 'spit on food')], 1: [], 2: [], 3: []}\n",
      "Part nine\n",
      "Part ten\n",
      "{0: [('someone', 'spit'), ('someone', 'spit on food')], 1: [('everyone', 'was chill'), ('everyone', 'was polite')], 2: [], 3: []}\n",
      "Part three\n",
      "{0: [], 1: [('food', 'go'), ('atmosphere', 'go')], 2: [], 3: [], 4: []}\n",
      "Part four\n",
      "{0: [], 1: [('food', 'go'), ('atmosphere', 'go')], 2: [('staff', 'does make difference')], 3: [], 4: []}\n",
      "Part seven\n",
      "Part eight\n",
      "PRON 2\n",
      "{0: [], 1: [('food', 'go'), ('atmosphere', 'go')], 2: [('staff', 'does make difference')], 3: [('staff', 'are friendly'), ('staff', 'are accommodating'), ('staff', 'are smiling')], 4: []}\n",
      "Part three\n",
      "Part five\n",
      "Part six\n",
      "{0: [('order', 'got'), ('order', 'got to speaker'), ('order', 'checked')], 1: [], 2: [], 3: [], 4: []}\n",
      "Part three\n",
      "{0: [('order', 'got'), ('order', 'got to speaker'), ('order', 'checked')], 1: [('Line', 'was not moving')], 2: [], 3: [], 4: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('order', 'got'), ('order', 'got to speaker'), ('order', 'checked')], 1: [('Line', 'was not moving')], 2: [], 3: [('Line', 'said')], 4: []}\n",
      "Part four\n",
      "{0: [('order', 'got'), ('order', 'got to speaker'), ('order', 'checked')], 1: [('Line', 'was not moving')], 2: [], 3: [('Line', 'said')], 4: [('manager', 'told me')]}\n",
      "Part seven\n",
      "{0: [], 1: [('service', 'was quick')]}\n",
      "Part three\n",
      "Part six\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [('lady', 'came')], 8: [], 9: [], 10: [], 11: [], 12: [], 13: []}\n",
      "Part four\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [('lady', 'came'), ('lady', 'was handling front counter')], 8: [], 9: [], 10: [], 11: [], 12: [], 13: []}\n",
      "Part three\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [('lady', 'came'), ('lady', 'was handling front counter')], 8: [('girl', 'appeared')], 9: [], 10: [], 11: [], 12: [], 13: []}\n",
      "Part three\n",
      "Part five\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [('lady', 'came'), ('lady', 'was handling front counter')], 8: [('girl', 'appeared')], 9: [], 10: [('complaint', 'not never got'), ('complaint', 'not never got to person')], 11: [], 12: [], 13: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [('lady', 'came'), ('lady', 'was handling front counter')], 8: [('girl', 'appeared')], 9: [], 10: [('complaint', 'not never got'), ('complaint', 'not never got to person'), ('complaint', 'ordered')], 11: [], 12: [], 13: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [('lady', 'came'), ('lady', 'was handling front counter')], 8: [('girl', 'appeared')], 9: [], 10: [('complaint', 'not never got'), ('complaint', 'not never got to person'), ('complaint', 'ordered')], 11: [], 12: [('complaint', 'do not even bother')], 13: []}\n",
      "Part three\n",
      "Part five\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part seven\n",
      "{0: [], 1: [('that', 'was full')], 2: [], 3: [], 4: [], 5: []}\n",
      "Part four\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part four\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [('lady', 'gave hard time')], 5: []}\n",
      "Part four\n",
      "{0: [('who', 'enjoy McDs')], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
      "Part seven\n",
      "{0: [('who', 'enjoy McDs')], 1: [('staff', 'is friendly')], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "{0: [('who', 'enjoy McDs')], 1: [('staff', 'is friendly')], 2: [], 3: [], 4: [], 5: [], 6: [('girls', 'joke')], 7: []}\n",
      "Part three\n",
      "{0: [('who', 'enjoy McDs')], 1: [('staff', 'is friendly')], 2: [], 3: [], 4: [], 5: [], 6: [('girls', 'joke'), ('staff', 'is going')], 7: []}\n",
      "{0: [], 1: [], 2: [('order', 'was supposed to have items'), ('order', 'was supposed to have drinks')], 3: [], 4: [], 5: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "{0: [], 1: [], 2: [('order', 'was supposed to have items'), ('order', 'was supposed to have drinks')], 3: [], 4: [('lady', 'ignored')], 5: []}\n",
      "Part three\n",
      "{0: [], 1: [], 2: [('order', 'was supposed to have items'), ('order', 'was supposed to have drinks')], 3: [], 4: [('lady', 'ignored')], 5: [('husband', 'went')]}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [('order', 'was supposed to have items'), ('order', 'was supposed to have drinks')], 3: [], 4: [('lady', 'ignored')], 5: [('husband', 'went'), ('husband', 'finished then')]}\n",
      "Part three\n",
      "{0: [('crew', 'seems')], 1: [], 2: [], 3: [], 4: []}\n",
      "Part nine\n",
      "Part ten\n",
      "{0: [('crew', 'seems')], 1: [('crew', 'is whole different experience'), ('crew', 'is lines')], 2: [], 3: [], 4: []}\n",
      "Part nine\n",
      "PRON 2\n",
      "{0: [('crew', 'seems')], 1: [('crew', 'is whole different experience'), ('crew', 'is lines')], 2: [('crew', 'was occurrence')], 3: [], 4: []}\n",
      "Part nine\n",
      "PRON 2\n",
      "{0: [('crew', 'seems')], 1: [('crew', 'is whole different experience'), ('crew', 'is lines')], 2: [('crew', 'was occurrence'), ('crew', 'is nightly problem')], 3: [], 4: []}\n",
      "Part seven\n",
      "{0: [('crew', 'seems')], 1: [('crew', 'is whole different experience'), ('crew', 'is lines')], 2: [('crew', 'was occurrence'), ('crew', 'is nightly problem')], 3: [('staff', 'is rude')], 4: []}\n",
      "Part three\n",
      "Part six\n",
      "PRON 2\n",
      "{0: [('crew', 'seems')], 1: [('crew', 'is whole different experience'), ('crew', 'is lines')], 2: [('crew', 'was occurrence'), ('crew', 'is nightly problem')], 3: [('staff', 'is rude')], 4: [('staff', 'will mistake well'), ('staff', 'will serve')]}\n",
      "Part nine\n",
      "Part ten\n",
      "{0: [], 1: [], 2: [('this', 'was hiccup'), ('this', 'was not thing')], 3: []}\n",
      "Part nine\n",
      "{0: [], 1: [], 2: [('this', 'was hiccup'), ('this', 'was not thing')], 3: [('egg', 'was size')]}\n",
      "Part three\n",
      "{0: [], 1: [('One', 'not fattened')], 2: []}\n",
      "Part four\n",
      "Part five\n",
      "{0: [], 1: [('One', 'not fattened')], 2: [('girl', 'handing order'), ('girl', 'handing at window'), ('girl', 'handing on January'), ('girl', 'handing at pm')]}\n",
      "Part nine\n",
      "{0: [('This', 'was dine')], 1: [], 2: []}\n",
      "Part seven\n",
      "{0: [('This', 'was dine')], 1: [('tables', 'were dirty')], 2: []}\n",
      "Part three\n",
      "Part six\n",
      "{0: [('line', 'was moving slowly'), ('line', 'was were'), ('line', 'was gave')]}\n",
      "Part four\n",
      "PRON 2\n",
      "{0: [('line', 'was moving slowly'), ('line', 'was were'), ('line', 'was gave'), ('line', 'gave cinnabun')]}\n",
      "Part seven\n",
      "{0: [('line', 'was moving slowly'), ('line', 'was were'), ('line', 'was gave'), ('line', 'gave cinnabun'), ('staff', 'was apologetic')]}\n",
      "Part seven\n",
      "PRON 2\n",
      "{0: [('line', 'was moving slowly'), ('line', 'was were'), ('line', 'was gave'), ('line', 'gave cinnabun'), ('staff', 'was apologetic'), ('line', 'were staffed')]}\n",
      "Part three\n",
      "Part six\n",
      "{0: [('boy', 'made'), ('boy', 'threw')], 1: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('boy', 'made'), ('boy', 'threw'), ('boy', 'attended')], 1: []}\n",
      "Part seven\n",
      "{0: [('boy', 'made'), ('boy', 'threw'), ('boy', 'attended'), ('drink', 'was wrong')], 1: []}\n",
      "Part three\n",
      "Part five\n",
      "{0: [], 1: [('smoothie', 'should not never taste'), ('smoothie', 'should not never taste like that')], 2: [], 3: [], 4: []}\n",
      "Part three\n",
      "Part six\n",
      "{0: [('staff', 'seems'), ('staff', 'is')]}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('staff', 'seems'), ('staff', 'is'), ('staff', 'makes')]}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('staff', 'seems'), ('staff', 'is'), ('staff', 'makes'), ('staff', 'can also lead')]}\n",
      "Part seven\n",
      "{0: [('staff', 'seems'), ('staff', 'is'), ('staff', 'makes'), ('staff', 'can also lead'), ('lobby', 'is clean')]}\n",
      "Part three\n",
      "{0: [], 1: [('employee', 'directly asks')], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 13: [], 14: [], 15: [], 16: [], 17: [], 18: [], 19: [], 20: [], 21: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part four\n",
      "{0: [], 1: [('employee', 'directly asks')], 2: [('cars', 'placing order')], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 13: [], 14: [], 15: [], 16: [], 17: [], 18: [], 19: [], 20: [], 21: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "Part five\n",
      "PRON 2\n",
      "{0: [], 1: [('employee', 'directly asks')], 2: [('cars', 'placing order')], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [('cars', 'talks'), ('cars', 'talks to customers'), ('cars', 'talks like this')], 10: [], 11: [], 12: [], 13: [], 14: [], 15: [], 16: [], 17: [], 18: [], 19: [], 20: [], 21: []}\n",
      "Part four\n",
      "{0: [], 1: [('employee', 'directly asks')], 2: [('cars', 'placing order')], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [('cars', 'talks'), ('cars', 'talks to customers'), ('cars', 'talks like this')], 10: [('name', 'to put review')], 11: [], 12: [], 13: [], 14: [], 15: [], 16: [], 17: [], 18: [], 19: [], 20: [], 21: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "{0: [], 1: [('employee', 'directly asks')], 2: [('cars', 'placing order')], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [('cars', 'talks'), ('cars', 'talks to customers'), ('cars', 'talks like this')], 10: [('name', 'to put review')], 11: [], 12: [], 13: [], 14: [], 15: [], 16: [], 17: [], 18: [], 19: [], 20: [('receipt', 'states')], 21: []}\n",
      "Part seven\n",
      "PRON 2\n",
      "{0: [], 1: [('employee', 'directly asks')], 2: [('cars', 'placing order')], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [('cars', 'talks'), ('cars', 'talks to customers'), ('cars', 'talks like this')], 10: [('name', 'to put review')], 11: [], 12: [], 13: [], 14: [], 15: [], 16: [], 17: [], 18: [], 19: [], 20: [('receipt', 'states')], 21: [('receipt', 'was intolerable')]}\n",
      "Part seven\n",
      "PRON 3\n",
      "Part seven\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [('order', 'is correct')], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 13: []}\n",
      "Part seven\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [('order', 'is correct')], 6: [('order', 'was correct')], 7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 13: []}\n",
      "Part nine\n",
      "Part ten\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [('order', 'is correct')], 6: [('order', 'was correct')], 7: [], 8: [('order', 'was large fries'), ('order', 'was nothing')], 9: [], 10: [], 11: [], 12: [], 13: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part seven\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [('order', 'is correct')], 6: [('order', 'was correct')], 7: [], 8: [('order', 'was large fries'), ('order', 'was nothing')], 9: [], 10: [('few', 'were hot')], 11: [], 12: [], 13: []}\n",
      "Part seven\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [('order', 'is correct')], 6: [('order', 'was correct')], 7: [], 8: [('order', 'was large fries'), ('order', 'was nothing')], 9: [], 10: [('few', 'were hot')], 11: [('staff', 'is pleasant')], 12: [], 13: []}\n",
      "Part four\n",
      "Part six\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [('order', 'is correct')], 6: [('order', 'was correct')], 7: [], 8: [('order', 'was large fries'), ('order', 'was nothing')], 9: [], 10: [('few', 'were hot')], 11: [('staff', 'is pleasant'), ('staff', 'leaves lot'), ('staff', 'thinking')], 12: [], 13: []}\n",
      "Part three\n",
      "Part five\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [('order', 'is correct')], 6: [('order', 'was correct')], 7: [], 8: [('order', 'was large fries'), ('order', 'was nothing')], 9: [], 10: [('few', 'were hot')], 11: [('staff', 'is pleasant'), ('staff', 'leaves lot'), ('staff', 'thinking'), ('staff', 'can talk negatively'), ('staff', 'can talk about people'), ('staff', 'can talk in open')], 12: [], 13: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [('order', 'is correct')], 6: [('order', 'was correct')], 7: [], 8: [('order', 'was large fries'), ('order', 'was nothing')], 9: [], 10: [('few', 'were hot')], 11: [('staff', 'is pleasant'), ('staff', 'leaves lot'), ('staff', 'thinking'), ('staff', 'can talk negatively'), ('staff', 'can talk about people'), ('staff', 'can talk in open'), ('staff', 'understand perfectly')], 12: [], 13: []}\n",
      "Part seven\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [('order', 'is correct')], 6: [('order', 'was correct')], 7: [], 8: [('order', 'was large fries'), ('order', 'was nothing')], 9: [], 10: [('few', 'were hot')], 11: [('staff', 'is pleasant'), ('staff', 'leaves lot'), ('staff', 'thinking'), ('staff', 'can talk negatively'), ('staff', 'can talk about people'), ('staff', 'can talk in open'), ('staff', 'understand perfectly')], 12: [], 13: [('staff', 'is wrong')]}\n",
      "Part three\n",
      "Part five\n",
      "{0: [], 1: [('staff', 'just shoved'), ('staff', 'just shoved in hands')], 2: []}\n",
      "Part nine\n",
      "{0: [('This', 'is worst meal')], 1: [], 2: [], 3: [], 4: []}\n",
      "Part five\n",
      "PRON 3\n",
      "Part seven\n",
      "{0: [('McDonalds', 'is proud')], 1: []}\n",
      "Part one\n",
      "PRON 2\n",
      "{0: [('McDonalds', 'is proud')], 1: [('McDonalds', 'have cream')]}\n",
      "Part three\n",
      "{0: [('McDonalds', 'is proud')], 1: [('McDonalds', 'have cream'), ('lady', 'replied')]}\n",
      "Part three\n",
      "{0: [('everything', 'looks')]}\n",
      "Part four\n",
      "PRON 3\n",
      "Part three\n",
      "{0: [], 1: [], 2: [('That', 's')], 3: []}\n",
      "Part seven\n",
      "{0: [('location', 'is wack')], 1: [], 2: [], 3: []}\n",
      "Part three\n",
      "Part six\n",
      "PRON 2\n",
      "{0: [('location', 'is wack')], 1: [('location', 'will literally stop'), ('location', 'will make')], 2: [], 3: []}\n",
      "Part three\n",
      "Part five\n",
      "{0: [('location', 'is wack')], 1: [('location', 'will literally stop'), ('location', 'will make'), ('people', 'sit'), ('people', 'sit in lot'), ('people', 'sit for minutes'), ('people', 'sit until a.m.')], 2: [], 3: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('location', 'is wack')], 1: [('location', 'will literally stop'), ('location', 'will make'), ('people', 'sit'), ('people', 'sit in lot'), ('people', 'sit for minutes'), ('people', 'sit until a.m.'), ('people', 'do not get anymore')], 2: [], 3: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('location', 'is wack')], 1: [('location', 'will literally stop'), ('location', 'will make'), ('people', 'sit'), ('people', 'sit in lot'), ('people', 'sit for minutes'), ('people', 'sit until a.m.'), ('people', 'do not get anymore')], 2: [('people', 'do')], 3: []}\n",
      "Part three\n",
      "Part five\n",
      "{0: [('location', 'is wack')], 1: [('location', 'will literally stop'), ('location', 'will make'), ('people', 'sit'), ('people', 'sit in lot'), ('people', 'sit for minutes'), ('people', 'sit until a.m.'), ('people', 'do not get anymore')], 2: [('people', 'do')], 3: [('McDonald', 'would not shown'), ('McDonald', 'would not shown at 2:30'), ('McDonald', 'would not shown in morning')]}\n",
      "Part three\n",
      "PRON 3\n",
      "Part nine\n",
      "{0: [], 1: [('This', 'is extra pickles')]}\n",
      "Part nine\n",
      "PRON 3\n",
      "Part four\n",
      "PRON 3\n",
      "Part seven\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part nine\n",
      "PRON 3\n",
      "Part one\n",
      "PRON 3\n",
      "Part seven\n",
      "{0: [('everything', 'was great')], 1: []}\n",
      "Part three\n",
      "{0: [('everything', 'was great'), ('that', 'did not last long')], 1: []}\n",
      "Part seven\n",
      "Part eight\n",
      "{0: [('everything', 'was great'), ('that', 'did not last long')], 1: [('service', 'is fast'), ('service', 'is with')]}\n",
      "Part nine\n",
      "{0: [('location', 'is horrible store')], 1: [], 2: [], 3: []}\n",
      "Part three\n",
      "{0: [('location', 'is horrible store')], 1: [('Delgado', 'does not know')], 2: [], 3: []}\n",
      "Part three\n",
      "{0: [('location', 'is horrible store')], 1: [('Delgado', 'does not know')], 2: [('Supervisor', 'does not care')], 3: []}\n",
      "Part three\n",
      "{0: [('location', 'is horrible store')], 1: [('Delgado', 'does not know')], 2: [('Supervisor', 'does not care'), ('Eduardo', 'does not care')], 3: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('location', 'is horrible store')], 1: [('Delgado', 'does not know')], 2: [('Supervisor', 'does not care'), ('Eduardo', 'does not care'), ('Supervisor', 'ran either')], 3: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('location', 'is horrible store')], 1: [('Delgado', 'does not know')], 2: [('Supervisor', 'does not care'), ('Eduardo', 'does not care'), ('Supervisor', 'ran either')], 3: [('Supervisor', 'used')]}\n",
      "Part nine\n",
      "{0: [('This', 'is 3rd time')]}\n",
      "Part seven\n",
      "{0: [('staff', 'are friendly')]}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('staff', 'are friendly'), ('staff', 'do perfectly')]}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [], 1: [('deluxe', 'said')]}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [], 1: [('deluxe', 'said'), ('deluxe', 'would fix then')]}\n",
      "Part four\n",
      "PRON 3\n",
      "Part nine\n",
      "{0: [('This', 'is EVER')], 1: [], 2: [], 3: [], 4: [], 5: [], 6: []}\n",
      "Part three\n",
      "Part six\n",
      "{0: [('This', 'is EVER')], 1: [('1130am', 'could barely hear'), ('1130am', 'could yelling')], 2: [], 3: [], 4: [], 5: [], 6: []}\n",
      "Part four\n",
      "PRON 3\n",
      "Part three\n",
      "{0: [('This', 'is EVER')], 1: [('1130am', 'could barely hear'), ('1130am', 'could yelling')], 2: [], 3: [('staff', 'looking')], 4: [], 5: [], 6: []}\n",
      "Part seven\n",
      "PRON 3\n",
      "Part three\n",
      "Part five\n",
      "PRON 3\n",
      "Part four\n",
      "Part five\n",
      "PRON 2\n",
      "{0: [('This', 'is EVER')], 1: [('1130am', 'could barely hear'), ('1130am', 'could yelling')], 2: [], 3: [('staff', 'looking')], 4: [], 5: [], 6: [('staff', 'now wear sets'), ('staff', 'now wear except worst leadership')]}\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "Part five\n",
      "PRON 3\n",
      "Part three\n",
      "{0: [], 1: [('food', 'tasted')]}\n",
      "Part seven\n",
      "Part eight\n",
      "{0: [], 1: [('food', 'is hot'), ('food', 'is fresh')]}\n",
      "Part seven\n",
      "{0: [('employee', 'was rude')]}\n",
      "Part four\n",
      "PRON 3\n",
      "Part four\n",
      "PRON 3\n",
      "Part seven\n",
      "{0: [], 1: [], 2: [('This', 'is not right')], 3: [], 4: [], 5: [], 6: []}\n",
      "Part three\n",
      "{0: [], 1: [], 2: [('This', 'is not right'), ('management', 'needs')], 3: [], 4: [], 5: [], 6: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [('This', 'is not right'), ('management', 'needs')], 3: [], 4: [('management', 'expect')], 5: [], 6: []}\n",
      "Part three\n",
      "Part five\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [('This', 'is not right'), ('management', 'needs')], 3: [], 4: [('management', 'expect'), ('management', 'should come'), ('management', 'should come in large')], 5: [], 6: []}\n",
      "Part seven\n",
      "{0: [], 1: [], 2: [('This', 'is not right'), ('management', 'needs')], 3: [], 4: [('management', 'expect'), ('management', 'should come'), ('management', 'should come in large')], 5: [('reviews', 'are low')], 6: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part seven\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [], 3: [('one', \"'s wrong\")], 4: []}\n",
      "Part three\n",
      "Part five\n",
      "PRON 3\n",
      "Part seven\n",
      "PRON 3\n",
      "Part four\n",
      "Part six\n",
      "PRON 3\n",
      "Part seven\n",
      "{0: [('night', 'was ridiculous')], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
      "Part three\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken')], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
      "Part four\n",
      "PRON 2\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [('order', 'told me')], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
      "Part four\n",
      "PRON 2\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [('order', 'told me'), ('order', 'could not take it')], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
      "Part three\n",
      "Part six\n",
      "PRON 2\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [('order', 'told me'), ('order', 'could not take it')], 3: [], 4: [('order', 'proceeded'), ('order', \"'s\")], 5: [], 6: [], 7: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [('order', 'told me'), ('order', 'could not take it')], 3: [], 4: [('order', 'proceeded'), ('order', \"'s\"), ('order', 'closed')], 5: [], 6: [], 7: []}\n",
      "Part three\n",
      "Part six\n",
      "PRON 2\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [('order', 'told me'), ('order', 'could not take it')], 3: [], 4: [('order', 'proceeded'), ('order', \"'s\"), ('order', 'closed'), ('order', 'claimed'), ('order', 'had')], 5: [], 6: [], 7: []}\n",
      "Part seven\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [('order', 'told me'), ('order', 'could not take it')], 3: [], 4: [('order', 'proceeded'), ('order', \"'s\"), ('order', 'closed'), ('order', 'claimed'), ('order', 'had')], 5: [('lane', 'was open')], 6: [], 7: []}\n",
      "Part nine\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [('order', 'told me'), ('order', 'could not take it')], 3: [], 4: [('order', 'proceeded'), ('order', \"'s\"), ('order', 'closed'), ('order', 'claimed'), ('order', 'had')], 5: [('lane', 'was open'), ('sign', 'was sign')], 6: [], 7: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [('order', 'told me'), ('order', 'could not take it')], 3: [], 4: [('order', 'proceeded'), ('order', \"'s\"), ('order', 'closed'), ('order', 'claimed'), ('order', 'had')], 5: [('lane', 'was open'), ('sign', 'was sign'), ('lane', 'were talking')], 6: [], 7: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [('order', 'told me'), ('order', 'could not take it')], 3: [], 4: [('order', 'proceeded'), ('order', \"'s\"), ('order', 'closed'), ('order', 'claimed'), ('order', 'had')], 5: [('lane', 'was open'), ('sign', 'was sign'), ('lane', 'were talking'), ('lane', 'had placed next')], 6: [], 7: []}\n",
      "Part three\n",
      "Part six\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [('order', 'told me'), ('order', 'could not take it')], 3: [], 4: [('order', 'proceeded'), ('order', \"'s\"), ('order', 'closed'), ('order', 'claimed'), ('order', 'had')], 5: [('lane', 'was open'), ('sign', 'was sign'), ('lane', 'were talking'), ('lane', 'had placed next')], 6: [('lane', 'looked'), ('lane', 'proceeded'), ('lane', 'slammed')], 7: []}\n",
      "Part three\n",
      "Part six\n",
      "PRON 2\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [('order', 'told me'), ('order', 'could not take it')], 3: [], 4: [('order', 'proceeded'), ('order', \"'s\"), ('order', 'closed'), ('order', 'claimed'), ('order', 'had')], 5: [('lane', 'was open'), ('sign', 'was sign'), ('lane', 'were talking'), ('lane', 'had placed next')], 6: [('lane', 'looked'), ('lane', 'proceeded'), ('lane', 'slammed'), ('lane', 'still proceeded'), ('lane', 'slammed')], 7: []}\n",
      "Part three\n",
      "{0: [('night', 'was ridiculous')], 1: [('order', 'taken'), ('line', 'was wrapped almost TWICE around building')], 2: [('order', 'told me'), ('order', 'could not take it')], 3: [], 4: [('order', 'proceeded'), ('order', \"'s\"), ('order', 'closed'), ('order', 'claimed'), ('order', 'had')], 5: [('lane', 'was open'), ('sign', 'was sign'), ('lane', 'were talking'), ('lane', 'had placed next')], 6: [('lane', 'looked'), ('lane', 'proceeded'), ('lane', 'slammed'), ('lane', 'still proceeded'), ('lane', 'slammed')], 7: [('managers', 'are reading')]}\n",
      "Part four\n",
      "PRON 3\n",
      "Part three\n",
      "{0: [('McDonalds', 'had forgotten')], 1: [], 2: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('McDonalds', 'had forgotten'), ('McDonalds', 'early completely refused')], 1: [], 2: []}\n",
      "Part four\n",
      "PRON 3\n",
      "Part seven\n",
      "Part eight\n",
      "{0: [('Representative', 'was rude'), ('Representative', 'was disrespectful')], 1: [], 2: []}\n",
      "Part three\n",
      "Part five\n",
      "{0: [('Representative', 'was rude'), ('Representative', 'was disrespectful')], 1: [], 2: [('Manager', 'pretended'), ('Manager', 'pretended like nothing')]}\n",
      "Part three\n",
      "Part six\n",
      "PRON 3\n",
      "Part three\n",
      "{0: [], 1: [], 2: [('NVR', 'get')], 3: [], 4: [], 5: [], 6: [], 7: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [('NVR', 'get')], 3: [('NVR', 'not ai')], 4: [], 5: [], 6: [], 7: []}\n",
      "Part seven\n",
      "{0: [], 1: [], 2: [('NVR', 'get')], 3: [('NVR', 'not ai')], 4: [('service', 'is better')], 5: [], 6: [], 7: []}\n",
      "Part four\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [('NVR', 'get')], 3: [('NVR', 'not ai')], 4: [('service', 'is better')], 5: [('service', 'love it')], 6: [], 7: []}\n",
      "Part three\n",
      "Part five\n",
      "{0: [], 1: [('one', 'came'), ('one', 'came with food')], 2: [], 3: [], 4: []}\n",
      "Part seven\n",
      "PRON 2\n",
      "{0: [], 1: [('one', 'came'), ('one', 'came with food')], 2: [('one', 'is not good')], 3: [], 4: []}\n",
      "Part three\n",
      "Part six\n",
      "{0: [], 1: [('one', 'came'), ('one', 'came with food')], 2: [('one', 'is not good')], 3: [('Lots', 'got'), ('Lots', 'cared')], 4: []}\n",
      "Part three\n",
      "Part five\n",
      "{0: [], 1: [('one', 'came'), ('one', 'came with food')], 2: [('one', 'is not good')], 3: [('Lots', 'got'), ('Lots', 'cared'), ('one', 'cared'), ('one', 'cared about driver')], 4: []}\n",
      "Part three\n",
      "Part five\n",
      "PRON 2\n",
      "{0: [], 1: [('one', 'came'), ('one', 'came with food')], 2: [('one', 'is not good')], 3: [('Lots', 'got'), ('Lots', 'cared'), ('one', 'cared'), ('one', 'cared about driver'), ('Lots', 'is already waiting'), ('Lots', 'is already waiting from long time')], 4: []}\n",
      "Part three\n",
      "{0: [], 1: [('that', 'then then means')], 2: [], 3: [], 4: [], 5: []}\n",
      "Part seven\n",
      "{0: [], 1: [('that', 'then then means'), ('place', 'is open')], 2: [], 3: [], 4: [], 5: []}\n",
      "Part three\n",
      "{0: [], 1: [('that', 'then then means'), ('place', 'is open')], 2: [('people', 'to work here')], 3: [], 4: [], 5: []}\n",
      "Part three\n",
      "Part six\n",
      "PRON 2\n",
      "{0: [], 1: [('that', 'then then means'), ('place', 'is open')], 2: [('people', 'to work here')], 3: [('people', 'do shower'), ('people', 'do brush'), ('people', 'do change')], 4: [], 5: []}\n",
      "Part nine\n",
      "PRON 2\n",
      "{0: [], 1: [('that', 'then then means'), ('place', 'is open')], 2: [('people', 'to work here')], 3: [('people', 'do shower'), ('people', 'do brush'), ('people', 'do change')], 4: [('people', 'are robots')], 5: []}\n",
      "Part nine\n",
      "{0: [], 1: [('that', 'then then means'), ('place', 'is open')], 2: [('people', 'to work here')], 3: [('people', 'do shower'), ('people', 'do brush'), ('people', 'do change')], 4: [('people', 'are robots')], 5: [('clones', \"'s Dolly\")]}\n",
      "Part nine\n",
      "PRON 2\n",
      "{0: [], 1: [('that', 'then then means'), ('place', 'is open')], 2: [('people', 'to work here')], 3: [('people', 'do shower'), ('people', 'do brush'), ('people', 'do change')], 4: [('people', 'are robots')], 5: [('clones', \"'s Dolly\"), ('clones', \"'s Dolly\")]}\n",
      "Part nine\n",
      "PRON 2\n",
      "{0: [], 1: [('that', 'then then means'), ('place', 'is open')], 2: [('people', 'to work here')], 3: [('people', 'do shower'), ('people', 'do brush'), ('people', 'do change')], 4: [('people', 'are robots')], 5: [('clones', \"'s Dolly\"), ('clones', \"'s Dolly\"), ('clones', \"'s hot NGL\")]}\n",
      "Part seven\n",
      "{0: [('order', 'was wrong')]}\n",
      "Part three\n",
      "{0: [('order', 'was wrong'), ('person', 'needs')]}\n",
      "Part nine\n",
      "{0: [('manager', 'IS RUDE')], 1: [], 2: [], 3: []}\n",
      "Part four\n",
      "Part six\n",
      "{0: [('manager', 'IS RUDE')], 1: [], 2: [('DISCRIPTION', 'SPANISH TALL'), ('DISCRIPTION', 'PULLED')], 3: []}\n",
      "Part nine\n",
      "{0: [('manager', 'IS RUDE')], 1: [], 2: [('DISCRIPTION', 'SPANISH TALL'), ('DISCRIPTION', 'PULLED')], 3: [('THRY', 'WERE not GOOD NEED')]}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('manager', 'IS RUDE')], 1: [], 2: [('DISCRIPTION', 'SPANISH TALL'), ('DISCRIPTION', 'PULLED')], 3: [('THRY', 'WERE not GOOD NEED'), ('THRY', 'IS GOING')]}\n",
      "Part five\n",
      "{0: [('mother', 'was again asking for coke')]}\n",
      "Part three\n",
      "Part six\n",
      "{0: [('mother', 'was again asking for coke'), ('lady', 'became'), ('lady', 'said')]}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('mother', 'was again asking for coke'), ('lady', 'became'), ('lady', 'said'), ('mother', 'said')]}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('mother', 'was again asking for coke'), ('lady', 'became'), ('lady', 'said'), ('mother', 'said'), ('mother', 'said')]}\n",
      "Part three\n",
      "{0: [], 1: [('lady', 'kept')], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: []}\n",
      "Part three\n",
      "Part five\n",
      "PRON 2\n",
      "{0: [], 1: [('lady', 'kept')], 2: [('lady', 'almost broke'), ('lady', 'almost broke with stuff')], 3: [], 4: [], 5: [], 6: [], 7: [], 8: []}\n",
      "Part three\n",
      "Part five\n",
      "Part six\n",
      "PRON 2\n",
      "{0: [], 1: [('lady', 'kept')], 2: [('lady', 'almost broke'), ('lady', 'almost broke with stuff')], 3: [], 4: [], 5: [('lady', 'just dismissed'), ('lady', 'just dismissed to 2nd window'), ('lady', 'not were')], 6: [], 7: [], 8: []}\n",
      "Part four\n",
      "PRON 2\n",
      "{0: [], 1: [('lady', 'kept')], 2: [('lady', 'almost broke'), ('lady', 'almost broke with stuff')], 3: [], 4: [], 5: [('lady', 'just dismissed'), ('lady', 'just dismissed to 2nd window'), ('lady', 'not were')], 6: [], 7: [], 8: [('receipt', 'to take ownership')]}\n",
      "Part three\n",
      "Part six\n",
      "PRON 2\n",
      "{0: [], 1: [('lady', 'kept')], 2: [('lady', 'almost broke'), ('lady', 'almost broke with stuff')], 3: [], 4: [], 5: [('lady', 'just dismissed'), ('lady', 'just dismissed to 2nd window'), ('lady', 'not were')], 6: [], 7: [], 8: [('receipt', 'to take ownership'), ('receipt', 'could possibly went'), ('receipt', 'could show'), ('receipt', 'could being')]}\n",
      "Part three\n",
      "Part five\n",
      "{0: [], 1: [('lady', 'kept')], 2: [('lady', 'almost broke'), ('lady', 'almost broke with stuff')], 3: [], 4: [], 5: [('lady', 'just dismissed'), ('lady', 'just dismissed to 2nd window'), ('lady', 'not were')], 6: [], 7: [], 8: [('receipt', 'to take ownership'), ('receipt', 'could possibly went'), ('receipt', 'could show'), ('receipt', 'could being'), ('manager', 'dismissed'), ('manager', 'dismissed with condescending')]}\n",
      "Part three\n",
      "Part six\n",
      "{0: [('employees', 'seem'), ('employees', \"'re\")], 1: []}\n",
      "Part seven\n",
      "Part eight\n",
      "PRON 2\n",
      "{0: [('employees', 'seem'), ('employees', \"'re\"), ('order', \"'re quick\"), ('order', \"'re nice\")], 1: []}\n",
      "Part seven\n",
      "{0: [('employees', 'seem'), ('employees', \"'re\"), ('order', \"'re quick\"), ('order', \"'re nice\")], 1: [('prices', 'are lower than other locations')]}\n",
      "Part three\n",
      "Part six\n",
      "PRON 3\n",
      "{0: [], 1: [('room', 'is closed for reason')], 2: [], 3: []}\n",
      "Part three\n",
      "Part five\n",
      "{0: [], 1: [('room', 'is closed for reason'), ('fun', 'getting back'), ('fun', 'getting in line'), ('fun', 'getting for sauce')], 2: [], 3: []}\n",
      "Part seven\n",
      "{0: [], 1: [('room', 'is closed for reason'), ('fun', 'getting back'), ('fun', 'getting in line'), ('fun', 'getting for sauce')], 2: [('fries', 'are not great')], 3: []}\n",
      "Part nine\n",
      "PRON 2\n",
      "{0: [], 1: [('room', 'is closed for reason'), ('fun', 'getting back'), ('fun', 'getting in line'), ('fun', 'getting for sauce')], 2: [('fries', 'are not great'), ('fries', 'is huge part')], 3: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part seven\n",
      "{0: [('staff', 'been polite')], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: []}\n",
      "Part one\n",
      "{0: [('staff', 'been polite')], 1: [('Drive', 'have issues')], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('staff', 'been polite')], 1: [('Drive', 'have issues')], 2: [('staff', 'constantly get')], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part seven\n",
      "PRON 2\n",
      "{0: [('staff', 'been polite')], 1: [('Drive', 'have issues')], 2: [('staff', 'constantly get')], 3: [], 4: [], 5: [('staff', 'are new')], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: []}\n",
      "Part four\n",
      "Part six\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('staff', 'been polite')], 1: [('Drive', 'have issues')], 2: [('staff', 'constantly get')], 3: [], 4: [], 5: [('staff', 'are new')], 6: [], 7: [('staff', 'repeat back correctly')], 8: [], 9: [], 10: [], 11: [], 12: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('staff', 'been polite')], 1: [('Drive', 'have issues')], 2: [('staff', 'constantly get')], 3: [], 4: [], 5: [('staff', 'are new')], 6: [], 7: [('staff', 'repeat back correctly'), ('staff', 'get')], 8: [], 9: [], 10: [], 11: [], 12: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('staff', 'been polite')], 1: [('Drive', 'have issues')], 2: [('staff', 'constantly get')], 3: [], 4: [], 5: [('staff', 'are new')], 6: [], 7: [('staff', 'repeat back correctly'), ('staff', 'get')], 8: [('staff', 'are reading')], 9: [], 10: [], 11: [], 12: []}\n",
      "Part three\n",
      "Part five\n",
      "{0: [('staff', 'been polite')], 1: [('Drive', 'have issues')], 2: [('staff', 'constantly get')], 3: [], 4: [], 5: [('staff', 'are new')], 6: [], 7: [('staff', 'repeat back correctly'), ('staff', 'get')], 8: [('staff', 'are reading')], 9: [], 10: [], 11: [('tea', 'has tasted'), ('tea', 'has tasted like soap'), ('tea', 'has tasted like drinks')], 12: []}\n",
      "Part seven\n",
      "{0: [('staff', 'been polite')], 1: [('Drive', 'have issues')], 2: [('staff', 'constantly get')], 3: [], 4: [], 5: [('staff', 'are new')], 6: [], 7: [('staff', 'repeat back correctly'), ('staff', 'get')], 8: [('staff', 'are reading')], 9: [], 10: [], 11: [('tea', 'has tasted'), ('tea', 'has tasted like soap'), ('tea', 'has tasted like drinks')], 12: [('fries', 'been cooked')]}\n",
      "Part seven\n",
      "{0: [('McDonald', 'is bad about shorts')], 1: [], 2: [], 3: []}\n",
      "Part nine\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('McDonald', 'is bad about shorts')], 1: [('McDonald', 'then tend')], 2: [], 3: []}\n",
      "Part three\n",
      "{0: [('McDonald', 'is bad about shorts')], 1: [('McDonald', 'then tend')], 2: [], 3: [('self', 'going back soon')]}\n",
      "Part three\n",
      "{0: [('Uber', 'eats')], 1: [], 2: []}\n",
      "Part one\n",
      "{0: [('Uber', 'eats'), ('eats', 'have issues')], 1: [], 2: []}\n",
      "Part nine\n",
      "PRON 2\n",
      "{0: [('Uber', 'eats'), ('eats', 'have issues')], 1: [('Uber', 'is not fault')], 2: []}\n",
      "Part three\n",
      "Part six\n",
      "PRON 2\n",
      "{0: [('Uber', 'eats'), ('eats', 'have issues')], 1: [('Uber', 'is not fault'), ('Uber', 'answered'), ('Uber', 'ignored'), ('Uber', 'left')], 2: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('Uber', 'eats'), ('eats', 'have issues')], 1: [('Uber', 'is not fault'), ('Uber', 'answered'), ('Uber', 'ignored'), ('Uber', 'left'), ('Uber', 'got')], 2: []}\n",
      "Part nine\n",
      "{0: [('Uber', 'eats'), ('eats', 'have issues')], 1: [('Uber', 'is not fault'), ('Uber', 'answered'), ('Uber', 'ignored'), ('Uber', 'left'), ('Uber', 'got')], 2: [('places', 'are short staffed')]}\n",
      "Part four\n",
      "PRON 3\n",
      "Part three\n",
      "{0: [('ticket', 'says')], 1: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('ticket', 'says')], 1: [('ticket', 'just wanted')]}\n",
      "Part seven\n",
      "{0: [('service', 'is good')], 1: [], 2: []}\n",
      "Part three\n",
      "{0: [('service', 'is good'), ('staff', 'play')], 1: [], 2: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('service', 'is good'), ('staff', 'play'), ('service', 'horsing around')], 1: [], 2: []}\n",
      "Part nine\n",
      "{0: [('service', 'is good'), ('staff', 'play'), ('service', 'horsing around')], 1: [('management', 'are jock')], 2: []}\n",
      "Part nine\n",
      "{0: [('nuggets', 'were way')]}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('nuggets', 'were way'), ('nuggets', 'means')]}\n",
      "Part three\n",
      "Part five\n",
      "Part six\n",
      "{0: [('nuggets', 'were way'), ('nuggets', 'means'), ('cashier', 'cut'), ('cashier', 'cut in middle'), ('cashier', 'was')]}\n",
      "Part seven\n",
      "{0: [('worker', 'was tired as hell')], 1: [], 2: [], 3: [], 4: []}\n",
      "Part seven\n",
      "{0: [('all', 'was sorry for wait')], 1: [], 2: [], 3: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "Part six\n",
      "PRON 3\n",
      "Part nine\n",
      "{0: [], 1: [], 2: [('location', 'is place')], 3: [], 4: [], 5: []}\n",
      "Part three\n",
      "{0: [], 1: [], 2: [('location', 'is place')], 3: [('order', 'will missing')], 4: [], 5: []}\n",
      "Part four\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [('location', 'is place')], 3: [('order', 'will missing')], 4: [], 5: [('location', 'to correct mistakes')]}\n",
      "Part seven\n",
      "Part eight\n",
      "{0: [], 1: [], 2: [('location', 'is place')], 3: [('order', 'will missing')], 4: [], 5: [('location', 'to correct mistakes'), ('rest', 'be cold'), ('rest', 'be soggy')]}\n",
      "Part seven\n",
      "{0: [('Service', 'is slow')], 1: []}\n",
      "Part seven\n",
      "{0: [('cashier', 'are nice')], 1: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('cashier', 'are nice')], 1: [('cashier', 'get')]}\n",
      "Part three\n",
      "{0: [('cashier', 'are nice')], 1: [('cashier', 'get'), ('food', 'done fast')]}\n",
      "Part three\n",
      "Part five\n",
      "{0: [('Manager', 'waited'), ('Manager', 'waited on me')], 1: [], 2: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part nine\n",
      "{0: [('manager', 'is nicest person')], 1: [], 2: []}\n",
      "Part three\n",
      "Part six\n",
      "PRON 2\n",
      "{0: [('manager', 'is nicest person'), ('manager', 'goes back'), ('manager', 'disassembles')], 1: [], 2: []}\n",
      "Part four\n",
      "Part six\n",
      "PRON 3\n",
      "Part three\n",
      "Part five\n",
      "PRON 2\n",
      "{0: [('manager', 'is nicest person'), ('manager', 'goes back'), ('manager', 'disassembles')], 1: [('manager', 'works'), ('manager', 'works with respect'), ('manager', 'works with her')], 2: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('manager', 'is nicest person'), ('manager', 'goes back'), ('manager', 'disassembles')], 1: [('manager', 'works'), ('manager', 'works with respect'), ('manager', 'works with her'), ('manager', 'might get')], 2: []}\n",
      "Part three\n",
      "{0: [('manager', 'is nicest person'), ('manager', 'goes back'), ('manager', 'disassembles')], 1: [('manager', 'works'), ('manager', 'works with respect'), ('manager', 'works with her'), ('manager', 'might get'), ('things', 'confused occasionally')], 2: []}\n",
      "Part seven\n",
      "{0: [('staff', 'is kind')], 1: []}\n",
      "{0: [('staff', 'is kind'), ('food', 'is prepared in timely manner'), ('food', 'is prepared for reason')], 1: []}\n",
      "Part nine\n",
      "{0: [('staff', 'is kind'), ('food', 'is prepared in timely manner'), ('food', 'is prepared for reason')], 1: [('line', 'be inconvenience')]}\n",
      "Part seven\n",
      "Part eight\n",
      "{0: [], 1: [('attendants', 'are rude'), ('attendants', 'are untrained'), ('attendants', 'are unprofessional')], 2: [], 3: []}\n",
      "Part three\n",
      "Part six\n",
      "{0: [], 1: [('attendants', 'are rude'), ('attendants', 'are untrained'), ('attendants', 'are unprofessional')], 2: [('attendants', 'lie'), ('attendants', 'say')], 3: []}\n",
      "Part three\n",
      "Part six\n",
      "{0: [], 1: [('attendants', 'are rude'), ('attendants', 'are untrained'), ('attendants', 'are unprofessional')], 2: [('attendants', 'lie'), ('attendants', 'say')], 3: [('attendant', 'screamed'), ('attendant', 'cursed')]}\n",
      "Part seven\n",
      "{0: [], 1: [('team', 'been friendly')], 2: [], 3: [], 4: []}\n",
      "Part three\n",
      "{0: [], 1: [('team', 'been friendly')], 2: [('Drive', 'can get')], 3: [], 4: []}\n",
      "Part three\n",
      "Part five\n",
      "PRON 3\n",
      "Part seven\n",
      "{0: [], 1: [('team', 'been friendly')], 2: [('Drive', 'can get')], 3: [], 4: [('Store', 'is clean')]}\n",
      "Part nine\n",
      "{0: [('Hashbrowns', 'were fire')], 1: []}\n",
      "Part three\n",
      "{0: [('Hashbrowns', 'were fire'), ('machine', 'is always working')], 1: []}\n",
      "PRON 3\n",
      "Part three\n",
      "Part five\n",
      "PRON 3\n",
      "Part four\n",
      "Part five\n",
      "Part six\n",
      "PRON 3\n",
      "Part four\n",
      "Part five\n",
      "PRON 3\n",
      "Part four\n",
      "Part five\n",
      "PRON 3\n",
      "Part three\n",
      "{0: [], 1: [], 2: [], 3: [], 4: [('place', 'has always given')]}\n",
      "Part seven\n",
      "{0: [], 1: [('burgers', 'were wrong')], 2: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part one\n",
      "{0: [], 1: [('burgers', 'were wrong')], 2: [('like', 'have grandkids')]}\n",
      "Part nine\n",
      "{0: [('McDonalds', 'is not company')], 1: [], 2: [], 3: []}\n",
      "Part nine\n",
      "PRON 2\n",
      "{0: [('McDonalds', 'is not company')], 1: [('McDonalds', \"'re company\")], 2: [], 3: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('McDonalds', 'is not company')], 1: [('McDonalds', \"'re company\")], 2: [('McDonalds', 'not never moves')], 3: []}\n",
      "Part three\n",
      "{0: [('McDonalds', 'is not company')], 1: [('McDonalds', \"'re company\")], 2: [('McDonalds', 'not never moves')], 3: [('robots', 'are going')]}\n",
      "Part four\n",
      "PRON 2\n",
      "{0: [('McDonalds', 'is not company')], 1: [('McDonalds', \"'re company\")], 2: [('McDonalds', 'not never moves')], 3: [('robots', 'are going'), ('robots', 'hate jobs')]}\n",
      "Part three\n",
      "{0: [('CONSTANTLY', 'gets')], 1: [], 2: [], 3: [], 4: []}\n",
      "Part three\n",
      "{0: [('CONSTANTLY', 'gets')], 1: [], 2: [('sandwiches', 'not complicated all'), ('fry', 'not complicated all')], 3: [], 4: []}\n",
      "Part four\n",
      "Part six\n",
      "{0: [('CONSTANTLY', 'gets')], 1: [], 2: [('sandwiches', 'not complicated all'), ('fry', 'not complicated all')], 3: [('manager', 'gave new one'), ('manager', 'shoved'), ('manager', 'ked')], 4: []}\n",
      "Part three\n",
      "Part five\n",
      "Part six\n",
      "{0: [('CONSTANTLY', 'gets')], 1: [], 2: [('sandwiches', 'not complicated all'), ('fry', 'not complicated all')], 3: [('manager', 'gave new one'), ('manager', 'shoved'), ('manager', 'ked'), ('girl', 'shoved'), ('girl', 'shoved in hands'), ('girl', 'ked')], 4: []}\n",
      "Part three\n",
      "Part six\n",
      "PRON 2\n",
      "{0: [('CONSTANTLY', 'gets')], 1: [], 2: [('sandwiches', 'not complicated all'), ('fry', 'not complicated all')], 3: [('manager', 'gave new one'), ('manager', 'shoved'), ('manager', 'ked'), ('girl', 'shoved'), ('girl', 'shoved in hands'), ('girl', 'ked')], 4: [('manager', 'made'), ('manager', 'turned')]}\n",
      "Part five\n",
      "{0: [('time', 'drive through team')], 1: [], 2: [], 3: [], 4: [], 5: [], 6: []}\n",
      "Part three\n",
      "{0: [('time', 'drive through team'), ('drive', 'seem')], 1: [], 2: [], 3: [], 4: [], 5: [], 6: []}\n",
      "Part nine\n",
      "PRON 2\n",
      "{0: [('time', 'drive through team'), ('drive', 'seem')], 1: [], 2: [], 3: [], 4: [], 5: [('time', \"'s response\")], 6: []}\n",
      "Part nine\n",
      "{0: [('This', 'is 3rd time')], 1: [], 2: [], 3: [], 4: [], 5: []}\n",
      "Part three\n",
      "Part six\n",
      "PRON 3\n",
      "Part seven\n",
      "Part eight\n",
      "{0: [('This', 'is 3rd time')], 1: [('this', 'be quick'), ('this', 'be easy')], 2: [], 3: [], 4: [], 5: []}\n",
      "Part three\n",
      "{0: [('This', 'is 3rd time')], 1: [('this', 'be quick'), ('this', 'be easy')], 2: [], 3: [], 4: [('one', 'answers')], 5: []}\n",
      "Part nine\n",
      "{0: [], 1: [('All', 'was coffee')], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: []}\n",
      "Part three\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part nine\n",
      "{0: [], 1: [('All', 'was coffee')], 2: [('one', 'was one')], 3: [], 4: [], 5: [], 6: [], 7: [], 8: []}\n",
      "Part three\n",
      "Part six\n",
      "PRON 3\n",
      "Part three\n",
      "PRON 3\n",
      "Part nine\n",
      "{0: [], 1: [('All', 'was coffee')], 2: [('one', 'was one')], 3: [('which', 'be medium')], 4: [], 5: [], 6: [], 7: [], 8: []}\n",
      "Part seven\n",
      "{0: [], 1: [('All', 'was coffee')], 2: [('one', 'was one')], 3: [('which', 'be medium')], 4: [], 5: [], 6: [('coffee', 'was full of grounds')], 7: [], 8: []}\n",
      "Part three\n",
      "{0: [], 1: [('one', 'however seemed')], 2: [], 3: [], 4: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [], 1: [('one', 'however seemed')], 2: [], 3: [], 4: [('one', 'indicates')]}\n",
      "Part seven\n",
      "PRON 2\n",
      "{0: [], 1: [('one', 'however seemed')], 2: [], 3: [], 4: [('one', 'indicates'), ('one', 'been nice')]}\n",
      "Part seven\n",
      "{0: [('place', 'is okay')], 1: [], 2: [], 3: [], 4: [], 5: [], 6: []}\n",
      "Part six\n",
      "PRON 3\n",
      "Part four\n",
      "Part five\n",
      "PRON 2\n",
      "{0: [('place', 'is okay')], 1: [('place', 'bring food'), ('place', 'bring to table')], 2: [], 3: [], 4: [], 5: [], 6: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('place', 'is okay')], 1: [('place', 'bring food'), ('place', 'bring to table')], 2: [], 3: [('place', 'came')], 4: [], 5: [], 6: []}\n",
      "Part seven\n",
      "{0: [('burger', 'was raw')], 1: [], 2: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [], 3: [('location', 'need')], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: []}\n",
      "Part seven\n",
      "{0: [], 1: [], 2: [], 3: [('location', 'need')], 4: [], 5: [('time', 'be different')], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: []}\n",
      "Part seven\n",
      "{0: [], 1: [], 2: [], 3: [('location', 'need')], 4: [], 5: [('time', 'be different')], 6: [], 7: [('staff', 'is horrible')], 8: [], 9: [], 10: [], 11: [], 12: []}\n",
      "Part one\n",
      "{0: [], 1: [], 2: [], 3: [('location', 'need')], 4: [], 5: [('time', 'be different')], 6: [], 7: [('staff', 'is horrible')], 8: [], 9: [('fil', 'have cars')], 10: [], 11: [], 12: []}\n",
      "Part four\n",
      "Part six\n",
      "{0: [], 1: [], 2: [], 3: [('location', 'need')], 4: [], 5: [('time', 'be different')], 6: [], 7: [('staff', 'is horrible')], 8: [], 9: [('fil', 'have cars')], 10: [], 11: [], 12: [('Moral', 'save time'), ('Moral', 'drive')]}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [], 1: [], 2: [], 3: [('location', 'need')], 4: [], 5: [('time', 'be different')], 6: [], 7: [('staff', 'is horrible')], 8: [], 9: [('fil', 'have cars')], 10: [], 11: [], 12: [('Moral', 'save time'), ('Moral', 'drive'), ('fil', 'got')]}\n",
      "Part three\n",
      "Part six\n",
      "{0: [('employees', 'conveniently forgot'), ('employees', 'talking')], 1: [], 2: []}\n",
      "Part seven\n",
      "Part eight\n",
      "PRON 2\n",
      "{0: [('employees', 'conveniently forgot'), ('employees', 'talking'), ('employees', 'is unprofessional'), ('employees', 'is uncalled')], 1: [], 2: []}\n",
      "Part three\n",
      "Part six\n",
      "PRON 2\n",
      "{0: [('employees', 'conveniently forgot'), ('employees', 'talking'), ('employees', 'is unprofessional'), ('employees', 'is uncalled')], 1: [('employees', 'said'), ('employees', 'was')], 2: []}\n",
      "Part one\n",
      "{0: [('location', 'have rude service')], 1: [], 2: []}\n",
      "Part one\n",
      "{0: [('Allways', 'have best cream')]}\n",
      "Part seven\n",
      "{0: [('Mcdonalds', 'is great')], 1: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('Mcdonalds', 'is great'), ('Mcdonalds', 'really need')], 1: []}\n",
      "Part three\n",
      "PRON 2\n",
      "{0: [('Mcdonalds', 'is great'), ('Mcdonalds', 'really need'), ('Mcdonalds', 'understand')], 1: []}\n",
      "Part three\n",
      "{0: [('Mcdonalds', 'is great'), ('Mcdonalds', 'really need'), ('Mcdonalds', 'understand'), ('speakers', 'trying')], 1: []}\n",
      "Part seven\n",
      "{0: [('Mcdonalds', 'is great'), ('Mcdonalds', 'really need'), ('Mcdonalds', 'understand'), ('speakers', 'trying')], 1: [('people', 'are nice')]}\n",
      "Part three\n",
      "Part six\n",
      "PRON 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>aspect</th>\n",
       "      <th>ability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3 months ago</td>\n",
       "      <td>Why does it look like someone spit on my food?...</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: [], 1: ['normal transaction'], 2: ['substa...</td>\n",
       "      <td>{0: [('someone', 'spit'), ('someone', 'spit on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>It'd McDonalds. It is what it is as far as the...</td>\n",
       "      <td>4</td>\n",
       "      <td>{0: [], 1: ['food', 'atmosphere'], 2: ['staff ...</td>\n",
       "      <td>{0: [], 1: [('food', 'go'), ('atmosphere', 'go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Made a mobile order got to the speaker and che...</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: ['mobile order'], 1: ['line'], 2: [], 3: [...</td>\n",
       "      <td>{0: [('order', 'got'), ('order', 'got to speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>a month ago</td>\n",
       "      <td>My mc. Crispy chicken sandwich was ÃÂ¯ÃÂ¿ÃÂ...</td>\n",
       "      <td>5</td>\n",
       "      <td>{0: [], 1: ['crispy chicken sandwich', 'custom...</td>\n",
       "      <td>{0: [], 1: [('service', 'was quick')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>I repeat my order 3 times in the drive thru, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>{0: ['order', 'large meal double filet', 'larg...</td>\n",
       "      <td>{0: [], 1: []}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviewer_id   review_time  \\\n",
       "0            1  3 months ago   \n",
       "1            2    5 days ago   \n",
       "2            3    5 days ago   \n",
       "3            4   a month ago   \n",
       "4            5  2 months ago   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  Why does it look like someone spit on my food?...       1   \n",
       "1  It'd McDonalds. It is what it is as far as the...       4   \n",
       "2  Made a mobile order got to the speaker and che...       1   \n",
       "3  My mc. Crispy chicken sandwich was ÃÂ¯ÃÂ¿ÃÂ...       5   \n",
       "4  I repeat my order 3 times in the drive thru, a...       1   \n",
       "\n",
       "                                              aspect  \\\n",
       "0  {0: [], 1: ['normal transaction'], 2: ['substa...   \n",
       "1  {0: [], 1: ['food', 'atmosphere'], 2: ['staff ...   \n",
       "2  {0: ['mobile order'], 1: ['line'], 2: [], 3: [...   \n",
       "3  {0: [], 1: ['crispy chicken sandwich', 'custom...   \n",
       "4  {0: ['order', 'large meal double filet', 'larg...   \n",
       "\n",
       "                                             ability  \n",
       "0  {0: [('someone', 'spit'), ('someone', 'spit on...  \n",
       "1  {0: [], 1: [('food', 'go'), ('atmosphere', 'go...  \n",
       "2  {0: [('order', 'got'), ('order', 'got to speak...  \n",
       "3             {0: [], 1: [('service', 'was quick')]}  \n",
       "4                                     {0: [], 1: []}  "
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ability'] = [process_ability(text, i, mapper_1, mapper_2) for i, text in enumerate(df['review'].values)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "14c4a8d2-807e-4057-b6be-613a3e98689d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>ability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why does it look like someone spit on my food?...</td>\n",
       "      <td>{0: [], 1: ['normal transaction'], 2: ['substa...</td>\n",
       "      <td>{0: [('someone', 'spit'), ('someone', 'spit on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It'd McDonalds. It is what it is as far as the...</td>\n",
       "      <td>{0: [], 1: ['food', 'atmosphere'], 2: ['staff ...</td>\n",
       "      <td>{0: [], 1: [('food', 'go'), ('atmosphere', 'go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Made a mobile order got to the speaker and che...</td>\n",
       "      <td>{0: ['mobile order'], 1: ['line'], 2: [], 3: [...</td>\n",
       "      <td>{0: [('order', 'got'), ('order', 'got to speak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My mc. Crispy chicken sandwich was ÃÂ¯ÃÂ¿ÃÂ...</td>\n",
       "      <td>{0: [], 1: ['crispy chicken sandwich', 'custom...</td>\n",
       "      <td>{0: [], 1: [('service', 'was quick')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I repeat my order 3 times in the drive thru, a...</td>\n",
       "      <td>{0: ['order', 'large meal double filet', 'larg...</td>\n",
       "      <td>{0: [], 1: []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>If I could give negative stars I would. This l...</td>\n",
       "      <td>{0: [], 1: ['location'], 2: ['single time'], 3...</td>\n",
       "      <td>{0: [], 1: [], 2: [], 3: [('location', 'need')...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>When ordering food the employees conveniently ...</td>\n",
       "      <td>{0: ['food'], 1: [], 2: []}</td>\n",
       "      <td>{0: [('employees', 'conveniently forgot'), ('e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>I donÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯Ã...</td>\n",
       "      <td>{0: ['location', 'service'], 1: [], 2: []}</td>\n",
       "      <td>{0: [('location', 'have rude service')], 1: []...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Allways has the best Fries and Ice cream in th...</td>\n",
       "      <td>{0: ['cream']}</td>\n",
       "      <td>{0: [('Allways', 'have best cream')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Mcdonalds is great but they really need to hir...</td>\n",
       "      <td>{0: ['people', 'spanish speakers', 'english or...</td>\n",
       "      <td>{0: [('Mcdonalds', 'is great'), ('Mcdonalds', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review  \\\n",
       "0   Why does it look like someone spit on my food?...   \n",
       "1   It'd McDonalds. It is what it is as far as the...   \n",
       "2   Made a mobile order got to the speaker and che...   \n",
       "3   My mc. Crispy chicken sandwich was ÃÂ¯ÃÂ¿ÃÂ...   \n",
       "4   I repeat my order 3 times in the drive thru, a...   \n",
       "..                                                ...   \n",
       "95  If I could give negative stars I would. This l...   \n",
       "96  When ordering food the employees conveniently ...   \n",
       "97  I donÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯Ã...   \n",
       "98  Allways has the best Fries and Ice cream in th...   \n",
       "99  Mcdonalds is great but they really need to hir...   \n",
       "\n",
       "                                               aspect  \\\n",
       "0   {0: [], 1: ['normal transaction'], 2: ['substa...   \n",
       "1   {0: [], 1: ['food', 'atmosphere'], 2: ['staff ...   \n",
       "2   {0: ['mobile order'], 1: ['line'], 2: [], 3: [...   \n",
       "3   {0: [], 1: ['crispy chicken sandwich', 'custom...   \n",
       "4   {0: ['order', 'large meal double filet', 'larg...   \n",
       "..                                                ...   \n",
       "95  {0: [], 1: ['location'], 2: ['single time'], 3...   \n",
       "96                        {0: ['food'], 1: [], 2: []}   \n",
       "97         {0: ['location', 'service'], 1: [], 2: []}   \n",
       "98                                     {0: ['cream']}   \n",
       "99  {0: ['people', 'spanish speakers', 'english or...   \n",
       "\n",
       "                                              ability  \n",
       "0   {0: [('someone', 'spit'), ('someone', 'spit on...  \n",
       "1   {0: [], 1: [('food', 'go'), ('atmosphere', 'go...  \n",
       "2   {0: [('order', 'got'), ('order', 'got to speak...  \n",
       "3              {0: [], 1: [('service', 'was quick')]}  \n",
       "4                                      {0: [], 1: []}  \n",
       "..                                                ...  \n",
       "95  {0: [], 1: [], 2: [], 3: [('location', 'need')...  \n",
       "96  {0: [('employees', 'conveniently forgot'), ('e...  \n",
       "97  {0: [('location', 'have rude service')], 1: []...  \n",
       "98              {0: [('Allways', 'have best cream')]}  \n",
       "99  {0: [('Mcdonalds', 'is great'), ('Mcdonalds', ...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['review', 'aspect', 'ability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "0755c1cf-0976-4f82-854d-e6f0ceee4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"delete.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b4a2d-0866-44b5-b8c2-c13279b92110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcbef22-f9bd-4445-9a43-631468924e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5ff7e-1227-4ffb-a571-9662e8538166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Just spent 10 minutes waiting at this McDonald's .\" +\\\n",
    "#         \"According to Google they're open 24/7. Finally we pull up to the window to see if anyone was there,\" +\\\n",
    "#         \" sure enough one employee seated in the lobby and another at the window. She told me they were closed.\" +\\\n",
    "#         \" Whoever these two are they need to be replaced. Get it together McDonald's. You're a corporate power house and you have a reputation to keep.\" +\\\n",
    "#         \"One of the most unprofessional experiences I've ever had with fast food. 0/10 would not recomend this location.\"\n",
    "text = \"I'm not really a huge fan of fast food, but I have 2 teenage daughter's who enjoy McDs. The staff here is always friendly and ALWAYS get their orders correct.\"+\\\n",
    "        \" By far one of the best McDs I have ever been to. I always enjoy and appreciate quality service from quality people... and you'll definitely find it here. \"+\\\n",
    "        \"At least from the evening crew. Haven't been here during the day. My girls joke that I go here so often to get them food, that the staff is going to end up knowing me\"+\\\n",
    "        \" and having their order ready to go. Lmao Great people and excellent service. ÃÂ¯ÃÂ¿ÃÂ½ÃÂ¯\"\n",
    "idx = 10\n",
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c65929-f505-4fc0-a6bb-18be58bb6dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, 'dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac40f9f-a2e1-480f-b6f5-886b49be31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ability(x, idx, mapper_1=None, mapper_2=None):\n",
    "    # Prepare sentence\n",
    "    texts = remove_extra_spaces(x)\n",
    "    texts = expand_contractions(x)\n",
    "    texts = remove_non_ascii(x)\n",
    "\n",
    "    # Get aspect\n",
    "    doc = nlp(texts)\n",
    "    result = get_raw_abilities(doc)\n",
    "    result = important_words_ability(result, idx_doc=idx, mapper_1=mapper_1, mapper_2=mapper_2)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "for idx, sent in enumerate(doc.sents):\n",
    "    print(idx, sent)\n",
    "\n",
    "process_ability(text, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e82c98-625d-4908-ab58-c0389f90cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============== SENTIMENT ANALYSIS ====================#\n",
    "from pattern.en import sentiment\n",
    "\n",
    "def pattern_lexicon_model(text, threshold=0.1):\n",
    "\n",
    "    # Preprocessing text\n",
    "    def preprocessing(text):\n",
    "        text = remove_extra_spaces(text)\n",
    "        text = expand_contractions(text)\n",
    "        text = remove_non_ascii(text)\n",
    "        return text\n",
    "\n",
    "    text = preprocessing(text)\n",
    "    analysis = sentiment(text)\n",
    "    sentiment_score = round(analysis[0], 2)\n",
    "    sentiment_subjectivity = round(analysis[0], 2)\n",
    "    \n",
    "    final_sentiment = 'positive' if sentiment_score >= threshold else 'negative'\n",
    "    \n",
    "    return final_sentiment\n",
    "\n",
    "\n",
    "def get_sentiment(text, threshold=0.1):\n",
    "    # Define local variable\n",
    "    storage = {}\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    for idx, sentence in enumerate(doc.sents):\n",
    "        print(idx, sentence.text)\n",
    "        storage[idx] = pattern_lexicon_model(sentence.text, threshold=threshold)\n",
    "\n",
    "    return storage\n",
    "\n",
    "# pprint(text)\n",
    "get_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f3fb5-7102-48a7-bad7-d1a2e118bd96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8161c2-e157-4a14-9de2-0e1a468a7de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8bbf2b-99f8-46ce-9965-463b4a69be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all aspect\n",
    "\n",
    "def get_all_aspect(list_):\n",
    "    temp = set()\n",
    "    for data in list_:\n",
    "        for values in data.values():\n",
    "            for value in values:\n",
    "                temp.add(value)\n",
    "    return temp\n",
    "\n",
    "all_aspects = get_all_aspect(df['aspect'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b593af-f3b6-4f66-8989-d4909cf0cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = \"\"\n",
    "for aspect in all_aspects:\n",
    "    text = text + \" \" + aspect\n",
    "    \n",
    "\n",
    "text = text.strip()\n",
    "\n",
    "wordcloud = WordCloud(background_color='white').generate(text)\n",
    "plt.style.use('classic')\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59bc426-afc7-4cd0-9a72-9b9e24143157",
   "metadata": {},
   "outputs": [],
   "source": [
    "asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5bd372-6cf9-4766-8922-9a4064937494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12b227-e67e-41d0-ab6f-8b2b6fbfd744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307461cc-b6f2-44a2-a66d-18cc5279e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing text\n",
    "def preprocessing(text):\n",
    "    text = remove_extra_spaces(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_non_ascii(text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f74f2-c950-48e8-abc2-328d8ead6309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_abilities(doc):\n",
    "    storage = {}\n",
    "    first_person_pronouns = [ 'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
    "\n",
    "    # Get sentence mapper and prepare storage\n",
    "    sentence_points = {}\n",
    "    for i, s in enumerate(doc.sents):\n",
    "        sentence_points[i] = (s.start, s.end)\n",
    "        storage[i] = []\n",
    "\n",
    "    # Get mapper pronoun and antecedents\n",
    "    mapper_pron_ant = get_mapper_pron_ant(doc)\n",
    "        \n",
    "    for idx, token in enumerate(doc):\n",
    "        subjects = []\n",
    "        abilities = []\n",
    "    \n",
    "        # If the token is verb\n",
    "        if token.pos_ == 'VERB':\n",
    "            \n",
    "            for t in token.children:\n",
    "                # Check if the token children contain subject.\n",
    "                if t.dep_ == 'nsubj':\n",
    "                    # Get current child index and text\n",
    "                    current_idx = t.i\n",
    "                    text = t.lemma_\n",
    "                    # If the current child is first person pronoun\n",
    "                    if t.pos_ == 'PRON' and t.text.lower() in first_person_pronouns:\n",
    "                        # The token is reference to \"the user\"\n",
    "                        text = 'the user'\n",
    "                    # If the current child is pronoun and its current_idx in mapper_pron_ant\n",
    "                    elif t.pos_ == 'PRON' and current_idx in mapper_pron_ant.keys():\n",
    "                        # Change current token\n",
    "                        idx_map = mapper_pron_ant[current_idx]\n",
    "                        t = doc[idx_map]\n",
    "                        text = t.lemma_\n",
    "                    # If the current child is pronoun (but not in mapper_pron_ant keys), or only contains special characters or numbers, or\n",
    "                    #   length text less than 3\n",
    "                    elif (t.pos_ == 'PRON') or (re.match(r'^[0-9\\W]+$', t.text)) or (len(t.text) < 3):\n",
    "                        continue\n",
    "                    subjects.append(text)\n",
    "                    # Looping through the children of subject.\n",
    "                    subjects += extract_conj(t, lemma=True)\n",
    "\n",
    "            if len(subjects) > 0:\n",
    "                # Make sure the subject is unique\n",
    "                subjects = list(set(subjects))\n",
    "                \n",
    "                # Store the result\n",
    "                result = cross_product_tuple(subjects, token.lemma_)\n",
    "                # Result should be lowercase\n",
    "                result = [(s.lower(), a.lower()) for s, a in result]\n",
    "                sentence_location = get_sentence_location(sentence_points, idx)\n",
    "                storage[sentence_location] += result\n",
    "    \n",
    "    \n",
    "        # If the token is aux\n",
    "        elif token.pos_ == 'AUX':\n",
    "            \n",
    "            # Looping through children\n",
    "            for t in token.children:\n",
    "                if t.dep_ == 'nsubj':\n",
    "                    # Get current child index and text\n",
    "                    current_idx = t.i\n",
    "                    text = t.lemma_\n",
    "                    # If the current child is pronoun I, me, my\n",
    "                    if t.pos_ == 'PRON' and t.text.lower() in first_person_pronouns:\n",
    "                        # The token is reference to \"the user\"\n",
    "                        text = 'the user'\n",
    "                    # If the current child is pronoun and its current_idx in mapper_pron_ant\n",
    "                    elif t.pos_ == 'PRON' and current_idx in mapper_pron_ant.keys():\n",
    "                        # Change current token\n",
    "                        idx_map = mapper_pron_ant[current_idx]\n",
    "                        t = doc[idx_map]\n",
    "                        text = t.lemma_\n",
    "                    # If the current child is pronoun (but not in mapper_pron_ant keys), or only contains special characters or numbers, or\n",
    "                    #   length text less than 3\n",
    "                    elif (t.pos_ == 'PRON') or (re.match(r'^[0-9\\W]+$', t.text)) or (len(t.text) < 3):\n",
    "                        continue\n",
    "                        \n",
    "                    subjects.append(text)\n",
    "                    # Looping through the children of subject.\n",
    "                    subjects += extract_conj(t, lemma=True)\n",
    "\n",
    "                # Check if the neglect exist and depend on token aux\n",
    "                neg = get_neglect(token)\n",
    "                if t.dep_ == 'acomp':\n",
    "                    # If neglection does not exist after aux, then check if it exist at first adj/verb\n",
    "                    if not neg:\n",
    "                        neg = get_neglect(t)\n",
    "                    abilities.append(t.lemma_)\n",
    "                    # Looping through the children of subject\n",
    "                    # If neglection does not appear in after aux or before first subject.\n",
    "                    #  Then check all neglection in first conjugation.\n",
    "                    if not neg:\n",
    "                        abilities += extract_conj(t, neglect=True, lemma=True)\n",
    "                    else:\n",
    "                        abilities += extract_conj(t, lemma=True)\n",
    "                        abilities = cross_product_str(neg, abilities)\n",
    "    \n",
    "            if len(subjects) > 0 and len(abilities) > 0 :\n",
    "                # Make sure the subject is unique\n",
    "                subjects = list(set(subjects))\n",
    "\n",
    "                # Store the result\n",
    "                result = cross_product_tuple(subjects, abilities)\n",
    "                # Result should be lowercase\n",
    "                result = [(s.lower(), a.lower()) for s, a in result]\n",
    "                sentence_location = get_sentence_location(sentence_points, idx)\n",
    "                storage[sentence_location] += result\n",
    "            \n",
    "        # If the token is noun\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            # If only contains special characters or numbers, or length text less than 3\n",
    "            if re.match(r'^[0-9\\W]+$', token.text) or len(token.text) < 3:\n",
    "                continue\n",
    "            for t in token.lefts:\n",
    "                if t.pos_ == 'ADJ':\n",
    "                    neg = get_neglect(t)\n",
    "                    if neg:\n",
    "                        abilities.append(neg + ' ' + t.lemma_)\n",
    "                    else:\n",
    "                        abilities.append(t.lemma_)\n",
    "    \n",
    "            # If the token contain abilities, then we check is there any conjugation\n",
    "            if len(abilities) > 0:\n",
    "                subjects.append(token.lemma_)\n",
    "                subjects += extract_conj(token, lemma=True)\n",
    "                # Make sure the subject is unique\n",
    "                subjects = list(set(subjects))\n",
    "\n",
    "\n",
    "                # Store the result\n",
    "                result = cross_product_tuple(subjects, abilities)\n",
    "                # Result should be lowercase\n",
    "                result = [(s.lower(), a.lower()) for s, a in result]\n",
    "                sentence_location = get_sentence_location(sentence_points, idx)\n",
    "                storage[sentence_location] += result\n",
    "\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7c923-92b0-40a7-8a13-b12c54d33c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# # Load the spaCy model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_ability_property(doc):\n",
    "    # doc = nlp(text)\n",
    "    results = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Check for noun/pronoun subjects\n",
    "        if token.dep_ in (\"nsubj\", \"nsubjpass\"):\n",
    "            subject = token.text\n",
    "            \n",
    "            # Check for verb indicating ability (auxiliary + ROOT)\n",
    "            if token.head.pos_ == \"VERB\" and any(child.dep_ == \"aux\" for child in token.head.children):\n",
    "                verb = token.head.text\n",
    "                results.append((subject, f\"can {verb}\"))\n",
    "            \n",
    "            # Check for adjectives or adverbs indicating property\n",
    "            for child in token.head.children:\n",
    "                if child.dep_ in (\"acomp\", \"advmod\"):\n",
    "                    property_ = child.text\n",
    "                    results.append((subject, property_))\n",
    "        \n",
    "        # Check for noun/pronoun objects\n",
    "        if token.dep_ == \"dobj\":\n",
    "            obj = token.text\n",
    "            \n",
    "            # Check if object has any descriptive adjectives\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"amod\":  # adjectival modifier\n",
    "                    property_ = child.text\n",
    "                    results.append((obj, property_))\n",
    "    \n",
    "        # Check for relative clauses, nominal modifiers, and prepositional phrases\n",
    "        for child in token.head.children:\n",
    "            if child.dep_ == \"relcl\" or child.dep_ == \"acl\":\n",
    "                property_ = child.text\n",
    "                results.append((token.text, property_))\n",
    "                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b95e9ec-ee6e-4640-b2bc-ad24ab5b1588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "sentence1 = \"The dog can run fast.\"\n",
    "sentence2 = \"The chef cooked a delicious meal.\"\n",
    "\n",
    "docex = nlp(sentence1)\n",
    "displacy.render(docex, 'dep')\n",
    "print(\"Algorithm 1: \", get_raw_abilities(docex))\n",
    "print(\"Algorithm 2: \", extract_ability_property(docex))  # Output: [('dog', 'can run'), ('dog', 'fast')]\n",
    "\n",
    "docex = nlp(sentence2)\n",
    "displacy.render(docex, 'dep')\n",
    "print(\"Algorithm 1: \", get_raw_abilities(docex))\n",
    "print(\"Algorithm 2: \", extract_ability_property(docex))  # Output: [('chef', 'cooked'), ('meal', 'delicious')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75721b7a-1c09-4f39-b35d-faa7ccfe3c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The cat can jump.\",\n",
    "    \"The phone has a camera.\",\n",
    "    \"Water can freeze.\",\n",
    "    \"This car is fast.\",\n",
    "    \"The sun gives light.\",\n",
    "    \"The dog has sharp teeth.\",\n",
    "    \"The computer can store data.\",\n",
    "    \"My bike is lightweight.\",\n",
    "    \"The wind can blow hard.\",\n",
    "    \"Glass can break easily.\",\n",
    "    \"The plant grows quickly.\",\n",
    "    \"This material is waterproof.\",\n",
    "    \"Birds can fly long distances.\",\n",
    "    \"The phone charges wirelessly.\",\n",
    "    \"This app automatically tracks your steps.\",\n",
    "    \"The engine runs efficiently in cold weather.\",\n",
    "    \"His voice carries across the room.\",\n",
    "    \"Some animals can camouflage themselves.\",\n",
    "    \"The robot is capable of learning new tasks.\",\n",
    "    \"The building withstands strong earthquakes.\",\n",
    "    \"This software adapts to user behavior over time.\",\n",
    "    \"The bridge supports heavy loads without bending.\",\n",
    "    \"The new processor processes data faster than previous models.\",\n",
    "    \"These shoes reduce the impact on joints while running.\",\n",
    "    \"The satellite transmits signals across continents.\",\n",
    "    \"The medicine has anti-inflammatory properties.\",\n",
    "    \"Solar panels convert sunlight into electricity efficiently.\",\n",
    "    \"The AI system can predict stock market trends accurately.\",\n",
    "    \"The genetic algorithm optimizes solutions for complex problems.\",\n",
    "    \"The spacecraft is designed to sustain life for extended missions.\"\n",
    "]\n",
    "\n",
    "\n",
    "for s in sentences:\n",
    "    print(\"Sentence: \", s)\n",
    "    sample = s\n",
    "    docex = nlp(sample)\n",
    "    displacy.render(docex, 'dep')\n",
    "    print(\"Algorithm 1: \", get_raw_abilities(docex))\n",
    "    print(\"Algorithm 2: \", extract_ability_property(docex))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193d92b-4abc-45dc-aa79-36311cebe56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neglected_sentences = [\n",
    "    \"The cat cannot jump.\",\n",
    "    \"The phone doesn't have a camera.\",\n",
    "    \"Water cannot freeze.\",\n",
    "    \"This car is not fast.\",\n",
    "    \"The sun doesn't give light.\",\n",
    "    \"The dog doesn't have sharp teeth.\",\n",
    "    \"The computer cannot store data.\",\n",
    "    \"My bike is not lightweight.\",\n",
    "    \"The wind cannot blow hard.\",\n",
    "    \"Glass cannot break easily.\",\n",
    "    \"The plant doesn't grow quickly.\",\n",
    "    \"This material is not waterproof.\",\n",
    "    \"Birds cannot fly long distances.\",\n",
    "    \"The phone doesn't charge wirelessly.\",\n",
    "    \"This app doesn't automatically track your steps.\",\n",
    "    \"The engine doesn't run efficiently in cold weather.\",\n",
    "    \"His voice doesn't carry across the room.\",\n",
    "    \"Some animals cannot camouflage themselves.\",\n",
    "    \"The robot is not capable of learning new tasks.\",\n",
    "    \"The building doesn't withstand strong earthquakes.\",\n",
    "    \"This software doesn't adapt to user behavior over time.\",\n",
    "    \"The bridge doesn't support heavy loads without bending.\",\n",
    "    \"The new processor doesn't process data faster than previous models.\",\n",
    "    \"These shoes don't reduce the impact on joints while running.\",\n",
    "    \"The satellite doesn't transmit signals across continents.\",\n",
    "    \"The medicine doesn't have anti-inflammatory properties.\",\n",
    "    \"Solar panels don't convert sunlight into electricity efficiently.\",\n",
    "    \"The AI system cannot predict stock market trends accurately.\",\n",
    "    \"The genetic algorithm doesn't optimize solutions for complex problems.\",\n",
    "    \"The spacecraft is not designed to sustain life for extended missions.\"\n",
    "]\n",
    "\n",
    "for s in neglected_sentences:\n",
    "    print(\"Sentence: \", s)\n",
    "    sample = s\n",
    "    docex = nlp(sample)\n",
    "    displacy.render(docex, 'dep')\n",
    "    print(\"Algorithm 1: \", get_raw_abilities(docex))\n",
    "    print(\"Algorithm 2: \", extract_ability_property(docex))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff644a-e71d-480b-b334-95f26327366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "passive_sentences = [\n",
    "    \"The door is opened easily.\",\n",
    "    \"The car is driven by many people.\",\n",
    "    \"The package was delivered yesterday.\",\n",
    "    \"The glass can be broken with a hammer.\",\n",
    "    \"The lights are turned on automatically.\",\n",
    "    \"The cake is baked in the oven.\",\n",
    "    \"The window was cleaned this morning.\",\n",
    "    \"The table is made of wood.\",\n",
    "    \"The music is played softly.\",\n",
    "    \"The email was sent last night.\",\n",
    "    \"The car was repaired quickly.\",\n",
    "    \"The movie was watched by millions.\",\n",
    "    \"The test is graded electronically.\",\n",
    "    \"The flowers are grown in a greenhouse.\",\n",
    "    \"The room was decorated for the party.\",\n",
    "    \"The document is stored in the cloud.\",\n",
    "    \"The road was blocked by traffic.\",\n",
    "    \"The password is encrypted for security.\",\n",
    "    \"The project was completed ahead of schedule.\",\n",
    "    \"The bridge is constructed to withstand earthquakes.\",\n",
    "    \"The machine is programmed to operate autonomously.\",\n",
    "    \"The software was updated to include new features.\",\n",
    "    \"The building is powered by solar energy.\",\n",
    "    \"The message was translated into several languages.\",\n",
    "    \"The painting is admired by art enthusiasts.\",\n",
    "    \"The data is analyzed by artificial intelligence.\",\n",
    "    \"The system was designed to improve efficiency.\",\n",
    "    \"The book was inspired by historical events.\",\n",
    "    \"The device is charged wirelessly overnight.\",\n",
    "    \"The spacecraft is equipped to explore distant planets.\"\n",
    "]\n",
    "\n",
    "\n",
    "for s in passive_sentences:\n",
    "    print(\"Sentence: \", s)\n",
    "    sample = s\n",
    "    docex = nlp(sample)\n",
    "    displacy.render(docex, 'dep')\n",
    "    print(\"Algorithm 1: \", get_raw_abilities(docex))\n",
    "    print(\"Algorithm 2: \", extract_ability_property(docex))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d24227-a55e-4b81-9ad5-28a80280dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "negated_passive_sentences = [\n",
    "    \"The door is not opened easily.\",\n",
    "    \"The car is not driven by many people.\",\n",
    "    \"The package was not delivered yesterday.\",\n",
    "    \"The glass cannot be broken with a hammer.\",\n",
    "    \"The lights are not turned on automatically.\",\n",
    "    \"The cake is not baked in the oven.\",\n",
    "    \"The window was not cleaned this morning.\",\n",
    "    \"The table is not made of wood.\",\n",
    "    \"The music is not played softly.\",\n",
    "    \"The email was not sent last night.\",\n",
    "    \"The car was not repaired quickly.\",\n",
    "    \"The movie was not watched by millions.\",\n",
    "    \"The test is not graded electronically.\",\n",
    "    \"The flowers are not grown in a greenhouse.\",\n",
    "    \"The room was not decorated for the party.\",\n",
    "    \"The document is not stored in the cloud.\",\n",
    "    \"The road was not blocked by traffic.\",\n",
    "    \"The password is not encrypted for security.\",\n",
    "    \"The project was not completed ahead of schedule.\",\n",
    "    \"The bridge is not constructed to withstand earthquakes.\",\n",
    "    \"The machine is not programmed to operate autonomously.\",\n",
    "    \"The software was not updated to include new features.\",\n",
    "    \"The building is not powered by solar energy.\",\n",
    "    \"The message was not translated into several languages.\",\n",
    "    \"The painting is not admired by art enthusiasts.\",\n",
    "    \"The data is not analyzed by artificial intelligence.\",\n",
    "    \"The system was not designed to improve efficiency.\",\n",
    "    \"The book was not inspired by historical events.\",\n",
    "    \"The device is not charged wirelessly overnight.\",\n",
    "    \"The spacecraft is not equipped to explore distant planets.\"\n",
    "]\n",
    "\n",
    "for s in negated_passive_sentences:\n",
    "    print(\"Sentence: \", s)\n",
    "    sample = s\n",
    "    docex = nlp(sample)\n",
    "    displacy.render(docex, 'dep')\n",
    "    print(\"Algorithm 1: \", get_raw_abilities(docex))\n",
    "    print(\"Algorithm 2: \", extract_ability_property(docex))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80086b0d-c994-46f8-aca3-872f19f06d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = 'The door is opened easily and not quickly.'\n",
    "# sample = 'The glass cannot be broken with a hammer, knife, and sword.'\n",
    "# sample = 'The machine is programmed to operate autonomously and not fastly'\n",
    "sample = 'The bridge is constructed to withstand earthquakes and flames'\n",
    "doc = nlp(sample)\n",
    "\n",
    "def get_raw_abilities_passive(doc):\n",
    "    result = []\n",
    "c                \n",
    "            # If at least one ability is extracted\n",
    "            if len(abilities) > 0:\n",
    "                # Store to the result storage\n",
    "                result += cross_product_tuple(token.text, abilities)\n",
    "    return result\n",
    "    \n",
    "\n",
    "get_raw_abilities_passive(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d9ffe-6cb8-4e2c-bf92-b0b7123087f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp('The machine is programmed to operate autonomously and fastly')\n",
    "displacy.render(doc, 'dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ec609f-384e-463f-962f-fad2cf558cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in passive_sentences:\n",
    "    print(\"Sentence: \", s)\n",
    "    sample = s\n",
    "    docex = nlp(sample)\n",
    "    displacy.render(docex, 'dep')\n",
    "    print(\"Algorithm 1: \", get_raw_abilities(docex))\n",
    "    print(\"Algorithm 2: \", extract_ability_property(docex))\n",
    "    print(\"Algorithm 3: \", get_raw_abilities_passive(docex))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db698af-fb1b-4c9e-8fdf-8256aa055a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in negated_passive_sentences:\n",
    "    print(\"Sentence: \", s)\n",
    "    sample = s\n",
    "    docex = nlp(sample)\n",
    "    displacy.render(docex, 'dep')\n",
    "    print(\"Algorithm 1: \", get_raw_abilities(docex))\n",
    "    print(\"Algorithm 2: \", extract_ability_property(docex))\n",
    "    print(\"Algorithm 3: \", get_raw_abilities_passive(docex))\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600822db-7230-4b37-9daa-7d1b9b3b8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DONE) 1. If nsubjpass ==> Go to Head (verb) ==> In head children, get auxpass ==> Go to adverb after verb (advmod)\n",
    "#    ==> Subject passive + auxpass + Verb + advmod\n",
    "#    E.g: The door is opened easily ==> (Door, is opened easily)\n",
    "\n",
    "# (DONE) 2. If nsubjpass ==> Go to Head (verb) ==> In head children, get auxpass ==> Go to adposition (ADP tag or agent dependency) ==> Get pobj\n",
    "#    ==> Subject passive + auxpass + Verb + agent + Preposition object\n",
    "#    E.g: The car is driven by many people ==> (Car, is driven by people)\n",
    "\n",
    "# (DONE) 3. If nsubjpass ==> Go to Head (verb) ==> In head children, get auxpass ==> Subject passive + auxpass + Verb\n",
    "#    E.g: The package was delivered yesterday ==> (Package, was delivered)\n",
    "\n",
    "# (DONE) 4. If nsubjpass ==> Go to Head (verb) ==> In head children, get auxpass ==> If head children contain aux, append it ==> Go to preposition phrase\n",
    "#    (ADP pos tag or prep dependency) ==> Get pobj ==> Subject passive + aux + auxpass + Preposition phrase + Preposition object\n",
    "#    E.g: The glass can be broken with a hammer ==> (Glass, can be broken with hammer)\n",
    "\n",
    "# (DONE) 5. If nsubjpass ==> Go to Head (verb) ==> In head children, get auxpass ==> Go to adverb after verb (advmod) ==> If contain preposition phrase\n",
    "#    ==> Subject passive + auxpass + Verb + advmod + Preposition phrase + Preposition object\n",
    "#    E.g: The project was completed ahead of schedule ==> (Project, was completed ahead of schedule)\n",
    "\n",
    "# (DONE) 6. If nsubjpass ==> Go to Head (verb) ==> In head children, get auxpass ==> Go to xcomp, get the xcomp ==> Go to direct object.\n",
    "#    ==> Subject passive + auxpass + Verb + xcomp + Direct object\n",
    "#    E.g: The bridge is constructed to withstand earthquakes ==> (Bridge, is constructed withstand earthquakes)\n",
    "\n",
    "# (DONE) 7. If nsubjpass ==> Go to Head (verb) ==> In head children, get auxpass ==> Go to xcomp, get the xcomp ==> Go to advmod.\n",
    "#    ==> Subject passive + auxpass + Verb + xcomp + advmod\n",
    "#    E.g: The machine is programmed to operate autonomously. ==> (Machine, is programmed operate autonomously)\n",
    "\n",
    "# (DONE) 8. If nsubjpass ==> Go to Head (verb) ==> In head children, get auxpass ==> Go to adverb after verb (advmod) ==> Get all adverb (or advmod) before\n",
    "#    current advmod.\n",
    "#    ==> Subject passive + auxpass + Verb + advmod + advmod\n",
    "#    E.g: The device is charged wirelessly overnight ==> (Device, is charged wirelessly overnight)\n",
    "\n",
    "\n",
    "# NOTE:- For subject/object, try to: (1) multiple subject/object (DONE), (2) Neglection (DONE), (3) (DONE) Adjective pre-modifier object. \n",
    "#         (4) (DONE) Adverb pre-modifier another adverb. (5) (DONE) Get the aux from xcomp token.\n",
    "#      - In case xcomp exist in sentence, the property of subject is xcomp + advmod/obj."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be1ed1d-eea1-4370-bf63-52e48880ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling_after_token_prep_phrase(token, neglect=False):\n",
    "    result = []\n",
    "    basis_idx = token.i\n",
    "    prep = get_all_token_dep(token, dep='prep')\n",
    "    if prep:\n",
    "        # If contain children: dep pcomp dep VERB pos tag; Until reach dobj or pobj\n",
    "        for p in prep:\n",
    "            prep_idx = p.i\n",
    "            # If the preposition on the left basis token index, continue\n",
    "            if basis_idx > prep_idx:\n",
    "                continue\n",
    "                \n",
    "            current = get_token_dep(p, dep=['pcomp', 'dobj', 'pobj'])\n",
    "            # Store objects\n",
    "            obj = []\n",
    "            # Store complement\n",
    "            comp = [p.text]\n",
    "            while current:\n",
    "                text = current.text\n",
    "                # If current token is object, get the pre-modifier adjective\n",
    "                if current.dep_ in ['dobj', 'pobj']:\n",
    "                    pre_adj = ' '.join(extract_pre_adj(current))\n",
    "                    obj += cross_product_str(pre_adj, text)\n",
    "\n",
    "                    # Extract conjunct object\n",
    "                    obj_conj = extract_conj(current, neglect=neglect)\n",
    "                    if len(obj_conj) > 0:\n",
    "                        obj += obj_conj\n",
    "                else:\n",
    "                    comp = cross_product_str(comp, text)\n",
    "                    \n",
    "                current = get_token_dep(current, dep=['pcomp', 'dobj', 'pobj'])\n",
    "\n",
    "            # temp = sorted(temp, key=lambda x: x[1])\n",
    "            # temp = \" \".join([t[0] for t in temp])\n",
    "\n",
    "            # result.append(temp)\n",
    "            result += cross_product_str(comp, obj)\n",
    "            \n",
    "    return result\n",
    "\n",
    "# def extract_pre_adj(token, lemma=False):\n",
    "#     result = []\n",
    "#     current_idx = token.i\n",
    "#     for child in token.children:\n",
    "#         if child.pos_ == 'ADJ' and child.i < current_idx:\n",
    "#             if lemma:\n",
    "#                 result.append((child.lemma_, child.i))\n",
    "#             else:\n",
    "#                 result.append((child.text, child.i))\n",
    "\n",
    "#     # Sort by its index\n",
    "#     result = sorted(result, key=lambda x: x[1])\n",
    "\n",
    "#     # Return only list of string\n",
    "#     result = [item[0] for item in result]\n",
    "\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9049a11b-b348-4911-827b-5baeeee0b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage = {}\n",
    "# first_person_pronouns = [ 'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
    "\n",
    "# # Get sentence mapper and prepare storage\n",
    "# sentence_points = {}\n",
    "# for i, s in enumerate(doc.sents):\n",
    "#     sentence_points[i] = (s.start, s.end)\n",
    "#     storage[i] = []\n",
    "\n",
    "# # Get mapper pronoun and antecedents\n",
    "# mapper_pron_ant = get_mapper_pron_ant(doc)\n",
    "\n",
    "import pandas as pd\n",
    "data_verb = pd.read_csv('verb_transitivity.tsv', sep='\\t')\n",
    "\n",
    "map_verb_intrans = data_verb[['verb', 'percent_intrans']].set_index('verb').to_dict()['percent_intrans']\n",
    "\n",
    "\n",
    "# sample = \"This software adapts to user behavior over time.\"\n",
    "# sample = \"The engine runs efficiently in cold weather\"\n",
    "# sample = \"His voice carries across the room\"\n",
    "# sample = \"Glass can break easily\"\n",
    "# sample = \"The sun gives light\" \n",
    "# sample = \"The robot is capable of learning new tasks.\"\n",
    "# sample = \"She quickly ran gracefully through the forest.\"\n",
    "\n",
    "# Case root is aux\n",
    "# sample = \"They are all friendly, accommodating and always smiling\"\n",
    "# Case root is verb\n",
    "# sample = \"They are accommodating, friendly, and never smiling\"\n",
    "sample = \"The robot is capable of learning new tasks.\"\n",
    "\n",
    "doc = nlp(sample)\n",
    "\n",
    "def get_raw_abilities_new(doc):\n",
    "    # Define local variable.\n",
    "    storage = {}\n",
    "    first_person_pronouns = [ 'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
    "\n",
    "    # Get sentence mapper and prepare storage\n",
    "    sentence_points = {}\n",
    "    for i, s in enumerate(doc.sents):\n",
    "        sentence_points[i] = (s.start, s.end)\n",
    "        storage[i] = []\n",
    "\n",
    "    # Get mapper pronoun and antecedents\n",
    "    mapper_pron_ant = get_mapper_pron_ant(doc)\n",
    "\n",
    "    # Define local variable.\n",
    "    result = []\n",
    "    for idx, token in enumerate(doc):\n",
    "        abilities = []\n",
    "\n",
    "        # Initial filter: If token only contains special characters or numbers, or length text less than 3\n",
    "        if (re.match(r'^[0-9\\W]+$', token.text)) or (len(token.text) < 3):\n",
    "            continue\n",
    "\n",
    "        ## ==================== SUBJECT ACTIVE SENTENCE =========================== ##\n",
    "        # If token is subject (should be nsubj and nsubjpass). This time only nsubj\n",
    "        # In case active sentence form\n",
    "        if token.dep_ == 'nsubj':\n",
    "            # Go to its head\n",
    "            head = token.head\n",
    "\n",
    "            # Get neglect; If there is no neglect, return empty text.\n",
    "            neg = get_neglect(head)\n",
    "\n",
    "            # If head is Verb.\n",
    "            if head.pos_ == 'VERB':\n",
    "\n",
    "                # Get all conjunct (except adjective with dependency acomp)\n",
    "                if neg:\n",
    "                    verb_conjunct = extract_conj(head)\n",
    "                else:\n",
    "                    # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "                    verb_conjunct = extract_conj(head, neglect=True)\n",
    "                \n",
    "                # Rule 2 and 5: If the verb is posession\n",
    "                if head.lemma_ == 'have':\n",
    "                    \n",
    "                    # Get direct object\n",
    "                    obj = get_token_dep(head, dep='dobj')\n",
    "                    \n",
    "                    # If direct object exist\n",
    "                    if obj:\n",
    "                        # Extract all possible pre-modifier adjectives\n",
    "                        pre_adj_text = ' '.join(extract_pre_adj(obj))\n",
    "                        \n",
    "                        # Concatenate components into: Adj (optional) + Direct object\n",
    "                        ability = cross_product_str(pre_adj_text, obj.text)\n",
    "                        # Concatenate components into: Verb (Have) + Adj (optional) + Direct object\n",
    "                        ability = cross_product_str(head.lemma_, ability)\n",
    "                        # Concatenate components into: not (optional) + Verb (Have) + Adj (optional) + Direct object\n",
    "                        ability = cross_product_str(neg, ability)\n",
    "                        \n",
    "                        # Add the ability into abilities\n",
    "                        abilities = abilities + ability\n",
    "                        # EXPECTED PATTERN: Subject + not (optional) + Verb (Have) + Adj (optional) + Direct object\n",
    "\n",
    "                        # If the object has conjunct\n",
    "                        obj_conjunct = extract_conj(obj)\n",
    "                        if len(obj_conjunct) > 0:\n",
    "                            # Concatenate components into: Verb (Have) + Conjunct\n",
    "                            ability = cross_product_str(head.lemma_, obj_conjunct)\n",
    "                            # Concatenate components into: not (optional) + Verb (Have) + Conjunct\n",
    "                            ability = cross_product_str(neg, ability)\n",
    "                            \n",
    "                            # Add the ability into abilities\n",
    "                            abilities = abilities + ability\n",
    "                            # EXPECTED PATTERN: Subject + not (optional) + Verb (Have) + Direct object\n",
    "                            # Note: Since normaly, If direct object is noun/propn/pron the conjuncts are noun/propn/pron too.\n",
    "                            #        This rule follow this concept. In somehow, the conjunct could be adjective or another verb.\n",
    "\n",
    "                    # If the verb is posession but do not have direct object (object that the subject posessed) continue\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    # Get aux\n",
    "                    aux = get_token_dep(head, dep='aux')\n",
    "        \n",
    "                    # Get Pre and post adv\n",
    "                    pre_adv, post_adv = extract_adv(head)\n",
    "                    # Convert pre and post adverb into string\n",
    "                    pre_adv = (' '.join(pre_adv)).strip()\n",
    "                    post_adv = (' '.join(post_adv)).strip()\n",
    "        \n",
    "                    # Get Preposition after verb\n",
    "                    prep_after_verb = crawling_after_token_prep_phrase(head)\n",
    "                    \n",
    "                    # Get intransitive rate score\n",
    "                    int_rate = map_verb_intrans.get(head.text) or map_verb_intrans.get(head.lemma_)\n",
    "                    # If the verb is not in the mapper ( we assume it is transitive verb )\n",
    "                    if not int_rate:\n",
    "                        int_rate = 0\n",
    "\n",
    "                    # Concatenate components into: adv (optional) + verb\n",
    "                    ability = cross_product_str(pre_adv, head.text)\n",
    "                    # Concatenate components into: not (optional) + adv (optional) + verb\n",
    "                    ability = cross_product_str(neg, ability)\n",
    "                    # Concatenate components into: not (optional) + adv (optional) + verb + adv (optional)\n",
    "                    ability = cross_product_str(ability, post_adv)\n",
    "                        \n",
    "                    # If aux exist, add the text into ability_text\n",
    "                    if aux:\n",
    "                        # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + adv (optional)\n",
    "                        ability = cross_product_str(aux.text, ability)                    \n",
    "                    \n",
    "                    # If the verb is intransitive or Adverb after verb exist\n",
    "                    if (int_rate >= 0.5) or (post_adv):\n",
    "                        # EXPECTED PATTERN: Subject + aux (optional) + not (optional) + adv (optional) + verb + adv\n",
    "                        abilities = abilities + ability\n",
    "\n",
    "                    # If the verb is transitive\n",
    "                    else:                          \n",
    "                        # Get the direct object\n",
    "                        obj = get_token_dep(head, dep='dobj')\n",
    "                        # If the direct object exist\n",
    "                        if obj:                            \n",
    "                            # Get pre adjective modifier direct object and convert in into string\n",
    "                            pre_adj = ' '.join(extract_pre_adj(obj))\n",
    "                            # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + adj (optional)\n",
    "                            ability = cross_product_str(ability, pre_adj)\n",
    "                            # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + adj (optional) + Direct object\n",
    "                            ability = cross_product_str(ability, obj.text)\n",
    "\n",
    "                            # EXPECTED PATTERN: Subject + aux (optional) + not (optional) + adv (optional) + verb + adj (optional) + Direct object\n",
    "                            abilities = abilities + ability\n",
    "\n",
    "                    # If the preposition phrase exists after verb\n",
    "                    if prep_after_verb:\n",
    "                        # Concatenate components into: adv (optional) + verb\n",
    "                        ability = cross_product_str(pre_adv, head.text)\n",
    "                        # Concatenate components into: not (optional) + adv (optional) + verb\n",
    "                        ability = cross_product_str(neg, ability)\n",
    "                        # If auxiliary token exist\n",
    "                        if aux:\n",
    "                            # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb\n",
    "                            ability = cross_product_str(aux.text, ability)\n",
    "                            \n",
    "                        # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + preposition phrase\n",
    "                        ability = cross_product_str(ability, prep_after_verb)\n",
    "\n",
    "                        # EXPECTED PATTERN: Subject + aux (optional) + not (optional) + adv (optional) + verb + preposition phrase\n",
    "                        abilities = abilities + ability\n",
    "\n",
    "                    if len(verb_conjunct) > 0:\n",
    "                        # Extract ability\n",
    "                        ability = cross_product_str(neg, verb_conjunct)\n",
    "                        if aux:\n",
    "                            ability = cross_product_str(aux.text, ability)\n",
    "                        abilities = abilities + ability\n",
    "    \n",
    "            # Rule 2 and 10\n",
    "            elif head.pos_ == 'AUX':\n",
    "                # Get the token\n",
    "                # NOTE: if 'AUX' is root, only have one adjective with dependency acomp.\n",
    "                adj_token = get_token_dep(head, dep='acomp')\n",
    "                \n",
    "                # Get Preposition after adjective\n",
    "                prep_after_adj = ' '.join(crawling_after_token_prep_phrase(adj_token))\n",
    "\n",
    "                # Concatenate components into: aux + not (optional)\n",
    "                ability = cross_product_str(head.text, neg)\n",
    "                # Concatenate components into: aux + not (optional) + adj\n",
    "                ability = cross_product_str(ability, adj_token.text)\n",
    "                # Concatenate components into: aux + not (optional) + adj + preposition phrase (optional)\n",
    "                ability = cross_product_str(ability, prep_after_adj)\n",
    "\n",
    "                # EXPECTED PATTERN: Subject + aux + not (optional) + adj\n",
    "                abilities = abilities + ability\n",
    "\n",
    "                # Get all conjunct (except adjective with dependency acomp)\n",
    "                if neg:\n",
    "                    adj_conjunct = extract_conj(adj_token)\n",
    "                else:\n",
    "                    # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "                    adj_conjunct = extract_conj(adj_token, neglect=True)\n",
    "                    \n",
    "                if len(adj_conjunct) > 0:\n",
    "                    # Concatenate components into: aux + not (optional)\n",
    "                    ability = cross_product_str(head.text, neg)\n",
    "                    # Concatenate components into: aux + not (optional) + adj\n",
    "                    ability = cross_product_str(ability, adj_conjunct)\n",
    "                    # EXPECTED PATTERN: Subject + aux + not (optional) + adj\n",
    "                    abilities = abilities + ability\n",
    "\n",
    "        ## ==================== SUBJECT PASSIVE SENTENCE =========================== ##\n",
    "        # If sentence is passive form.\n",
    "        if token.dep_ == 'nsubjpass':\n",
    "            # Get the token head (verb). Since passive form at least form: Subject + auxpass + verb \n",
    "            head = token.head\n",
    "\n",
    "            ##================= GET ALL POSSIBLE COMPONENTS ===============##\n",
    "            # Get neglect; If there is no neglect, return empty text.\n",
    "            neg = get_neglect(head)\n",
    "    \n",
    "            # Get auxpass token\n",
    "            auxpass = get_token_dep(head, dep='auxpass')\n",
    "    \n",
    "            # Get aux token\n",
    "            aux = get_token_dep(head, dep='aux')\n",
    "            \n",
    "            # Get advmod after verb token\n",
    "            advmod_main = get_token_dep_right(head, dep=['advmod', 'npadvmod'])\n",
    "            if advmod_main:\n",
    "                pre_advmod_main, post_advmod_main = extract_adv(advmod_main)\n",
    "                pre_advmod_main = ' '.join(pre_advmod_main)\n",
    "                post_advmod_main = ' '.join(post_advmod_main)\n",
    "    \n",
    "            # Get the agent token\n",
    "            agent = get_token_dep_right(head, dep='agent')\n",
    "            obj_agent = None\n",
    "            # If the agent token exist\n",
    "            if agent:\n",
    "                # Get the object that refers to 'agent' token\n",
    "                obj_agent = get_token_dep(agent, dep=['pobj', 'dobj'])\n",
    "    \n",
    "            # Get prepositional phrase\n",
    "            if neg:\n",
    "                prep_after_verb = crawling_after_token_prep_phrase(head)\n",
    "            else:\n",
    "                prep_after_verb = crawling_after_token_prep_phrase(head, neglect=True)\n",
    "    \n",
    "            # Get xcomp token\n",
    "            xcomp = get_token_dep(head, dep='xcomp')\n",
    "            # Initalize object and advmod of xcomp.\n",
    "            obj_xcomp = None\n",
    "            advmod_xcomp = None\n",
    "            if xcomp:\n",
    "                # Get the aux, adv, and obj of xcomp tokens.\n",
    "                aux_xcomp = get_token_dep(xcomp, dep='aux')\n",
    "                obj_xcomp = get_token_dep(xcomp, dep=['pobj', 'dobj'])\n",
    "                advmod_xcomp = get_token_dep_right(xcomp, dep='advmod')\n",
    "                # If adv modifier of xcomp exist\n",
    "                if advmod_xcomp:\n",
    "                    # Get pre and post adverb of main adverb modifier xcomp.\n",
    "                    pre_advmod_xcomp, post_advmod_xcomp = extract_adv(advmod_xcomp)\n",
    "                    pre_advmod_xcomp = ' '.join(pre_advmod_xcomp)\n",
    "                    post_advmod_xcomp = ' '.join(post_advmod_xcomp)\n",
    "\n",
    "            ##================= STORING ABILITIES ===============##                \n",
    "            # Store ability: If adverb modifier exist\n",
    "            if advmod_main:\n",
    "                # If aux exist\n",
    "                if aux:\n",
    "                    # Concatenate components: aux (optional) + neg (optional)\n",
    "                    ability = cross_product_str(aux.text, neg)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                    ability = cross_product_str(ability, auxpass.text)\n",
    "                else:\n",
    "                    # Concatenate components: auxpass + neg (optional)\n",
    "                    ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "                ability = cross_product_str(ability, head.text)\n",
    "                \n",
    "                # If pre adverb exist\n",
    "                if pre_advmod_main:\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + pre-advmod (optional)\n",
    "                    ability = cross_product_str(ability, pre_advmod_main)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + pre-advmod (optional) + advmod\n",
    "                ability = cross_product_str(ability, advmod_main.text)\n",
    "    \n",
    "                # Get prepositional phrase after adverb\n",
    "                prep_after_adv = crawling_after_token_prep_phrase(advmod_main)\n",
    "                if prep_after_adv:\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + pre-advmod (optional)\n",
    "                    #                            + advmod + preposition phrase (optional)\n",
    "                    ability = cross_product_str(ability, prep_after_adv)\n",
    "\n",
    "                # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + pre-advmod (optional)\n",
    "                #                      + advmod + preposition phrase (optional)\n",
    "                abilities += ability\n",
    "\n",
    "                # Get all the adverb conjuncts\n",
    "                # If neglection exist\n",
    "                if neg:\n",
    "                    # It assume that all conjuncts are neglection \n",
    "                    advmod_main_conj = extract_conj(advmod_main)\n",
    "                else:\n",
    "                    # If neglect do not come at first, then check neglection in front each conjunct\n",
    "                    advmod_main_conj = extract_conj(advmod_main, neglect=True)\n",
    "\n",
    "                # If adverb has conjunct                \n",
    "                if len(advmod_main_conj) > 0:\n",
    "                    # If contain aux\n",
    "                    if aux:\n",
    "                        # Concatenate components: aux (optional) + neg (optional)\n",
    "                        ability = cross_product_str(aux.text, neg)\n",
    "                        # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                        ability = cross_product_str(ability, auxpass.text)\n",
    "                    else:\n",
    "                        # Concatenate components: auxpass + neg (optional)\n",
    "                        ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                    ability = cross_product_str(ability, head.text)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + advmod\n",
    "                    ability = cross_product_str(ability, advmod_main_conj)\n",
    "                    # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass\n",
    "                    abilities = abilities + ability\n",
    "\n",
    "            # Store ability: If agent and object agent token exist\n",
    "            if obj_agent and agent:\n",
    "                # If aux exist\n",
    "                if aux:\n",
    "                    # Concatenate components: aux (optional) + neg (optional)\n",
    "                    ability = cross_product_str(aux.text, neg)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                    ability = cross_product_str(ability, auxpass.text)\n",
    "                else:\n",
    "                    # Concatenate components: auxpass + neg (optional)\n",
    "                    ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb \n",
    "                ability = cross_product_str(ability, head.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + agent\n",
    "                ability = cross_product_str(ability, agent.text)\n",
    "                \n",
    "                # Get the pre adjective of object\n",
    "                pre_adj = ' '.join(extract_pre_adj(obj_agent))\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + agent + adj (optional)\n",
    "                ability = cross_product_str(ability, pre_adj)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + agent + adj (optional) + object\n",
    "                ability = cross_product_str(ability, obj_agent.text)\n",
    "                # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + agent + adj (optional) + object\n",
    "                abilities += ability\n",
    "\n",
    "                # Get all object conjuncts\n",
    "                if neg:\n",
    "                    obj_agent_conj = extract_conj(obj_agent)\n",
    "                else:\n",
    "                    # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "                    obj_agent_conj = extract_conj(obj_agent, neglection=True)\n",
    "                # If object conjuncts exist\n",
    "                if len(obj_agent_conj) > 0:\n",
    "                    # If aux exist\n",
    "                    if aux:\n",
    "                        # Concatenate components: aux (optional) + neg (optional)\n",
    "                        ability = cross_product_str(aux.text, neg)\n",
    "                        # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                        ability = cross_product_str(ability, auxpass.text)\n",
    "                    else:\n",
    "                        # Concatenate components: auxpass + neg (optional)\n",
    "                        ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "                    ability = cross_product_str(ability, head.text)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + object\n",
    "                    ability = cross_product_str(ability, obj_agent_conj)\n",
    "                    # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + object\n",
    "                    abilities = abilities + ability\n",
    "    \n",
    "            # Store ability: If preposition after verb exist\n",
    "            if prep_after_verb:\n",
    "                # If aux exist\n",
    "                if aux:\n",
    "                    # Concatenate components: aux (optional) + neg (optional)\n",
    "                    ability = cross_product_str(aux.text, neg)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                    ability = cross_product_str(ability, auxpass.text)\n",
    "                else:\n",
    "                    # Concatenate components: auxpass + neg (optional)\n",
    "                    ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "                ability = cross_product_str(ability, head.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + preposition phrase\n",
    "                ability = cross_product_str(ability, prep_after_verb)\n",
    "\n",
    "                # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + preposition phrase\n",
    "                abilities += ability\n",
    "    \n",
    "            # Store ability: If xcomp and object xcomp exist\n",
    "            if obj_xcomp and xcomp:\n",
    "                # If aux exist\n",
    "                if aux:\n",
    "                    # Concatenate components: aux (optional) + neg (optional)\n",
    "                    ability = cross_product_str(aux.text, neg)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                    ability = cross_product_str(ability, auxpass.text)\n",
    "                else:\n",
    "                    # Concatenate components: auxpass + neg (optional)\n",
    "                    ability = cross_product_str(auxpass.text, neg)\n",
    "                    \n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "                ability = cross_product_str(ability, head.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp\n",
    "                ability = cross_product_str(ability, aux_xcomp.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp\n",
    "                ability = cross_product_str(ability, xcomp.text)\n",
    "                \n",
    "                # Get the pre adjective of object\n",
    "                pre_adj = ' '.join(extract_pre_adj(obj_xcomp))\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + adj (optional)\n",
    "                ability = cross_product_str(ability, pre_adj)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + adj (optional) + object\n",
    "                ability = cross_product_str(ability, obj_xcomp.text)\n",
    "                # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + adj (optional) + object\n",
    "                abilities += ability\n",
    "                \n",
    "                # Get all object conjuncts\n",
    "                if neg:\n",
    "                    obj_xcomp_conj = extract_conj(obj_xcomp)\n",
    "                else:\n",
    "                    # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "                    obj_xcomp_conj = extract_conj(obj_xcomp, neglect=True)\n",
    "                # If object conjuncts exist\n",
    "                if len(obj_xcomp_conj) > 0:\n",
    "                    # If aux exist\n",
    "                    if aux:\n",
    "                        # Concatenate components: aux (optional) + neg (optional)\n",
    "                        ability = cross_product_str(aux.text, neg)\n",
    "                        # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                        ability = cross_product_str(ability, auxpass.text)\n",
    "                    else:\n",
    "                        # Concatenate components: auxpass + neg (optional)\n",
    "                        ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "                    ability = cross_product_str(ability, head.text)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp\n",
    "                    ability = cross_product_str(ability, aux_xcomp.text)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp\n",
    "                    ability = cross_product_str(ability, xcomp.text)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + object\n",
    "                    ability = cross_product_str(ability, obj_xcomp_conj)\n",
    "                    # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + object\n",
    "                    abilities = abilities + ability\n",
    "\n",
    "            # Store ability: If xcomp and object xcomp exist\n",
    "            if advmod_xcomp and xcomp:\n",
    "                # If aux exist\n",
    "                if aux:\n",
    "                    # Concatenate components: aux (optional) + neg (optional)\n",
    "                    ability = cross_product_str(aux.text, neg)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                    ability = cross_product_str(ability, auxpass.text)\n",
    "                else:\n",
    "                    # Concatenate components: auxpass + neg (optional)\n",
    "                    ability = cross_product_str(auxpass.text, neg)\n",
    "                    \n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "                ability = cross_product_str(ability, head.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xomp\n",
    "                ability = cross_product_str(ability, aux_xcomp.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xomp + xcomp\n",
    "                ability = cross_product_str(ability, xcomp.text)\n",
    "                # If pre advmod xcomp exist\n",
    "                if pre_advmod_xcomp:\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xomp + xcomp + pre-advmod (optional)\n",
    "                    ability = cross_product_str(ability, pre_advmod_xcomp)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xomp + xcomp + pre-advmod (optional) + advmod\n",
    "                ability = cross_product_str(ability, advmod_xcomp.text)\n",
    "                # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + aux-xomp + xcomp + pre-advmod (optional) + advmod\n",
    "                abilities += ability  \n",
    "\n",
    "                # Get all advmod conjuncts\n",
    "                if neg:\n",
    "                    advmod_xcomp_conj = extract_conj(advmod_xcomp)\n",
    "                else:\n",
    "                    # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "                    advmod_xcomp_conj = extract_conj(advmod_xcomp, neglect=True)\n",
    "                # If advmod conjuncts exist\n",
    "                if len(advmod_xcomp_conj) > 0:\n",
    "                    if aux:\n",
    "                        # Concatenate components: aux (optional) + neg (optional)\n",
    "                        ability = cross_product_str(aux.text, neg)\n",
    "                        # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                        ability = cross_product_str(ability, auxpass.text)\n",
    "                    else:\n",
    "                        # Concatenate components: auxpass + neg (optional)\n",
    "                        ability = cross_product_str(auxpass.text, neg)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "                    ability = cross_product_str(ability, head.text)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp\n",
    "                    ability = cross_product_str(ability, aux_xcomp.text)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp\n",
    "                    ability = cross_product_str(ability, xcomp.text)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + advmod\n",
    "                    ability = cross_product_str(ability, advmod_xcomp_conj)\n",
    "                    # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + advmod\n",
    "                    abilities += ability\n",
    "                \n",
    "        # Store final result\n",
    "        if len(abilities) > 0:\n",
    "            # Subject handling\n",
    "            text = token.lemma_\n",
    "            # current_idx = token.i\n",
    "            # If the subject is pronouns and first person pronouns\n",
    "            if token.pos_ == 'PRON' and token.text.lower() in first_person_pronouns:\n",
    "                subject = 'the user'\n",
    "            # If subject is pronouns and its token location in mapper_pron_ant\n",
    "            elif token.pos_ == 'PRON' and idx in mapper_pron_ant.keys():\n",
    "                # Get the antecedent index location\n",
    "                idx_map = mapper_pron_ant[idx]\n",
    "                # Change current token subject\n",
    "                token = doc[idx_map]\n",
    "                text = token.lemma_\n",
    "            # If the current child is pronoun (but not in mapper_pron_ant keys)\n",
    "            elif token.pos_ == 'PRON':\n",
    "                continue\n",
    "            \n",
    "            # Get all conj subject + current subject\n",
    "            subjects = [token.text] + extract_conj(token)\n",
    "            # Store result\n",
    "            result += cross_product_tuple(subjects, abilities)\n",
    "            # Storage final result\n",
    "            sentence_location = get_sentence_location(sentence_points, idx)\n",
    "            storage[sentence_location] += result\n",
    "\n",
    "    # Storing final result\n",
    "    return storage\n",
    "\n",
    "get_raw_abilities_new(doc)\n",
    "\n",
    "\n",
    "# NOTE: \n",
    "# - This rule is not good enough for subject passive. (should update it)\n",
    "# - This rule is not good enough for pronouns type subject. Fix it.\n",
    "# - Try case conjunct of verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6b3733-c2be-439e-a72a-7216ef7cf329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp('We have no information on whether users are at risk')\n",
    "# doc = nlp('I am excited about the trip and that we will meet again.')\n",
    "doc = nlp('This software adapts to user behavior over time.')\n",
    "displacy.render(doc, 'dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8d5f9-6363-4465-a5e9-c39586756525",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    print(\"Sentence: \", s)\n",
    "    sample = s\n",
    "    docex = nlp(sample)\n",
    "    displacy.render(docex, 'dep')\n",
    "    print(\"Algorithm 1: \", get_raw_abilities(docex))\n",
    "    print(\"Algorithm 2: \", extract_ability_property(docex))\n",
    "    print(\"Algorithm 3: \", get_raw_abilities_new(docex))\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "# FAIL:\n",
    "# His voice carries across the room. (SOLVED)\n",
    "# This software adapts to user behavior over time. (SOLVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb912b08-07d6-4a09-87db-bad1e0d5e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential more than one: (1) (DONE) Prepositional phrase (2) (DONE) Subject (3) Object (4) (DONE) Ability ==> Verb + adv or Verb + Prep + object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbab898-b865-4e99-a831-e69c2f211fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (DONE) 1. If token is NOUN (or subject) ==> Go to verb ==> If the verb contain child as dep aux ==> SUBJECT + aux + Verb.\n",
    "#      E.g: The cat can jump ==> (cat, can jump)\n",
    "\n",
    "# (DONE) 2. If token is NOUN (or subject) ==> Go to verb (if the verb is 'have' or 'has'; better using lemma for generalizer past form)\n",
    "#      ==> If the verb contain child as dep dobj ==> Subject + Has + dobj. \n",
    "#      E.g: The phone has a camera ==> (phone, has camera)\n",
    "\n",
    "# (DONE) 3. If token is NOUN (or subject) ==> Go to aux (and verb do not exist); aux is head of subject ==> If the aux contain adj (or acomp) \n",
    "#      ==> Subject + aux + ADJ.\n",
    "#      E.g: This car is fast ==> (car, is fast)\n",
    "\n",
    "# (DONE) 4. If the token is NOUN (or subject) ==> Go to verb, if the verb is \"transfer posession verb\" or \"keeping posession\" or \"verbs adapting behavior\"\n",
    "#      ==>  If the verb contain child as dep aux or dep dobj ==> Subject + aux (optional) + verb + dobj.\n",
    "#      E.g: The sun gives light ==> (sun, gives light)\n",
    "#           The computer can store data ==> (computer, can store data)\n",
    "\n",
    "# (DONE) 5. If token is NOUN (or subject) ==> Go to verb (if the varb lemma is 'have' or 'has') ==> If the verb contain child as dep dobj\n",
    "#     ==> If the dobj has pre-modifier (ADJ on the left; or dependency amod) ==> Subject + verb + pre-modifier + dobj\n",
    "#     E.g: The dog has sharp teeth ==> (Dog, has sharp teeth)\n",
    "\n",
    "# (DONE) 6. If token is NOUN (or subject) ==> Go to verb ==> If the verb contain child as dep aux ==> If the verb contain adverb or dep advmod on the right\n",
    "#    (do not contain dobj) ==> Subject + aux + verb + adv\n",
    "#    E.g: Glass can break easily ==> (Glass, can break easily) \n",
    "\n",
    "# (DONE) 7. If token is NOUN (or subject) ==> Go to verb ==> If the verb contain adverb or dep advmod on the left ==> If the verb contain obj\n",
    "#    ==> Subject + adv + verb + dobj\n",
    "#    E.g: This app automatically tracks your steps ==> (app, automatically tracks steps)\n",
    "\n",
    "# (DONE) 8. If the token is NOUN (or subject) ==> Go to verb ==> If the verb children contains adv (dep advmod), append it ==> If the verb children contains\n",
    "#    dep prep (or ADP pos tag) ==> If it contain preposition object ==> If contain adjective modifier. ==> Subject + Verb + Adv + Prep + Object\n",
    "#    E.g: The engine runs efficiently in cold weather ==> [(Engine, runs efficiently), (Engine, runs in cold weather)]\n",
    "\n",
    "# (DONE) 9. If the token is NOUN (or subject) ==> Go to verb ==> If the verb children contains dep prep (or ADP pos tag) ==> If it contain preposition object\n",
    "#    ==> Subject + Verb + prep + object\n",
    "#    E.g: His voice carries across the room ==> (voice, carries accross room)\n",
    "\n",
    "\n",
    "# (DONE) 10. If token is NOUN (or subject) ==> Go to aux (and verb do not exist); aux is head of subject ==> If the aux contain adj (or dep acomp)\n",
    "#      ==> If the adj contain prep ==> If the prep children contain verb (or dep pcomp) ==> If the pcomp children contain noun (or dobj)\n",
    "#      ==> Subject + aux + ADJ + prep + pcomp + dobj\n",
    "#      E.g: The robot is capable of learning new tasks. ==> (robot, is capable of learning tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7c64a-1bf4-42f2-9f34-e69f4bc2346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trf_posession_verb = [\n",
    "    \"give\",\n",
    "    \"donate\",\n",
    "    \"hand\",\n",
    "    \"deliver\",\n",
    "    \"pass\",\n",
    "    \"grant\",\n",
    "    \"transfer\",\n",
    "    \"transmit\",\n",
    "    \"convey\",\n",
    "    \"bestow\",\n",
    "    \"lend\",\n",
    "    \"offer\",\n",
    "    \"present\",\n",
    "    \"assign\",\n",
    "    \"bequeath\",\n",
    "    \"contribute\",\n",
    "    \"yield\",\n",
    "    \"endow\",\n",
    "    \"lease\",\n",
    "    \"sell\",\n",
    "    \"trade\",\n",
    "    \n",
    "]\n",
    "\n",
    "verbs_keeping_possession = [\n",
    "    \"keep\",\n",
    "    \"hold\",\n",
    "    \"retain\",\n",
    "    \"maintain\",\n",
    "    \"store\",\n",
    "    \"save\",\n",
    "    \"withhold\",\n",
    "    \"withstand\",\n",
    "    \"preserve\",\n",
    "    \"guard\",\n",
    "    \"hoard\",\n",
    "    \"reserve\",\n",
    "    \"secure\",\n",
    "    \"protect\",\n",
    "    \"cling\",\n",
    "    \"possess\",\n",
    "    \"own\",\n",
    "    \"control\",\n",
    "    \"occupy\",\n",
    "    \"safeguard\",\n",
    "    \"grasp\"\n",
    "]\n",
    "\n",
    "\n",
    "verbs_adapting_behavior = [\n",
    "    \"adapt\",\n",
    "    \"adjust\",\n",
    "    \"acclimate\",\n",
    "    \"accommodate\",\n",
    "    \"modify\",\n",
    "    \"alter\",\n",
    "    \"conform\",\n",
    "    \"respond\",\n",
    "    \"revise\",\n",
    "    \"recalibrate\",\n",
    "    \"reorient\",\n",
    "    \"shift\",\n",
    "    \"readjust\",\n",
    "    \"realign\",\n",
    "    \"reform\",\n",
    "    \"innovate\",\n",
    "    \"evolve\",\n",
    "    \"transition\",\n",
    "    \"improvise\",\n",
    "    \"customize\",\n",
    "    \"learning\",\n",
    "    \"reduce\",\n",
    "    \"support\",\n",
    "    \"convert\",\n",
    "    \"optimizie\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd58273-56b0-4734-9107-891d4fd7e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docex = nlp(\"They have a discussion about the project\")\n",
    "displacy.render(docex, 'dep')\n",
    "\n",
    "for token in docex:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ca849a-a961-431c-83a7-bf41553a6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_verb = pd.read_csv('verb_transitivity.tsv', sep='\\t')\n",
    "\n",
    "map_verb_intrans = data_verb[['verb', 'percent_intrans']].set_index('verb').to_dict()['percent_intrans']\n",
    "\n",
    "map_verb_intrans['optimize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cedd828-ae98-412b-9837-b3e42ceedf55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
