{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c615831-571b-45e7-ac47-8b869dc50e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eac21c66-2234-4386-87f0-6d9c63bdd187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Preparation text\n",
    "\n",
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "##========== PREPARATION TEXT ===========##\n",
    "\n",
    "# Contraction\n",
    "def expand_contractions(sentence, contraction_mapping=CONTRACTION_MAP):\n",
    "    \"\"\"\n",
    "    Expand the contractions in a sentence. For example don't => do not.\n",
    "    \n",
    "    Paramters:\n",
    "    sentence (str): The input sentence to clean.\n",
    "    contraction_mapping (dict): A dictionary for mapping contractions.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    str: The expanded contraction sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expanded_match(contraction):\n",
    "        \"\"\"\n",
    "        Filter for expanding the matched contraction.\n",
    "        \n",
    "        Parameters:\n",
    "        contraction (str): The input of contraction\n",
    "        \n",
    "        Returns:\n",
    "        str: The expanded contraction.\n",
    "        \"\"\"\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n",
    "        \n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_sentence = contractions_pattern.sub(expanded_match, sentence)\n",
    "    return expanded_sentence\n",
    "\n",
    "\n",
    "def remove_extra_spaces(sentence):\n",
    "    # Use regex to replace multiple spaces with a single space\n",
    "    return re.sub(r'\\s+', ' ', sentence).strip()\n",
    "\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"\n",
    "    Remove all non-ASCII characters from the text.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to clean.\n",
    "\n",
    "    Returns:\n",
    "    str: The cleaned text with only ASCII characters.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return ''.join([char for char in text if ord(char) < 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2314849b-0de2-4d27-ae9b-c48be7df3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper get specific token and handling token\n",
    "\n",
    "\n",
    "##=========== EXTRACT ASPECT ============##\n",
    "# Cross product two lists\n",
    "def cross_product_str(first, second):\n",
    "    \"\"\"\n",
    "    Do cross product\n",
    "\n",
    "    parameters\n",
    "    -----------\n",
    "    first: list/string\n",
    "    second: list/string\n",
    "\n",
    "    return: list of string\n",
    "    \"\"\"\n",
    "    temp = []\n",
    "    if type(first) == str:\n",
    "        first = [first]\n",
    "    if type(second) == str:\n",
    "        second = [second]\n",
    "    for i in first:\n",
    "        for j in second:\n",
    "            text = (i + ' ' + j).strip()\n",
    "            temp.append(text)\n",
    "    return temp\n",
    "\n",
    "def cross_product_tuple(first, second):\n",
    "    \"\"\"\n",
    "    Do cross product\n",
    "\n",
    "    parameters\n",
    "    -----------\n",
    "    first: list/string\n",
    "    second: list/string\n",
    "\n",
    "    return: list of tuple\n",
    "    \"\"\"\n",
    "    temp = []\n",
    "    if type(first) == str:\n",
    "        first = [first]\n",
    "    if type(second) == str:\n",
    "        second = [second]\n",
    "    for i in first:\n",
    "        for j in second:\n",
    "            temp.append((i, j))\n",
    "    return temp\n",
    "\n",
    "# Get neglection text\n",
    "def get_neglect(token):\n",
    "    if token:\n",
    "        for t in token.children:\n",
    "            if (t.dep_ == 'neg') or (t.dep_ == 'det' and t.text.lower() == 'no'):\n",
    "                return 'not'\n",
    "    return ''\n",
    "\n",
    "# Get token specific pos tag\n",
    "def get_token_pos(token, pos):\n",
    "    if type(pos) == str:\n",
    "        pos = [pos]\n",
    "    for t in token.children:\n",
    "        if t.pos_ in pos:\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_token_pos_left(token, pos):\n",
    "    if type(pos) == str:\n",
    "        pos = [pos]\n",
    "    for t in token.children:\n",
    "        if (t.pos_ in pos) and (t.i < token.i):\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_token_pos_right(token, pos):\n",
    "    if type(pos) == str:\n",
    "        pos = [pos]\n",
    "    for t in token.children:\n",
    "        if (t.pos_ in pos) and (t.i > token.i):\n",
    "            return t\n",
    "    return None\n",
    "    \n",
    "# Get token spcific dependency\n",
    "def get_token_dep(token, dep):\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if t.dep_ in dep:\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_token_dep_left(token, dep):\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if (t.dep_ in dep) and (t.i < token.i):\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_token_dep_right(token, dep):\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if (t.dep_ in dep) and (t.i > token.i):\n",
    "            return t\n",
    "    return None\n",
    "\n",
    "def get_all_token_dep(token, dep):\n",
    "    result = []\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if t.dep_ in dep:\n",
    "            result.append(t)\n",
    "    return result\n",
    "\n",
    "def get_all_token_dep_right(token, dep):\n",
    "    result = []\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if t.dep_ in dep and t.i > token.i:\n",
    "            result.append(t)\n",
    "    return result\n",
    "\n",
    "def get_all_token_dep_left(token, dep):\n",
    "    result = []\n",
    "    if type(dep) == str:\n",
    "        dep = [dep]\n",
    "    for t in token.children:\n",
    "        if t.dep_ in dep and t.i < token.i:\n",
    "            result.append(t)\n",
    "    return result\n",
    "\n",
    "# Get token coordinate conjugation\n",
    "# def get_token_cc(token):\n",
    "#     for t in token.children:\n",
    "#         if t.dep_ == 'cc':\n",
    "#             return t\n",
    "#     return None\n",
    "\n",
    "# Crawling all possibile conjunct\n",
    "def extract_conj(token, neglect=False, lemma=False):\n",
    "    result = []\n",
    "    current = get_token_dep(token, dep='conj')\n",
    "    while current:\n",
    "        if neglect:\n",
    "            neg = get_neglect(current)\n",
    "            # If lemma\n",
    "            if lemma:\n",
    "                text = (neg + ' ' + current.lemma_).strip()\n",
    "            else:\n",
    "                text = (neg + ' ' + current.text).strip()\n",
    "                    \n",
    "            result.append(text)\n",
    "        else:\n",
    "            result.append(current.text)\n",
    "        current = get_token_dep(current, dep='conj')\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_all_token_conj(token):\n",
    "    result = []\n",
    "    current = get_token_dep(token, dep='conj')\n",
    "    while current:\n",
    "        result.append(current)\n",
    "        current = get_token_dep(current, dep='conj')\n",
    "    return result\n",
    "\n",
    "# Get sentences that include coordinating conjunction and its conjunct\n",
    "def get_text_conj(token):\n",
    "    # Get all sentence of series include the conjugation\n",
    "    tokens = [token]\n",
    "    # Get all token\n",
    "    tokens += extract_conj(token, all_token=True)\n",
    "\n",
    "    text = ''\n",
    "    for i, t in enumerate(tokens):\n",
    "        text = text + t.text\n",
    "        if i < len(tokens) - 1:\n",
    "            if t.dep_ == 'cc':\n",
    "                text += ' '\n",
    "            else:\n",
    "                text += ', '\n",
    "\n",
    "    # text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Crawling all possibile pre modifier object\n",
    "def extract_pre_amod(token, lemma=False):\n",
    "    result = []\n",
    "    current_idx = token.i\n",
    "    for child in token.children:\n",
    "        if child.dep_ == 'amod' and child.i < current_idx:\n",
    "            if lemma:\n",
    "                result.append((child.lemma_, child.i))\n",
    "            else:\n",
    "                result.append((child.text, child.i))\n",
    "\n",
    "    # Sort by its index\n",
    "    result = sorted(result, key=lambda x: x[1])\n",
    "\n",
    "    # Return only list of string\n",
    "    result = [item[0] for item in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "# Crawling all possible post modifier object\n",
    "def extract_post_amod(token, lemma=False):\n",
    "    result = []\n",
    "    current_idx = token.i\n",
    "    for child in token.children:\n",
    "        if child.dep_ == 'amod' and child.i > current_idx:\n",
    "            if lemma:\n",
    "                result.append((child.lemma_, child.i))\n",
    "            else:\n",
    "                result.append((child.text, child.i))\n",
    "\n",
    "    # Sort by its index\n",
    "    result = sorted(result, key=lambda x: x[1])\n",
    "\n",
    "    # Return only list of string\n",
    "    result = [item[0] for item in result]\n",
    "\n",
    "    return result\n",
    "\n",
    "# Crawling all possible adverb\n",
    "def extract_adv(token, lemma=True):\n",
    "    conjunctions = [\n",
    "    # Coordinating conjunctions\n",
    "    \"for\", \"and\", \"nor\", \"but\", \"or\", \"yet\", \"so\",\n",
    "    \n",
    "    # Subordinating conjunctions\n",
    "    \"although\", \"because\", \"since\", \"if\", \"when\", \"while\", \"before\", \"after\", \"unless\", \"though\",\n",
    "    \n",
    "    # Correlative conjunctions (listed as single strings)\n",
    "    \"either\", \"neither\", \"both\", \"also\", \"whether\", \"as\",\n",
    "    \n",
    "    # Conjunctive adverbs\n",
    "    \"however\", \"therefore\", \"moreover\", \"consequently\", \"nevertheless\", \"thus\", \"furthermore\"\n",
    "    ]\n",
    "\n",
    "    result_pre = []\n",
    "    result_post = []\n",
    "    current_idx = token.i\n",
    "    for child in token.children:\n",
    "        # If pre-position adverb\n",
    "        if child.pos_ == 'ADV' and child.i < current_idx and child.lemma_.lower() not in conjunctions:\n",
    "            if lemma:\n",
    "                result_pre.append((child.lemma_, child.i))\n",
    "            else:\n",
    "                result_pre.append((child.text, child.i))\n",
    "\n",
    "        # If post-position adverb\n",
    "        if child.pos_ == 'ADV' and child.i > current_idx and child.lemma_.lower() not in conjunctions:\n",
    "            if lemma:\n",
    "                result_post.append((child.lemma_, child.i))\n",
    "            else:\n",
    "                result_post.append((child.text, child.i))\n",
    "\n",
    "    # Sort by its index\n",
    "    result_pre = sorted(result_pre, key=lambda x: x[1])\n",
    "    result_post = sorted(result_post, key=lambda x: x[1])\n",
    "\n",
    "    # Return only list of string\n",
    "    result_pre = [item[0] for item in result_pre]\n",
    "    result_post = [item[0] for item in result_post]\n",
    "\n",
    "    return result_pre, result_post\n",
    "\n",
    "# Crawling preposition phrase after particullar token\n",
    "def crawling_after_token_prep_phrase(token, neglect=False):\n",
    "    result = []\n",
    "    basis_idx = token.i\n",
    "    prep = get_all_token_dep(token, dep='prep')\n",
    "    if prep:\n",
    "        # If contain children: dep pcomp dep VERB pos tag; Until reach dobj or pobj\n",
    "        for p in prep:\n",
    "            prep_idx = p.i\n",
    "            # If the preposition on the left basis token index, continue\n",
    "            if basis_idx > prep_idx:\n",
    "                continue\n",
    "                \n",
    "            current = get_token_dep(p, dep=['pcomp', 'dobj', 'pobj'])\n",
    "            # Store objects\n",
    "            obj = []\n",
    "            # Store complement\n",
    "            comp = [p.text]\n",
    "            while current:\n",
    "                text = current.text\n",
    "                # If current token is object, get the pre-modifier adjective\n",
    "                if current.dep_ in ['dobj', 'pobj']:\n",
    "                    pre_adj = ' '.join(extract_pre_amod(current))\n",
    "                    obj += cross_product_str(pre_adj, text)\n",
    "\n",
    "                    # Extract conjunct object\n",
    "                    obj_conj = extract_conj(current, neglect=neglect)\n",
    "                    if len(obj_conj) > 0:\n",
    "                        obj += obj_conj\n",
    "                else:\n",
    "                    comp = cross_product_str(comp, text)\n",
    "                    \n",
    "                current = get_token_dep(current, dep=['pcomp', 'dobj', 'pobj'])\n",
    "\n",
    "            result += cross_product_str(comp, obj)\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "# # Get the sentence point mapper\n",
    "# def get_sentence_mapper():\n",
    "#     sentence_point = {}\n",
    "#     for i, s in enumerate(doc.sents):\n",
    "#         sentence_point[i] = (s.start, s.end)\n",
    "#     return sentence_point\n",
    "    \n",
    "# # Get location sentence\n",
    "# sentence_mapper = get_sentence_mapper(doc)\n",
    "\n",
    "def get_sentence_location(mapper, position):\n",
    "    for s in mapper.keys():\n",
    "        interval = mapper[s]\n",
    "        if position >= interval[0] and position < interval[1]:\n",
    "            return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a6b5412-19a8-4275-86fa-34f09ad41ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coreference Resolution\n",
    "\n",
    "# Generate mapper pronouns-antecedents (subject only)\n",
    "def get_mapper_pron_ant(doc):\n",
    "    \n",
    "    def locate_subject_ant_pron(_doc):\n",
    "        # Locate potential antecedents and pronouns (subject only)\n",
    "    \n",
    "        # Define local variables\n",
    "        antecedents = []\n",
    "        pron = []\n",
    "        prohibit_pronouns = [ 'i', 'you', 'me', 'my', 'mine']\n",
    "    \n",
    "        # Get sentence mapper\n",
    "        sentence_points = {}\n",
    "        for i, s in enumerate(_doc.sents):\n",
    "            sentence_points[i] = (s.start, s.end)\n",
    "        \n",
    "        for token in _doc:\n",
    "            # Condition potential antecedents\n",
    "            # If the token is not pronouns and it's a subject\n",
    "            if (token.pos_ in ['NOUN', 'PROPN']) and (token.dep_ == 'nsubj'):\n",
    "                start = token.i\n",
    "                end = start + 1\n",
    "                location_sentence = get_sentence_location(sentence_points, start)\n",
    "                antecedents.append((token, start, location_sentence))\n",
    "                # Check is there any conj\n",
    "                # antecedents += extract_conj(token, only_token=True)\n",
    "        \n",
    "            # if (token.pos_ != 'PRON') and (token.dep_ == 'dobj' or token.dep_ == 'pobj'):\n",
    "            #     start = token.i\n",
    "            #     end = start + 1\n",
    "            #     location_sentence = get_sentence_location(sentence_points, start)\n",
    "            #     antecedents.append((token, start, location_sentence))\n",
    "            #     # Check is there any conj\n",
    "            #     # antecedents += extract_conj(token, only_token=True)    \n",
    "        \n",
    "            # Condition potential pronouns\n",
    "            # Rule 1\n",
    "            # If pron is subject (it could be same sentence or previously)\n",
    "            if (token.pos_ == 'PRON' and token.text.lower() not in prohibit_pronouns) and (token.dep_ == 'nsubj'):\n",
    "                start = token.i\n",
    "                end = start + 1\n",
    "                location_sentence = get_sentence_location(sentence_points, start)\n",
    "                pron.append((token, start, location_sentence))\n",
    "                \n",
    "            # Rule 2\n",
    "            # If pron is possesion (ant is subject in the same sentence)\n",
    "            if (token.pos_ == 'PRON' and token.text.lower() not in prohibit_pronouns) and (token.dep_ == 'poss'):\n",
    "                start = token.i\n",
    "                end = start + 1\n",
    "                location_sentence = get_sentence_location(sentence_points, start)\n",
    "                pron.append((token, start, location_sentence))\n",
    "        \n",
    "            # Rule 3\n",
    "            # If pron is object\n",
    "            # if (token.pos_ == 'PRON') and (token.dep_ == 'dobj' or token.dep_ == 'pobj'):\n",
    "            #     start = token.i\n",
    "            #     end = start + 1\n",
    "            #     location_sentence = get_sentence_location(sentence_points, start)\n",
    "            #     pron.append((token, start, location_sentence))\n",
    "        \n",
    "        \n",
    "        return (antecedents, pron)\n",
    "\n",
    "    # Filter sentence\n",
    "    def filter_sentence(_list, location):\n",
    "        temp = []\n",
    "        for e in _list:\n",
    "            if e[-1] == location:\n",
    "                temp.append(e)\n",
    "        return temp\n",
    "\n",
    "    # Define local variable\n",
    "    mapper = {}\n",
    "    result = None\n",
    "\n",
    "    antecedents, pronouns = locate_subject_ant_pron(doc)\n",
    "    \n",
    "    if len(pronouns) > 0:\n",
    "        for p in pronouns:\n",
    "            # Current status\n",
    "            is_success = False\n",
    "\n",
    "            # Get current text, index token, and location sentence token\n",
    "            token_pron, index_pron, sent_pron = p\n",
    "            current_sentence = sent_pron\n",
    "            \n",
    "            while current_sentence > -1:\n",
    "                # Get the antecedents\n",
    "                filter_antecedents = filter_sentence(antecedents, current_sentence)\n",
    "\n",
    "                # If the filter antecedents exist\n",
    "                if len(filter_antecedents) > 0:\n",
    "                    for ant in filter_antecedents:\n",
    "                        token_ant, index_ant, sent_ant = ant\n",
    "                        # If antecedent is subject and pronouns is subject or possession and antecedent on the left of pronoun\n",
    "                        if ('subj' in token_ant.dep_) and ('subj' in token_pron.dep_ or 'poss' in token_pron.dep_) and (index_ant < index_pron):\n",
    "                            mapper[index_pron] = index_ant\n",
    "                            is_success = True\n",
    "                            break\n",
    "                        # if ('obj' in token_ant.dep_ and 'obj' in token_pron.dep_) and (index_ant < index_pron):\n",
    "                        #     mapper[index_pron] = index_ant\n",
    "                        #     is_success = True\n",
    "                        #     break\n",
    "                \n",
    "                # If already success, break it.\n",
    "                if is_success:\n",
    "                    break\n",
    "                    \n",
    "                current_sentence -= 1\n",
    "\n",
    "    return mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae6cb69b-3794-4586-94af-e7d17835e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main aspect extraction\n",
    "\n",
    "# Extract all raw aspects\n",
    "def get_raw_aspects(doc):\n",
    "    # Define global variables\n",
    "    global bing_liu_opinion_words\n",
    "    \n",
    "    # Define local variables\n",
    "    storage = []\n",
    "\n",
    "    # Define helper function\n",
    "    def is_abnormal_noun(text):\n",
    "        \"\"\"\n",
    "            If text only contains special character/number/both OR total length less than 3 it specified as abnormal.\n",
    "        \"\"\"\n",
    "        if re.match(r'^[0-9\\W]+$', token.text) or len(token.text) < 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Going through all token\n",
    "    for idx, token in enumerate(doc):\n",
    "        # Make sure the text is not abnormal\n",
    "        if is_abnormal_noun(token.text):\n",
    "            continue\n",
    "\n",
    "        # If the word is noun and preceded by an adjective\n",
    "        if idx != 0 and (token.pos_ == 'NOUN' and doc[idx - 1].pos_ == 'ADJ'):\n",
    "            # If the adjective is an opinion\n",
    "            if doc[idx - 1].text not in bing_liu_opinion_words:\n",
    "                # Concatenate adj + word then add to storage\n",
    "                text = doc[idx - 1].text + ' ' + token.text\n",
    "                storage.append((text, idx - 1, idx + 1))\n",
    "            else:\n",
    "                # Else, add noun only\n",
    "                text = token.text\n",
    "                storage.append((text, idx, idx + 1))\n",
    "            continue\n",
    "            \n",
    "        # If the word is noun and preceded by another noun\n",
    "        if idx != 0 and (token.pos_ == 'NOUN' and doc[idx - 1].pos_ == 'NOUN'):\n",
    "            text = doc[idx - 1].text + ' ' + token.text\n",
    "            storage.append((text, idx - 1, idx + 1))\n",
    "            continue\n",
    "\n",
    "        # If the word is noun and direct object\n",
    "        if token.pos_ == 'NOUN' and (token.dep_ == 'dobj'):\n",
    "            text = token.text\n",
    "            storage.append((text, idx, idx + 1))\n",
    "            continue\n",
    "    \n",
    "        # If the word is noun and a subject of sentence\n",
    "        if token.pos_ == 'NOUN' and token.dep_ == 'nsubj':\n",
    "            text = token.text\n",
    "            storage.append((text, idx, idx + 1))\n",
    "            continue\n",
    "    \n",
    "        # If the word is noun and a conj of another noun\n",
    "        if (token.pos_ == 'NOUN' and token.dep_ == 'conj') and (token.head.pos_ == 'NOUN'):\n",
    "            text = token.text\n",
    "            storage.append((text, idx, idx + 1))\n",
    "            continue\n",
    "    \n",
    "        # # If the sentence contains SUBJECT VERB, then makes it true\n",
    "        # if token.dep_ == 'nsubj' and token.head.pos_ == 'VERB':\n",
    "        #     is_contain_subject_verb = True\n",
    "    \n",
    "        # # If token is word that contain pre-modifier\n",
    "        # if (token.dep_ == 'amod' and token.head.pos_ == 'NOUN'):\n",
    "        #     if token.head.i != idx + 1:\n",
    "        #         continue\n",
    "        #     text = token.text + ' ' + token.head.text\n",
    "        #     storage.append((text, idx, token.head.i + 1))\n",
    "    \n",
    "        # # If token is word that contain post-modifier\n",
    "        # if (token.dep_ == 'pobj' and token.pos_ == 'NOUN'):\n",
    "        #     if token.head.dep_ == 'prep' and token.head.head.pos_ == 'NOUN':\n",
    "        #         text = token.head.head.text + ' ' + token.head.text + ' ' + token.text\n",
    "        #         start = token.head.head.i\n",
    "        #         storage.append((text, start, idx + 1))\n",
    "            \n",
    "        \n",
    "        # If token is adverb modifier and its head is NOUN then store it.\n",
    "        if (token.dep_ == 'advmod' and token.head.pos_ == 'NOUN'):\n",
    "            text = token.head.text + ' ' + token.text\n",
    "            storage.append((text, token.head.i, idx + 1))\n",
    "            # adv_adj_mod.append((text, idx, idx + 1))\n",
    "\n",
    "    # Sort storage\n",
    "    storage = list(set(storage))\n",
    "    storage = sorted(storage, key=lambda x: (x[1], x[0]))\n",
    "\n",
    "    return storage\n",
    "\n",
    "# Prunning raw aspect\n",
    "def prunning_aspect(list_, doc):\n",
    "    # Define local variables\n",
    "    drop_idx = []\n",
    "    storage = {}\n",
    "    \n",
    "    # Get sentence mapper and prepare storage\n",
    "    sentence_points = {}\n",
    "    for i, s in enumerate(doc.sents):\n",
    "        sentence_points[i] = (s.start, s.end)\n",
    "        storage[i] = []\n",
    "\n",
    "    for idx, item in enumerate(list_):\n",
    "        # As long as current idx does not more than maximum list_ index\n",
    "        if idx != len(list_) - 1:\n",
    "            # Get the next item\n",
    "            next_item = list_[idx + 1]\n",
    "            # If current item start position and next item end position are overlapping\n",
    "            if item[-1] - 1 == next_item[1]:\n",
    "                # We merge the text based on last text in current item and first text in next item\n",
    "                append_text = ' '.join(next_item[0].split()[1:])\n",
    "                # Update next item values\n",
    "                new_text = item[0] + ' ' + append_text\n",
    "                new_start = item[1]\n",
    "                new_end = next_item[-1]\n",
    "                list_[idx + 1] = (new_text, new_start, new_end)\n",
    "\n",
    "                # Add current index into dropped index list\n",
    "                drop_idx.append(idx)\n",
    "            \n",
    "            # If current item start position = next item end position (They are next to each other)\n",
    "            if item[-1] == next_item[1]:\n",
    "                # Update the next value (do not have to merge the text based on specific text).\n",
    "                new_text = item[0] + ' ' + next_item[0]\n",
    "                new_start = item[1]\n",
    "                new_end = next_item[-1]\n",
    "                list_[idx + 1] = (new_text, new_start, new_end)\n",
    "\n",
    "                # Add current index into dropped index list\n",
    "                drop_idx.append(idx)\n",
    "                \n",
    "    list_ = [list_[i] for i in range(len(list_)) if i not in drop_idx]\n",
    "\n",
    "    # Create return as mapper\n",
    "    for i, s in enumerate(list_):\n",
    "        text, start, end = s\n",
    "        sentence_location = get_sentence_location(sentence_points, start)\n",
    "        # Update value and store text as lowercase\n",
    "        storage[sentence_location].append(text.lower())\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f42562f-6132-4aaf-878f-42847795364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Subject active rules (Conjunct Handling)\n",
    "\n",
    "\n",
    "##==================== CONJUNCT HANDLING ==============================##\n",
    "\n",
    "def ability_obj_conjunct(obj, comp=None, base=None, **kwargs):\n",
    "    #====== Conjunct Object =======#\n",
    "    result = []\n",
    "    neg = ' '\n",
    "    conjuncts = get_all_token_conj(obj)\n",
    "    if len(conjuncts) > 0:\n",
    "        for conjunct in conjuncts:\n",
    "            # Get neglection object\n",
    "            pre_amod_token = get_token_dep_left(conjunct, dep='amod')\n",
    "            # If the neglection does not appear at front of object, it may refers to the most left pre modifier\n",
    "            neg = get_neglect(conjunct) or get_neglect(pre_amod_token)\n",
    "            # Get pre adjectvie modifier of conjunct\n",
    "            pre_adj = ' '.join(extract_pre_amod(conjunct))\n",
    "            # If custom base exist\n",
    "            if base:\n",
    "                ability = base\n",
    "            # If compliment as base\n",
    "            elif comp:\n",
    "                # Concatenate components: aux (optional) + not (optional) + adv (optional) \n",
    "                #                           + verb + aux-comp (optional) + compliment\n",
    "                ability = base_sentence_comp(comp, **kwargs)\n",
    "            # If main base as base\n",
    "            else:        \n",
    "                # Concatenate components: aux (optional) + not (optional) + adv (optional) + verb\n",
    "                ability = base_sentence(main_aux=kwargs.get('main_aux'),\n",
    "                                        main_verb=kwargs.get('main_verb'),\n",
    "                                        main_pre_adv=kwargs.get('main_pre_adv'), \n",
    "                                        neg=kwargs.get('neg'))\n",
    "                \n",
    "            # Concatenate components (compliment) into: aux (optional) + not (optional) + adv (optional) \n",
    "            #                                             + verb + aux-comp (optional) + compliment + not (optional)\n",
    "            # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + not (optional)           \n",
    "            ability = cross_product_str(ability, neg)\n",
    "            # Concatenate components (compliment) into: aux (optional) + not (optional) + adv (optional) \n",
    "            #                                             + verb + aux-comp (optional) + compliment + not (optional) + adj (optional)\n",
    "            # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + not (optional) + adj (optional)\n",
    "            ability = cross_product_str(ability, pre_adj)\n",
    "            # Concatenate components (compliment) into: aux (optional) + not (optional) + adv (optional) \n",
    "            #                                             + verb + aux-comp (optional) + compliment + not (optional) + adj (optional) + Conjunct object\n",
    "            # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + not (optional) + adj (optional) + Conjunct object\n",
    "            ability = cross_product_str(ability, conjunct.text)\n",
    "        \n",
    "            # Add the ability into abilities\n",
    "            result += ability\n",
    "            # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb + adj (optional) + Conjunct object\n",
    "            # EXPECTED PATTERN (compliment) : aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional) + compliment \n",
    "            #                                   + adj (optional) + Conjunct object\n",
    "            # Note: Since normaly, If direct object is noun/propn/pron the conjuncts are noun/propn/pron too.\n",
    "            #        This rule follow this concept. In somehow, the conjunct could be adjective or another verb.\n",
    "    return result\n",
    "\n",
    "def ability_adj_conjunct(adj, comp=None, **kwargs):\n",
    "    #====== Conjunct Adjective =======#\n",
    "    result = []\n",
    "    conjuncts = get_all_token_conj(adj)\n",
    "    if len(conjuncts) > 0:\n",
    "        for conjunct in conjuncts:\n",
    "            # Get neglection adjective\n",
    "            neg = get_neglect(conjunct)\n",
    "            # Concatenate components: aux + not (optional)\n",
    "            ability = base_sentence(main_aux=kwargs.get('main_aux'),\n",
    "                                    main_verb=kwargs.get('main_verb'),\n",
    "                                    main_pre_adv=kwargs.get('main_pre_adv'), \n",
    "                                    neg=neg)\n",
    "    \n",
    "            # Concatenate components into: aux (optional) + not (optional) + adj\n",
    "            ability = cross_product_str(ability, conjunct.text)\n",
    "    \n",
    "            # Add the ability into abilities\n",
    "            result += ability\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05e7c81f-9263-43f8-a3d9-7e97cd508914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: subject active rules (Head is verb) components\n",
    "\n",
    "def get_components_verb(verb):\n",
    "    # Get auxiliary verb token\n",
    "    aux = get_token_dep(verb, dep='aux')\n",
    "    # Get direct object verb token\n",
    "    obj = get_token_dep(verb, dep='dobj')\n",
    "    # Get pre-modifier adverb 'string'\n",
    "    advmod_left = get_token_dep_left(verb, dep=['advmod', 'npadvmod'])\n",
    "    pre_adv = []\n",
    "    if advmod_left:\n",
    "        # Get pre and post adverb after pre main verb\n",
    "        pre_advmod_left, post_advmod_left = extract_adv(advmod_left)\n",
    "        pre_advmod_left = ' '.join(pre_advmod_left)\n",
    "        post_advmod_left = ' '.join(post_advmod_left)\n",
    "                \n",
    "        pre_adv = [pre_advmod_left, advmod_left.text, post_advmod_left]\n",
    "    pre_adv = (' '.join(pre_adv)).strip()\n",
    "\n",
    "    # Get post-modifier adverb token\n",
    "    advmod_right = get_token_dep_right(verb, dep=['advmod', 'npadvmod'])\n",
    "    # Get preposition after verb token\n",
    "    prep = get_token_dep(verb, dep='prep')\n",
    "    # Get adjectival complement\n",
    "    acomp = get_token_dep(verb, dep='acomp')\n",
    "    \n",
    "    return {'aux': aux, 'obj': obj, 'pre_adv': pre_adv, 'advmod': advmod_right, 'prep': prep, 'acomp': acomp}\n",
    "\n",
    "def base_sentence(main_aux, main_verb, main_pre_adv, neg):\n",
    "    if not main_pre_adv:\n",
    "        main_pre_adv = ' '\n",
    "    if (main_verb) or (main_aux):\n",
    "        # Concatenate components into: not (optional)\n",
    "        ability = cross_product_str(neg, ' ')\n",
    "        if main_verb:\n",
    "            # Concatenate components into: not (optional) + adv (optional)\n",
    "            ability = cross_product_str(ability, main_pre_adv)\n",
    "            # Concatenate components into: not (optional) + adv (optional) + verb\n",
    "            ability = cross_product_str(ability, main_verb.text)\n",
    "        # If auxiliary token exist\n",
    "        if main_aux:\n",
    "            # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb\n",
    "            ability = cross_product_str(main_aux.text, ability)\n",
    "        return ability\n",
    "    return [' ']\n",
    "\n",
    "\n",
    "def ability_advmod(advmod, comp=None, base=None, **kwargs):\n",
    "    # Get pre and post adverb after verb\n",
    "    post_adv = []\n",
    "    if advmod:\n",
    "        # Get pre and post adverb after post main verb\n",
    "        pre_advmod_right_main, post_advmod_right_main = extract_adv(advmod)\n",
    "        pre_advmod_right_main = ' '.join(pre_advmod_right_main)\n",
    "        post_advmod_right_main = ' '.join(post_advmod_right_main)\n",
    "\n",
    "        post_adv = [pre_advmod_right_main, advmod.text, post_advmod_right_main]\n",
    "        \n",
    "    post_adv = (' '.join(post_adv)).strip()\n",
    "    # Concatenate components: adv\n",
    "    ability = cross_product_str(' ', post_adv)\n",
    "\n",
    "    if not base:\n",
    "        if comp:\n",
    "            # Get base sentence: aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional) + compliment\n",
    "            base = base_sentence_comp(comp, **kwargs)  \n",
    "        else:\n",
    "            # Get base sentence: aux (optional) + not (optional) + adv (optional) + verb\n",
    "            base = base_sentence(main_aux=kwargs.get('main_aux'),\n",
    "                                    main_verb=kwargs.get('main_verb'),\n",
    "                                    main_pre_adv=kwargs.get('main_pre_adv'), \n",
    "                                    neg=kwargs.get('neg'))\n",
    "        \n",
    "    # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + adv\n",
    "    # Concatenate components into (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional) + compliment + adv\n",
    "    # Concatenate components into (if custom base exist): base + adv\n",
    "    ability = cross_product_str(base, ability)\n",
    "    \n",
    "    # Get preposition after adverb\n",
    "    prep_after_advmod = crawling_after_token_prep_phrase(advmod)\n",
    "    # If preposition after adverb exist\n",
    "    if prep_after_advmod:\n",
    "        # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + adv + preposition phrase (optional)\n",
    "        # Concatenate components into (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional)\n",
    "        #                                             + compliment + adv + preposition phrase (optional)\n",
    "        # Concatenate components into (if custom base exist): base + adv + preposition phrase (optional)\n",
    "        ability = cross_product_str(ability, prep_after_advmod)\n",
    "                    \n",
    "    # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb + adv + preposition phrase (optional)\n",
    "    # EXPECTED PATTERN (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional)\n",
    "    #                                             + compliment + adv + preposition phrase (optional)\n",
    "    # EXPECTED PATTERN (if custom base exist): base + adv + preposition phrase (optional)\n",
    "    return ability\n",
    "\n",
    "def ability_dobj(obj, comp=None, base=None, **kwargs):\n",
    "    result = []\n",
    "\n",
    "    # Get neglection direct object\n",
    "    pre_amod_token = get_token_dep_left(obj, dep='amod')\n",
    "    # If the neglection does not appear at front of object, it may refers to the most left pre modifier\n",
    "    neg = get_neglect(obj) or get_neglect(pre_amod_token)\n",
    "    \n",
    "    # Get pre adjectvie modifier of object\n",
    "    pre_adj = ' '.join(extract_pre_amod(obj))\n",
    "    \n",
    "    # Concatenate components: adj (optional) + Direct object\n",
    "    ability = cross_product_str(pre_adj, obj.text)\n",
    "\n",
    "    # Concatenate components: not (optional) + adj (optional) + Direct object\n",
    "    ability = cross_product_str(neg, ability)\n",
    "\n",
    "    if not base:\n",
    "        if comp:\n",
    "            # Get base sentence: aux (optional) + not (optional) + adv (optional) \n",
    "            #                      + verb + aux-comp (optional) + compliment\n",
    "            base = base_sentence_comp(comp, **kwargs)  \n",
    "        else:\n",
    "            # Get base sentence: aux (optional) + not (optional) + adv (optional) + verb\n",
    "            base = base_sentence(main_aux=kwargs.get('main_aux'),\n",
    "                                    main_verb=kwargs.get('main_verb'),\n",
    "                                    main_pre_adv=kwargs.get('main_pre_adv'), \n",
    "                                    neg=kwargs.get('neg'))\n",
    "\n",
    "    # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + not (optional) + adj (optional) + Direct object\n",
    "    # Concatenate components into (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional)\n",
    "    #                                             + compliment + not (optional) + adj (optional) + Direct object\n",
    "    # Concatenate components into (if custom base exist): base + not (optional) + adj (optional) + Direct object\n",
    "    ability = cross_product_str(base, ability)\n",
    "\n",
    "    # GET PREPOSITION AFTER OBJ and ADNOMINAL CLAUSE\n",
    "    prep = get_token_dep(obj, dep='prep')\n",
    "    acl = get_token_dep(obj, dep='acl')\n",
    "    if (prep) or (acl):\n",
    "        if prep:\n",
    "            # Get phrase: preposition + preposition-compliment (optional) + pre-adj (optional) + object\n",
    "\n",
    "            phrase = ability_prep(prep, comp=None, phrase_only=True)        \n",
    "            # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb + not (optional) + adj (optional) + Direct object\n",
    "            #                     + preposition + preposition-compliment (optional) + pre-adj (optional) + not (optional) + object\n",
    "            # EXPECTED PATTERN (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional)\n",
    "            #                                  + compliment + not (optional) + adj (optional) + Direct object \n",
    "            #                                  + preposition + preposition-compliment (optional) \n",
    "            #                                  not (optional) + pre-adj (optional) + object\n",
    "            # EXPECTED PATTERN (compliment): \n",
    "            result += cross_product_str(ability, phrase)\n",
    "        if acl:\n",
    "            # Get aux acl\n",
    "            aux_acl = get_token_dep(acl, dep='aux')\n",
    "            if aux_acl:\n",
    "                # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + not (optional) + adj (optional) \n",
    "                #                                 + Direct object + aux-acl\n",
    "                # Concatenate components into (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional)\n",
    "                #                                             + compliment + not (optional) + adj (optional) + Direct object + aux-acl\n",
    "                # Concatenate components into (if custom base exist): base + not (optional) + adj (optional) + Direct object + aux-acl\n",
    "                temp = cross_product_str(ability, aux_acl.text)\n",
    "                # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + not (optional) + adj (optional) \n",
    "                #                                 + Direct object + aux-acl + acl\n",
    "                # Concatenate components into (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional)\n",
    "                #                                             + compliment + not (optional) + adj (optional) + Direct object + aux-acl + acl\n",
    "                # Concatenate components into (if custom base exist): base + not (optional) + adj (optional) + Direct object + aux-acl + acl\n",
    "                temp = cross_product_str(temp, acl.text)\n",
    "            else:\n",
    "                temp = cross_product_str(ability, acl.text)\n",
    "            # EXPECTED PATTERN: base + not (optional) + adj (optional) + Direct object + aux-acl + acl + all possible option\n",
    "            result += ability_adnominal_clause(acl=acl, base=ability, **kwargs)  \n",
    "    else:\n",
    "        # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb + adj (optional) + not (optional) + Direct object\n",
    "        # EXPECTED PATTERN (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional)\n",
    "        #                                  + compliment + adj (optional) + not (optional) + Direct object\n",
    "        result += ability\n",
    "    \n",
    "    # Conjunct object handling\n",
    "    result += ability_obj_conjunct(obj, **kwargs)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def ability_prep(prep, comp=None, phrase_only=False, base=None, **kwargs):\n",
    "    # Get object of preposition\n",
    "    obj = get_token_dep_right(prep, dep=['dobj', 'pobj'])\n",
    "    pcomp = None\n",
    "    # If object does not exist\n",
    "    if not obj:\n",
    "        # Get the preposition complement\n",
    "        pcomp = get_token_dep_right(prep, dep='pcomp')\n",
    "        if pcomp:\n",
    "            # Get the object that refers to preposition complement\n",
    "            obj = get_token_dep_right(pcomp, dep=['dobj', 'pobj'])\n",
    "    \n",
    "    # Concatenate components: preposition\n",
    "    ability = cross_product_str(prep.text, ' ')\n",
    "    if (pcomp) or (obj):\n",
    "        # If preposition compliment exist\n",
    "        if pcomp:\n",
    "            # Concatenate components: preposition + preposition-compliment (optional)\n",
    "            ability = cross_product_str(ability, pcomp.text)\n",
    "        # If object exist\n",
    "        if obj:\n",
    "            # Get neglection object\n",
    "            pre_amod_token = get_token_dep_left(obj, dep='amod')\n",
    "            # If the neglection does not appear at front of object, it may refers to the most left pre modifier\n",
    "            neg = get_neglect(obj) or get_neglect(pre_amod_token)\n",
    "            # Temporary storage\n",
    "            temp = []\n",
    "            # Get pre adjective modifier object\n",
    "            pre_adj = ' '.join(extract_pre_amod(obj))\n",
    "            # Concatenate components: preposition + preposition-compliment (optional) + not (optional)\n",
    "            temporary = cross_product_str(ability, neg)\n",
    "            # Concatenate components: preposition + preposition-compliment (optional) + not (optional) + pre-adj (optional)\n",
    "            temporary = cross_product_str(temporary, pre_adj)\n",
    "            # Concatenate components: preposition + preposition-compliment (optional) + not (optional) + pre-adj (optional) + object\n",
    "            temp += cross_product_str(temporary, obj.text)\n",
    "            \n",
    "            # Conjunct object handling\n",
    "            temp += ability_obj_conjunct(obj, base=ability)\n",
    "            \n",
    "            ability = temp\n",
    "\n",
    "        if phrase_only:\n",
    "            # EXPECTED PATTERN: preposition + preposition-compliment (optional) + pre-adj (optional) + object\n",
    "            return ability\n",
    "        else:\n",
    "            if not base:\n",
    "                if comp:\n",
    "                    # Get base sentence: aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional) + compliment\n",
    "                    base = base_sentence_comp(comp, **kwargs)   \n",
    "                else:\n",
    "                    # Get base sentence: aux (optional) + not (optional) + adv (optional) + verb\n",
    "                    base = base_sentence(main_aux=kwargs.get('main_aux'),\n",
    "                                        main_verb=kwargs.get('main_verb'),\n",
    "                                        main_pre_adv=kwargs.get('main_pre_adv'), \n",
    "                                        neg=kwargs.get('neg'))\n",
    "            # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb\n",
    "            #                     preposition + preposition-compliment (optional) + pre-adj (optional) + object\n",
    "            # EXPECTED PATTERN (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional) + compliment\n",
    "            #                                  + preposition + preposition-compliment (optional) + pre-adj (optional) + object\n",
    "            # EXPECTED PATTERN (if custom base exist): base + preposition + preposition-compliment (optional) + pre-adj (optional) + object\n",
    "            ability = cross_product_str(base, ability)\n",
    "            return ability\n",
    "            \n",
    "    return []\n",
    "\n",
    "\n",
    "def ability_acomp(acomp, comp=None, base=None, **kwargs):\n",
    "    # Get neglection \n",
    "    neg = get_neglect(acomp)\n",
    "    if not base:\n",
    "        if comp:\n",
    "            # Get base sentence: aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional) + compliment\n",
    "            base = base_sentence_comp(comp, **kwargs)  \n",
    "        else:\n",
    "            # Get base sentence: aux (optional) + not (optional) + adv (optional) + verb\n",
    "            base = base_sentence(main_aux=kwargs.get('main_aux'),\n",
    "                                    main_verb=kwargs.get('main_verb'),\n",
    "                                    main_pre_adv=kwargs.get('main_pre_adv'), \n",
    "                                    neg=kwargs.get('neg'))\n",
    "\n",
    "    # Concatenate components into: not (optional) + acomp\n",
    "    ability = cross_product_str(neg, acomp.text)\n",
    "    # Concatenate components into: aux (optional) + not (optional) + adv (optional) + verb + not (optional) + acomp\n",
    "    # Concatenate components into (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional) \n",
    "    #                                             + compliment + not (optional) + acomp\n",
    "    # Concatenate components into (if custom base exist): base + not (optional) + acomp\n",
    "    ability = cross_product_str(base, ability)                    \n",
    "    # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb + adv + preposition phrase (optional)\n",
    "    # EXPECTED PATTERN (compliment): aux (optional) + not (optional) + adv (optional) + verb + aux-comp (optional)\n",
    "    #                                             + compliment + adv + preposition phrase (optional)\n",
    "    # EXPECTED PATTERN (if custom base exist): base + adv + preposition phrase (optional)\n",
    "    return ability\n",
    "\n",
    "\n",
    "def ability_adnominal_clause(acl, base=None, **kwargs):\n",
    "    # Define local variable\n",
    "    result = []\n",
    "    aux_acl, obj_acl, advmod_acl, prep_acl, acomp_acl = get_components_comp(acl).values()\n",
    "    \n",
    "    # Get intransitive rate score\n",
    "    int_rate_acl = map_verb_intrans.get(acl.text) or map_verb_intrans.get(acl.lemma_)\n",
    "    # If the verb is not in the mapper ( we assume it is transitive verb )\n",
    "    if not int_rate_acl:\n",
    "        int_rate_acl = 0\n",
    "        \n",
    "    # Add auxiliary clause into kwargs\n",
    "    kwargs['aux_comp'] = aux_acl    \n",
    "    if (obj_acl) or (advmod_acl) or (prep_acl) or (acomp_acl):\n",
    "        # If direct object exist\n",
    "        if obj_acl:          \n",
    "            # ability = ability_comp_dobj(comp=acl, obj=obj_acl, neglect=False, **kwargs)\n",
    "            ability = ability_dobj(obj=obj_acl, comp=acl, base=base, **kwargs)\n",
    "            result += ability\n",
    "            \n",
    "        # If advmod exist\n",
    "        if advmod_acl:\n",
    "            # ability = ability_comp_advmod(comp=acl, advmod=advmod_acl, **kwargs)\n",
    "            ability = ability_advmod(advmod_acl, comp=acl, base=base, **kwargs)\n",
    "            result += ability\n",
    "    \n",
    "        # If preposition after acl appears\n",
    "        if prep_acl:\n",
    "            # ability = ability_comp_prep(comp=xcomp, prep=prep_xcomp, **components)\n",
    "            ability = ability_prep(prep_acl, comp=acl, base=base, **kwargs)\n",
    "            result += ability\n",
    "        \n",
    "        # If acomp after acl appears\n",
    "        if acomp_acl:\n",
    "            ability = ability_comp_acomp(comp=acl, acomp=acomp_acl, base=base, **kwargs)\n",
    "            result += ability\n",
    "\n",
    "    else:\n",
    "        # If do not contain any of that, but intransitive verb\n",
    "        if int_rate_acl > 0.5:\n",
    "            # ability = base_sentence(main_aux=kwargs.get('main_aux'),\n",
    "            #                         main_verb=kwargs.get('main_verb'),\n",
    "            #                         main_pre_adv=kwargs.get('main_pre_adv'), \n",
    "            #                         neg=kwargs.get('neg'))\n",
    "            # ability = base_sentence_comp(acl, **kwargs)\n",
    "            ability = cross_product_str(base, aux_acl.text)\n",
    "            ability = cross_product_str(ability, acl.text)\n",
    "            # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb\n",
    "            result += ability\n",
    "    return result\n",
    "\n",
    "\n",
    "###======== COMPONENTS COMPLIMENTS RULES ===============####\n",
    "\n",
    "def get_components_comp(comp):\n",
    "    if not comp:\n",
    "        return\n",
    "\n",
    "    # Get auxiliary verb of clausal complement token\n",
    "    aux = get_token_dep(comp, dep='aux')\n",
    "    # Get direct object of clausal complement token\n",
    "    obj = get_token_dep(comp, dep='dobj')\n",
    "    # Get adverb modifier of clausal complement token\n",
    "    advmod = get_token_dep(comp, dep='advmod')\n",
    "    # Get preposition after clausal complement token\n",
    "    prep = get_token_dep_right(comp, dep='prep')\n",
    "    # Get adjectival complement of clausal complement token\n",
    "    # acomp = get_token_dep_right(comp, dep='acomp') or get_token_pos_right(comp, pos='ADJ')\n",
    "    acomp = get_token_dep_right(comp, dep='acomp')\n",
    "    \n",
    "    return {'aux': aux, 'obj': obj, 'advmod': advmod, 'prep': prep, 'acomp': acomp}\n",
    "\n",
    "\n",
    "def base_sentence_comp(comp, **kwargs):\n",
    "    # Generate base sentence: aux (optional) + not (optional) + adv (optional) + verb\n",
    "    ability = base_sentence(main_aux=kwargs.get('main_aux'),\n",
    "                            main_verb=kwargs.get('main_verb'),\n",
    "                            main_pre_adv=kwargs.get('main_pre_adv'), \n",
    "                            neg=kwargs.get('neg'))\n",
    "    if kwargs.get('aux_comp'):\n",
    "        # Concatenate components: aux (optional) + not (optional) + adv (optional) \n",
    "        #                           + verb + aux-comp (optional)\n",
    "        ability = cross_product_str(ability, kwargs['aux_comp'].text)\n",
    "    # Concatenate components: aux (optional) + not (optional) + adv (optional) \n",
    "    #                           + verb + aux-comp (optional) + compliment\n",
    "    ability = cross_product_str(ability, comp.text)\n",
    "    \n",
    "    return ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9be3ac54-76a5-43b9-8c73-bea173f42a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Case Head is verb\n",
    "\n",
    "def ability_relative_verb(verb, is_comp=False, **kwargs):\n",
    "    # Get ability that relative to particlar verb\n",
    "    result = []\n",
    "    comp = None\n",
    "\n",
    "    # If the verb is compliment\n",
    "    if is_comp:\n",
    "        comp = verb\n",
    "        \n",
    "    # Extract all token\n",
    "    aux, obj, pre_adv, advmod_right, prep, acomp = get_components_verb(verb).values()\n",
    "    \n",
    "    if (advmod_right) or (prep) or (obj) or (acomp):\n",
    "        # If adverb after verb exist\n",
    "        if advmod_right:\n",
    "            ability = ability_advmod(advmod_right, comp=comp, **kwargs)\n",
    "            # EXPECTED PATTERN: Subject + aux (optional) + not (optional) + adv (optional) + verb + adv + prepositional phrase (optional)\n",
    "            result += ability\n",
    "    \n",
    "        # If prep after verb\n",
    "        if prep:\n",
    "            ability = ability_prep(prep, comp=comp, **kwargs)\n",
    "            # EXPECTED PATTERN: Subject + aux (optional) + not (optional) + adv (optional) + verb + preposition phrase\n",
    "            result += ability\n",
    "    \n",
    "        # If direct object exist\n",
    "        if obj:\n",
    "            ability = ability_dobj(obj, comp=comp, **kwargs)\n",
    "            result += ability\n",
    "    \n",
    "        # If adjective compliment exist\n",
    "        if acomp:\n",
    "            ability = ability_acomp(acomp, comp=comp, **kwargs)\n",
    "            result += ability\n",
    "    \n",
    "    return result\n",
    "\n",
    "def ability_verb(main_verb):\n",
    "    result = []\n",
    "    ###============ DEFINE VARIABLES ================###\n",
    "    aux, obj, pre_adv, advmod_right, prep, acomp = get_components_verb(main_verb).values()\n",
    "    # Get neglect; If there is no neglect, return empty text.\n",
    "    neg = get_neglect(main_verb)\n",
    "    # Get intransitive rate score\n",
    "    int_rate = map_verb_intrans.get(main_verb.text) or map_verb_intrans.get(main_verb.lemma_)\n",
    "    # If the verb is not in the mapper ( we assume it is transitive verb )\n",
    "    if not int_rate:\n",
    "        int_rate = 0\n",
    "\n",
    "    #==================== COMPLIMENT ========================#\n",
    "    # Get compliment verb\n",
    "    comp = get_token_dep(main_verb, dep=['xcomp', 'ccomp'])\n",
    "    if comp and (comp.pos_ != 'VERB' or get_token_dep(comp, dep='auxpass')):\n",
    "        comp = None\n",
    "        \n",
    "    # NOTE: a single verb to directly have both a ccomp and an xcomp dependency simultaneously \n",
    "    #         is rare and typically wouldn't occur. If a verb does have two clausal complements, \n",
    "    #         each clause would serve a different function or role in the sentence.\n",
    "    int_rate_comp = map_verb_intrans.get(main_verb.text) or map_verb_intrans.get(main_verb.lemma_)\n",
    "    if not int_rate_comp:\n",
    "        int_rate_comp = 0\n",
    "        \n",
    "    ###===================== CONDITION =====================###\n",
    "    components = {'main_aux': aux, 'main_verb': main_verb, 'main_pre_adv': pre_adv,\n",
    "                  'neg': neg, }\n",
    "    result += ability_relative_verb(main_verb, **components)\n",
    "    if comp:\n",
    "        components['aux_comp'] = get_token_dep(comp, dep='aux')               \n",
    "        temp = ability_relative_verb(comp, is_comp=True, **components)\n",
    "        result += temp\n",
    "        if len(temp) == 0:\n",
    "            # If do not contain any of that, but intransitive verb ==> Subject + aux (optional) + not (optional) + adv (optional) + verb\n",
    "            if int_rate_comp > 0.5 and comp.lemma_.lower() not in ['be', 'do', 'have']:\n",
    "                ability = base_sentence_comp(comp=comp, **components)\n",
    "                # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb\n",
    "                result += ability            \n",
    "\n",
    "    if len(result) == 0:\n",
    "        # If do not contain any of that, but intransitive verb ==> Subject + aux (optional) + not (optional) + adv (optional) + verb\n",
    "        if int_rate > 0.5 and main_verb.lemma_.lower() not in ['be', 'do', 'have']:\n",
    "            ability = base_sentence(**components)\n",
    "            # EXPECTED PATTERN: aux (optional) + not (optional) + adv (optional) + verb\n",
    "            result += ability\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3c891e8-53a4-40d2-a281-518ba9329622",
   "metadata": {},
   "outputs": [],
   "source": [
    "###============ AUXILIARY ================###\n",
    "\n",
    "def ability_aux_adj(adj, **kwargs):\n",
    "    result = []\n",
    "    # Get Preposition after adjective\n",
    "    # prep_after_adj = ' '.join(crawling_after_token_prep_phrase(adj_token))\n",
    "\n",
    "    # Generate base sentence: aux + not (optional)\n",
    "    ability = base_sentence(main_aux=kwargs.get('main_aux'),\n",
    "                            main_verb=kwargs.get('main_verb'),\n",
    "                            main_pre_adv=kwargs.get('main_pre_adv'), \n",
    "                            neg=kwargs.get('neg'))\n",
    "\n",
    "    # GET PREPOSITION AFTER ADJ\n",
    "    prep = get_token_dep(adj, dep='prep')\n",
    "    if prep:\n",
    "        phrase = ability_prep(prep, comp=None, neglect=False, phrase_only=True)\n",
    "        # Concatenate components into: aux + not (optional) + adj\n",
    "        temp = cross_product_str(ability, adj.text)\n",
    "        result += cross_product_str(temp, phrase)\n",
    "    else:\n",
    "        # Concatenate components into: aux + not (optional) + adj\n",
    "        result += cross_product_str(ability, adj.text)\n",
    "\n",
    "    # GET CONJUNCT\n",
    "    result += ability_adj_conjunct(adj, **kwargs)\n",
    "    return result\n",
    "\n",
    "def ability_aux_noun(noun, **kwargs):\n",
    "    # Get pre-modifier adjective of noun\n",
    "    pre_adj = ' '.join(extract_pre_amod(noun))\n",
    "    # Generate base sentence: aux + not (optional)\n",
    "    ability = base_sentence(main_aux=kwargs.get('main_aux'),\n",
    "                            main_verb=kwargs.get('main_verb'),\n",
    "                            main_pre_adv=kwargs.get('main_pre_adv'), \n",
    "                            neg=kwargs.get('neg'))\n",
    "    # Concatenate components into: aux + not (optional) + pre-modifier adjective (optional)\n",
    "    ability = cross_product_str(ability, pre_adj)\n",
    "    # Concatenate components into: aux + not (optional) + pre-modifier adjective (optional) + noun\n",
    "    ability = cross_product_str(ability, noun.text)\n",
    "    return ability\n",
    "\n",
    "def ability_aux_prep(prep, **kwargs):\n",
    "    # Get preposition phrase\n",
    "    phrase = ability_prep(prep, phrase_only=True)\n",
    "    # Generate base sentence: aux + not (optional)\n",
    "    ability = base_sentence(main_aux=kwargs.get('main_aux'),\n",
    "                            main_verb=kwargs.get('main_verb'),\n",
    "                            main_pre_adv=kwargs.get('main_pre_adv'), \n",
    "                            neg=kwargs.get('neg'))\n",
    "    # Concatenate components into: aux + not (optional) + pre-modifier adjective (optional) + phrase\n",
    "    ability = cross_product_str(ability, phrase)\n",
    "    return ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288340c-dfff-4378-bed2-750f444bedf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ce2a4a-cf9b-4a01-9faf-b18a87dceed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd22410-10d6-4640-8c92-0807e72810e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0700b3c-5c9a-43f1-b36e-5541dffff2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4731a9da-4f41-4c16-a4fb-13221e843618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: Helper subject active rules\n",
    "\n",
    "def is_contain_question(token):\n",
    "    questions = ['what', 'who', 'why', 'whom', 'when', 'which', 'where', 'whose', 'how']\n",
    "    tokens = get_all_token_dep(token, dep=['advmod', 'attr'])\n",
    "    for t in tokens:\n",
    "        if t.text.lower() in questions:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_perfect_sentence(sent):\n",
    "    for token in sent:\n",
    "        if token.dep_ in ['nsubj', 'nsubjpass', 'csubj', 'csubjpass']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def imperfect_sentence_rules(token):\n",
    "    properties = []\n",
    "    if token.head.text == token.text:\n",
    "        # Get compound or amod\n",
    "        temp = get_all_token_dep_left(token, dep=['compound', 'amod'])\n",
    "        properties = [t.text for t in temp]\n",
    "\n",
    "    if len(properties) > 0:           \n",
    "        properties = cross_product_str('be', properties)\n",
    "        return properties\n",
    "        \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b09456f-f615-44f8-bea0-505fe07d7131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_active_rules(token, subject):\n",
    "    abilities = []\n",
    "    # Go to its head\n",
    "    head = token.head\n",
    "\n",
    "    # If head is Verb and it is root\n",
    "    # if (head.pos_ == 'VERB') and ((head.head.text == head.text) or head.dep_ == 'conj'):\n",
    "    if (head.pos_ == 'VERB'):\n",
    "        any_question = is_contain_question(head)\n",
    "        if any_question:\n",
    "            return abilities\n",
    "        \n",
    "        elif (head.head.text == head.text):\n",
    "            verb_conjunct = [head]\n",
    "    \n",
    "            verb_conjunct += get_all_token_conj(head)\n",
    "            ###============ GET ALL TOKENS ================###\n",
    "            for verb in verb_conjunct:\n",
    "                compare = get_token_dep(verb, dep=['nsubj', 'nsubjpass'])\n",
    "                if not compare or (compare.text == subject.text):\n",
    "                    abilities += ability_verb(verb)\n",
    "\n",
    "        elif (head.pos_ == 'VERB') and (head.dep_ in ['conj', 'advcl']):\n",
    "            compare = get_token_dep(head, dep=['nsubj'])\n",
    "            if compare and (subject.text == compare.text):\n",
    "                verb_conjunct = [head]\n",
    "                if head.dep_ == 'advcl':\n",
    "                    verb_conjunct += get_all_token_conj(head)\n",
    "                for verb in verb_conjunct:\n",
    "                    abilities += ability_verb(verb)\n",
    "        \n",
    "    # If head is aux\n",
    "    elif head.pos_ == 'AUX':\n",
    "        neg = get_neglect(head)\n",
    "        components = {'main_aux': head, 'neg': neg, }\n",
    "        # Get the token\n",
    "        # NOTE: if 'AUX' is root, only have one adjective with dependency acomp.\n",
    "        adj = get_token_dep(head, dep='acomp')\n",
    "        noun = get_token_pos_right(head, pos=['NOUN', 'PROPN'])\n",
    "        if noun and noun.dep_ in ['nsubj', 'nsubjpass', 'csubj', 'csubjpass']:\n",
    "            noun = None\n",
    "        prep = get_token_dep(head, dep='prep')\n",
    "\n",
    "        if adj:\n",
    "            ability = ability_aux_adj(adj, **components)\n",
    "            # EXPECTED PATTERN: Subject + aux + not (optional) + adj\n",
    "            abilities += ability\n",
    "\n",
    "\n",
    "        if noun:\n",
    "            ability = ability_aux_noun(noun, **components)\n",
    "            # EXPECTED PATTERN: Subject + aux + not (optional) + pre-modifier adjective (optional) + noun\n",
    "            abilities += ability\n",
    "\n",
    "        if prep:\n",
    "            ability = ability_aux_prep(prep, **components)\n",
    "            # EXPECTED PATTERN: Subject + aux + not (optional) + phrase\n",
    "            abilities += ability\n",
    "            \n",
    "            \n",
    "    return abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b39907a7-a551-491d-bd93-ad0b64a4fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_passive_rules(token):\n",
    "    abilities = []\n",
    "    # Get the token head (verb). Since passive form at least form: Subject + auxpass + verb \n",
    "    head = token.head\n",
    "    if head.pos_ != 'VERB':\n",
    "        return []\n",
    "\n",
    "    ##================= GET ALL POSSIBLE COMPONENTS ===============##\n",
    "    # Get neglect; If there is no neglect, return empty text.\n",
    "    neg = get_neglect(head)\n",
    "\n",
    "    # Get auxpass token\n",
    "    auxpass = get_token_dep(head, dep='auxpass')\n",
    "    if not auxpass:\n",
    "        # Since passive sentence must have auxpass in its component.\n",
    "        return []\n",
    "\n",
    "    # Get aux token\n",
    "    aux = get_token_dep(head, dep='aux')\n",
    "\n",
    "    # Get advmod after verb token\n",
    "    advmod_main = get_token_dep_right(head, dep=['advmod', 'npadvmod'])\n",
    "    if advmod_main:\n",
    "        pre_advmod_main, post_advmod_main = extract_adv(advmod_main)\n",
    "        pre_advmod_main = ' '.join(pre_advmod_main)\n",
    "        post_advmod_main = ' '.join(post_advmod_main)\n",
    "\n",
    "    # Get the agent token\n",
    "    agent = get_token_dep_right(head, dep='agent')\n",
    "    obj_agent = None\n",
    "    # If the agent token exist\n",
    "    if agent:\n",
    "        # Get the object that refers to 'agent' token\n",
    "        obj_agent = get_token_dep(agent, dep=['pobj', 'dobj'])\n",
    "\n",
    "    # Get prepositional phrase\n",
    "    if neg:\n",
    "        prep_after_verb = crawling_after_token_prep_phrase(head)\n",
    "    else:\n",
    "        prep_after_verb = crawling_after_token_prep_phrase(head, neglect=True)\n",
    "\n",
    "    # Get xcomp token\n",
    "    xcomp = get_token_dep(head, dep='xcomp')\n",
    "    # Initalize object and advmod of xcomp.\n",
    "    obj_xcomp = None\n",
    "    advmod_xcomp = None\n",
    "    if xcomp:\n",
    "        # Get the aux, adv, and obj of xcomp tokens.\n",
    "        aux_xcomp = get_token_dep(xcomp, dep='aux')\n",
    "        obj_xcomp = get_token_dep(xcomp, dep=['pobj', 'dobj'])\n",
    "        advmod_xcomp = get_token_dep_right(xcomp, dep='advmod')\n",
    "        # If adv modifier of xcomp exist\n",
    "        if advmod_xcomp:\n",
    "            # Get pre and post adverb of main adverb modifier xcomp.\n",
    "            pre_advmod_xcomp, post_advmod_xcomp = extract_adv(advmod_xcomp)\n",
    "            pre_advmod_xcomp = ' '.join(pre_advmod_xcomp)\n",
    "            post_advmod_xcomp = ' '.join(post_advmod_xcomp)\n",
    "\n",
    "    ##================= STORING ABILITIES ===============##     \n",
    "    # Store ability: If adverb modifier exist\n",
    "    if advmod_main:\n",
    "        # If aux exist\n",
    "        if aux:\n",
    "            # Concatenate components: aux (optional) + neg (optional)\n",
    "            ability = cross_product_str(aux.text, neg)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "            ability = cross_product_str(ability, auxpass.text)\n",
    "        else:\n",
    "            # Concatenate components: auxpass + neg (optional)\n",
    "            ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "        ability = cross_product_str(ability, head.text)\n",
    "\n",
    "        # If pre adverb exist\n",
    "        if pre_advmod_main:\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + pre-advmod (optional)\n",
    "            ability = cross_product_str(ability, pre_advmod_main)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + pre-advmod (optional) + advmod\n",
    "        ability = cross_product_str(ability, advmod_main.text)\n",
    "\n",
    "        # Get prepositional phrase after adverb\n",
    "        prep_after_adv = crawling_after_token_prep_phrase(advmod_main)\n",
    "        if prep_after_adv:\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + pre-advmod (optional)\n",
    "            #                            + advmod + preposition phrase (optional)\n",
    "            ability = cross_product_str(ability, prep_after_adv)\n",
    "\n",
    "        # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + pre-advmod (optional)\n",
    "        #                      + advmod + preposition phrase (optional)\n",
    "        abilities += ability\n",
    "\n",
    "        # Get all the adverb conjuncts\n",
    "        # If neglection exist\n",
    "        if neg:\n",
    "            # It assume that all conjuncts are neglection \n",
    "            advmod_main_conj = extract_conj(advmod_main)\n",
    "        else:\n",
    "            # If neglect do not come at first, then check neglection in front each conjunct\n",
    "            advmod_main_conj = extract_conj(advmod_main, neglect=True)\n",
    "\n",
    "        # If adverb has conjunct                \n",
    "        if len(advmod_main_conj) > 0:\n",
    "            # If contain aux\n",
    "            if aux:\n",
    "                # Concatenate components: aux (optional) + neg (optional)\n",
    "                ability = cross_product_str(aux.text, neg)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                ability = cross_product_str(ability, auxpass.text)\n",
    "            else:\n",
    "                # Concatenate components: auxpass + neg (optional)\n",
    "                ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "            ability = cross_product_str(ability, head.text)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + advmod\n",
    "            ability = cross_product_str(ability, advmod_main_conj)\n",
    "            # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass\n",
    "            abilities += ability\n",
    "\n",
    "    # Store ability: If agent and object agent token exist\n",
    "    if obj_agent and agent:\n",
    "        # If aux exist\n",
    "        if aux:\n",
    "            # Concatenate components: aux (optional) + neg (optional)\n",
    "            ability = cross_product_str(aux.text, neg)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "            ability = cross_product_str(ability, auxpass.text)\n",
    "        else:\n",
    "            # Concatenate components: auxpass + neg (optional)\n",
    "            ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb \n",
    "        ability = cross_product_str(ability, head.text)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + agent\n",
    "        ability = cross_product_str(ability, agent.text)\n",
    "\n",
    "        # Get the pre adjective of object\n",
    "        pre_adj = ' '.join(extract_pre_amod(obj_agent))\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + agent + adj (optional)\n",
    "        ability = cross_product_str(ability, pre_adj)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + agent + adj (optional) + object\n",
    "        ability = cross_product_str(ability, obj_agent.text)\n",
    "        # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + agent + adj (optional) + object\n",
    "        abilities += ability\n",
    "\n",
    "        # Get all object conjuncts\n",
    "        if neg:\n",
    "            obj_agent_conj = extract_conj(obj_agent)\n",
    "        else:\n",
    "            # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "            obj_agent_conj = extract_conj(obj_agent, neglect=True)\n",
    "        # If object conjuncts exist\n",
    "        if len(obj_agent_conj) > 0:\n",
    "            # If aux exist\n",
    "            if aux:\n",
    "                # Concatenate components: aux (optional) + neg (optional)\n",
    "                ability = cross_product_str(aux.text, neg)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                ability = cross_product_str(ability, auxpass.text)\n",
    "            else:\n",
    "                # Concatenate components: auxpass + neg (optional)\n",
    "                ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "            ability = cross_product_str(ability, head.text)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + object\n",
    "            ability = cross_product_str(ability, obj_agent_conj)\n",
    "            # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + object\n",
    "            abilities += ability\n",
    "\n",
    "    # Store ability: If preposition after verb exist\n",
    "    if prep_after_verb:\n",
    "        # If aux exist\n",
    "        if aux:\n",
    "            # Concatenate components: aux (optional) + neg (optional)\n",
    "            ability = cross_product_str(aux.text, neg)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "            ability = cross_product_str(ability, auxpass.text)\n",
    "        else:\n",
    "            # Concatenate components: auxpass + neg (optional)\n",
    "            ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "        ability = cross_product_str(ability, head.text)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + preposition phrase\n",
    "        ability = cross_product_str(ability, prep_after_verb)\n",
    "        # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + preposition phrase (optional)\n",
    "        abilities += ability\n",
    "\n",
    "    # Store ability: If xcomp exist\n",
    "    if xcomp:\n",
    "        # If aux exist\n",
    "        if aux:\n",
    "            # Concatenate components: aux (optional) + neg (optional)\n",
    "            ability = cross_product_str(aux.text, neg)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "            ability = cross_product_str(ability, auxpass.text)\n",
    "        else:\n",
    "            # Concatenate components: auxpass + neg (optional)\n",
    "            ability = cross_product_str(auxpass.text, neg)\n",
    "\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "        ability = cross_product_str(ability, head.text)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp\n",
    "        ability = cross_product_str(ability, aux_xcomp.text)\n",
    "        # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp\n",
    "        ability = cross_product_str(ability, xcomp.text)\n",
    "\n",
    "        # Store ability: If object xcomp exist\n",
    "        if obj_xcomp:\n",
    "            # Get the pre adjective of object\n",
    "            pre_adj = ' '.join(extract_pre_amod(obj_xcomp))\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + adj (optional)\n",
    "            ability = cross_product_str(ability, pre_adj)\n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + adj (optional) + object\n",
    "            ability = cross_product_str(ability, obj_xcomp.text)\n",
    "            # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + adj (optional) + object\n",
    "            abilities += ability\n",
    "                    \n",
    "            # Get all object conjuncts\n",
    "            if neg:\n",
    "                obj_xcomp_conj = extract_conj(obj_xcomp)\n",
    "            else:\n",
    "                # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "                obj_xcomp_conj = extract_conj(obj_xcomp, neglect=True)\n",
    "            \n",
    "            # If object conjuncts exist\n",
    "            if len(obj_xcomp_conj) > 0:\n",
    "                # If aux exist\n",
    "                if aux:\n",
    "                    # Concatenate components: aux (optional) + neg (optional)\n",
    "                    ability = cross_product_str(aux.text, neg)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                    ability = cross_product_str(ability, auxpass.text)\n",
    "                else:\n",
    "                    # Concatenate components: auxpass + neg (optional)\n",
    "                    ability = cross_product_str(auxpass.text, neg)\n",
    "    \n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "                ability = cross_product_str(ability, head.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp\n",
    "                ability = cross_product_str(ability, aux_xcomp.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp\n",
    "                ability = cross_product_str(ability, xcomp.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + object\n",
    "                ability = cross_product_str(ability, obj_xcomp_conj)\n",
    "                # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + object\n",
    "                abilities += ability\n",
    "        \n",
    "        # Store ability: If advmod xcomp exist\n",
    "        if advmod_xcomp:\n",
    "            # If pre advmod xcomp exist\n",
    "            if pre_advmod_xcomp:\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xomp + xcomp + pre-advmod (optional)\n",
    "                ability = cross_product_str(ability, pre_advmod_xcomp)\n",
    "                \n",
    "            # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xomp + xcomp + pre-advmod (optional) + advmod\n",
    "            ability = cross_product_str(ability, advmod_xcomp.text)\n",
    "            # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + aux-xomp + xcomp + pre-advmod (optional) + advmod\n",
    "            abilities += ability  \n",
    "\n",
    "            # Get all advmod conjuncts\n",
    "            if neg:\n",
    "                advmod_xcomp_conj = extract_conj(advmod_xcomp)\n",
    "            else:\n",
    "                # If neglect do not come at first, then check neglection in front each conjunct.\n",
    "                advmod_xcomp_conj = extract_conj(advmod_xcomp, neglect=True)\n",
    "            # If advmod conjuncts exist\n",
    "            if len(advmod_xcomp_conj) > 0:\n",
    "                if aux:\n",
    "                    # Concatenate components: aux (optional) + neg (optional)\n",
    "                    ability = cross_product_str(aux.text, neg)\n",
    "                    # Concatenate components: aux (optional) + neg (optional) + auxpass\n",
    "                    ability = cross_product_str(ability, auxpass.text)\n",
    "                else:\n",
    "                    # Concatenate components: auxpass + neg (optional)\n",
    "                    ability = cross_product_str(auxpass.text, neg)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb\n",
    "                ability = cross_product_str(ability, head.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp\n",
    "                ability = cross_product_str(ability, aux_xcomp.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp\n",
    "                ability = cross_product_str(ability, xcomp.text)\n",
    "                # Concatenate components: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + advmod\n",
    "                ability = cross_product_str(ability, advmod_xcomp_conj)\n",
    "                # EXPECTED PATTERN: aux (optional) + neg (optional) + auxpass + verb + aux-xcomp + xcomp + advmod\n",
    "                abilities += ability\n",
    "               \n",
    "    return abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e15c5e3-be77-43f0-b705-3d4e920ae95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imperfect_sentence_rules(token):\n",
    "    properties = []\n",
    "    if token.head.text == token.text:\n",
    "        # Get compound or amod\n",
    "        temp = get_all_token_dep_left(token, dep=['compound', 'amod'])\n",
    "        properties = [t.text for t in temp]\n",
    "\n",
    "    if len(properties) > 0:           \n",
    "        result = cross_product_str(token.text, 'be')\n",
    "        # Get posession\n",
    "        temp = get_all_token_dep_left(token, dep='poss')\n",
    "        poss = [t.text + \"'s\" if t.pos_ == 'PROPN' else t.text for t in temp]\n",
    "        if poss:\n",
    "            result = cross_product_str(poss, result)\n",
    "        result = cross_product_str(result, properties)\n",
    "        return result\n",
    "        \n",
    "    return []\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e8afc4b-5808-42a7-b556-efb2acc1ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_abilities(doc, ant_first_pron='the user'):\n",
    "    # Define local variable.\n",
    "    storage = {}\n",
    "    first_person_pronouns = [ 'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']\n",
    "    pronouns = [\n",
    "    \"he\", \"she\", \"they\", \"it\", # Personal Pronouns (Subjective)\n",
    "    \"him\", \"her\", \"them\", \"it\", \"you\",  # Personal Pronouns (Objective)\n",
    "    \"his\", \"hers\", \"theirs\", \"its\", \"mine\", \"yours\", \"ours\",  # Possessive Pronouns\n",
    "    \"her\", \"their\", \"its\",  # Possessive Adjectives\n",
    "    \"himself\", \"herself\", \"themself\", \"themselves\", \"Itself\",  # Reflexive Pronouns,\n",
    "    \"this\", \"that\", \"these\", \"those\", # Demonstrative Pronouns\n",
    "    \"who\", \"whom\", \"whose\", \"which\", \"that\"  # Relative Pronouns\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Get sentence mapper, prepare storage, and type of sentence\n",
    "    sentence_points = {}\n",
    "    type_sentence = {}\n",
    "    for i, s in enumerate(doc.sents):\n",
    "        sentence_points[i] = (s.start, s.end)\n",
    "        storage[i] = []\n",
    "        \n",
    "        if is_perfect_sentence(s):\n",
    "            type_sentence[i] = 'perfect'\n",
    "        else:\n",
    "            type_sentence[i] = 'imperfect'\n",
    "\n",
    "    # Get mapper pronoun and antecedents\n",
    "    mapper_pron_ant = get_mapper_pron_ant(doc)\n",
    "\n",
    "    # Define local variable.\n",
    "    result = []\n",
    "    for idx, token in enumerate(doc):\n",
    "        abilities = []\n",
    "\n",
    "        ## ==================== SUBJECT ACTIVE SENTENCE =========================== ##\n",
    "        # If token is subject (should be nsubj and nsubjpass). This time only nsubj\n",
    "        # In case active sentence form\n",
    "        if token.dep_ == 'nsubj':\n",
    "            abilities += subject_active_rules(token, subject=token)\n",
    "\n",
    "        ## ==================== SUBJECT PASSIVE SENTENCE =========================== ##\n",
    "        # If sentence is passive form.\n",
    "        if token.dep_ == 'nsubjpass':\n",
    "            abilities += subject_passive_rules(token)\n",
    "\n",
    "        ## ==================== IF TOKEN \n",
    "        sentence_location = get_sentence_location(sentence_points, idx)\n",
    "        if (type_sentence[sentence_location] == 'imperfect') and token.pos_ in ['NOUN', 'PROPN', 'PRON']:\n",
    "            abilities += imperfect_sentence_rules(token)\n",
    "            \n",
    "                \n",
    "        # Store final result\n",
    "        if len(abilities) > 0:\n",
    "            # Subject handling\n",
    "            subject = token.lemma_\n",
    "            # subjects = [token] + get_all_token_conj(token)\n",
    "            # for subject in subjects:\n",
    "            #     # Get posession\n",
    "            #     temp = get_token_dep_left(token, dep='poss')\n",
    "            #     if temp:\n",
    "            #         subjects = cross_product_str((temp.text + \"'s\") if temp.pos_ == 'PROPN' else temp.text, subject.text)\n",
    "                \n",
    "            # current_idx = token.i\n",
    "            # If the subject is pronouns and first person pronouns\n",
    "            # if token.pos_ == 'PRON' and token.text.lower() in first_person_pronouns:\n",
    "            #     subject = ant_first_pron\n",
    "            # # If subject is pronouns and its token location in mapper_pron_ant\n",
    "            # elif token.pos_ == 'PRON' and idx in mapper_pron_ant.keys():\n",
    "            #     # Get the antecedent index location\n",
    "            #     idx_map = mapper_pron_ant[idx]\n",
    "            #     # Change current token subject\n",
    "            #     token = doc[idx_map]\n",
    "            #     subject = token.lemma_\n",
    "            # # If the current child is pronoun (but not in mapper_pron_ant keys)\n",
    "            # elif token.pos_ == 'PRON' and token.text.lower() in pronouns:\n",
    "            #     continue\n",
    "            # # If token only contains special characters or numbers, or length text less than 3 (NOT PRONOUNS)\n",
    "            # elif (re.match(r'^[0-9\\W]+$', token.text)) or (len(token.text) < 3):\n",
    "            #     continue\n",
    "                \n",
    "            # Get all conj subject + current subject\n",
    "            subjects = [subject] + extract_conj(token)\n",
    "            # # Store result\n",
    "            # result += cross_product_tuple(subjects, abilities)\n",
    "            # Storage final result\n",
    "            sentence_location = get_sentence_location(sentence_points, idx)\n",
    "            storage[sentence_location] += cross_product_tuple(subjects, abilities)\n",
    "\n",
    "    # Storing final result\n",
    "    # Make storage unique only\n",
    "    if storage:\n",
    "        for key, value in storage.items():\n",
    "            storage[key] = list(set(value))\n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01971f72-589c-4528-856a-d9d5fa263725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ab0b4-9a7c-4339-8a98-cb87eb8d69d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b96cf-4d13-4dc2-b58d-88ac47ef2406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3144e136-d10f-4c01-9e0d-53b24ee97aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "620adcac-39b8-4f09-b33b-8888a1e64313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bing Liu's opinion word dictionary\n",
    "bing_liu_opinion_words = set()  # Add the actual list of opinion words here\n",
    "\n",
    "# Function to load opinion words from Bing Liu lexicon\n",
    "def load_opinion_words(filepath):\n",
    "    global bing_liu_opinion_words\n",
    "    temp = pd.read_table(filepath, comment=';', header=None)[0].to_list()\n",
    "    bing_liu_opinion_words = bing_liu_opinion_words.union(set(temp))\n",
    "\n",
    "\n",
    "# Load opinion words\n",
    "current_dir = os.getcwd()\n",
    "load_opinion_words(os.path.join(current_dir, 'util/opinion-lexicon-English/negative-words.txt'))\n",
    "load_opinion_words(os.path.join(current_dir, 'util/opinion-lexicon-English/positive-words.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "856a7bc6-0472-4c9b-95fe-d5d7d53e187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intransitive rate verb\n",
    "\n",
    "data_verb = pd.read_csv('verb_transitivity.tsv', sep='\\t')\n",
    "\n",
    "map_verb_intrans = data_verb[['verb', 'percent_intrans']].set_index('verb').to_dict()['percent_intrans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb91066e-c17f-446e-9cdb-deb92bdf650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nlp model\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c42b932-345d-4f3f-a766-0dff46278928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The staff were incredibly helpful and patient,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I had a great experience purchasing my phone h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Their selection of phones is amazing, and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I appreciate how the staff walked me through s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great customer service, I left with the phone ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  The staff were incredibly helpful and patient,...\n",
       "1  I had a great experience purchasing my phone h...\n",
       "2  Their selection of phones is amazing, and the ...\n",
       "3  I appreciate how the staff walked me through s...\n",
       "4  Great customer service, I left with the phone ..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "positive_reviews = [\n",
    "    \"The staff were incredibly helpful and patient, helping me find the perfect phone!\",\n",
    "    \"I had a great experience purchasing my phone here, the process was smooth and quick.\",\n",
    "    \"Their selection of phones is amazing, and the prices are very competitive!\",\n",
    "    \"I appreciate how the staff walked me through setting up my new device.\",\n",
    "    \"Great customer service, I left with the phone I wanted and all my questions answered.\",\n",
    "    \"They offer amazing deals on phones, I couldnt resist upgrading.\",\n",
    "    \"The technician fixed my phones issue faster than I expected. Highly recommend!\",\n",
    "    \"Fantastic experience, the staff really know their stuff!\",\n",
    "    \"I found the perfect phone case here, and the variety was impressive.\",\n",
    "    \"Upgrading my phone was a breeze thanks to their professional service.\",\n",
    "    \"Staff was knowledgeable and made sure I knew everything about my new phone.\",\n",
    "    \"Prices were reasonable and the staff very courteous!\",\n",
    "    \"Very happy with my purchase, the staff really went the extra mile.\",\n",
    "    \"Excellent service! They helped me find exactly what I was looking for.\",\n",
    "    \"Great deals on accessories, and the staff was super friendly!\",\n",
    "    \"I love this store! Always a smooth experience buying or fixing my phone.\",\n",
    "    \"I got a really good trade-in deal on my old phone.\",\n",
    "    \"Their repair services are quick and reliable.\",\n",
    "    \"The staff was extremely helpful in setting up my phone and transferring all my data.\",\n",
    "    \"Very professional and friendly service, Im super satisfied!\",\n",
    "    \"Great variety of phones, and the staff was very patient with my questions.\",\n",
    "    \"The process was super simple, and Im thrilled with my new phone.\",\n",
    "    \"They helped me choose a phone within my budget, which I really appreciated.\",\n",
    "    \"My phone was fixed in less than 30 minutes, such fast service!\",\n",
    "    \"Im a loyal customer because their customer service is always outstanding.\",\n",
    "    \"Best phone store in town, hands down!\",\n",
    "    \"The staff made sure I was completely comfortable with my purchase.\",\n",
    "    \"I found exactly what I needed, and they helped me get a great deal.\",\n",
    "    \"This store has a fantastic warranty service!\",\n",
    "    \"The staff was very informative, I learned a lot about phone features I didnt know about.\",\n",
    "    \"Excellent store for buying phone accessories, so much variety!\",\n",
    "    \"The phone I bought here is working perfectly, couldnt be happier.\",\n",
    "    \"They were super quick in setting up my phone, I was out of there in no time.\",\n",
    "    \"Always come here for upgrades, they never disappoint!\",\n",
    "    \"The store layout is easy to navigate and staff are always ready to help.\",\n",
    "    \"Best pricing for phone plans, they helped me save a lot!\",\n",
    "    \"Ive been to many phone stores, but this one by far provides the best service.\",\n",
    "    \"Customer service here is top-notch, they always resolve my issues quickly.\",\n",
    "    \"I always recommend this store to friends and family, they never fail to impress.\",\n",
    "    \"The staff took the time to show me all my options, no pressure sales.\",\n",
    "    \"Amazing place to buy the latest phones at great prices!\",\n",
    "    \"Their warranty plan is worth every penny, such a relief!\",\n",
    "    \"I appreciate how they were able to fix my phone on the same day.\",\n",
    "    \"Got a great deal on my new phone and an awesome case as well!\",\n",
    "    \"The staff was very accommodating when I had questions about phone features.\",\n",
    "    \"I had a great experience with their trade-in program.\",\n",
    "    \"Service was quick and efficient, I was in and out within 15 minutes!\",\n",
    "    \"They even helped me transfer all my contacts and data without extra charge.\",\n",
    "    \"My phone has been working flawlessly since I bought it from here.\",\n",
    "    \"They fixed my screen perfectly and even gave me a discount on the repair.\",\n",
    "    \"This is my go-to store for any phone issues, always reliable.\",\n",
    "    \"They offer fantastic promotions and discounts!\",\n",
    "    \"Great phone selection and even better customer service.\",\n",
    "    \"They resolved my issue very quickly and professionally.\",\n",
    "    \"I love how organized the store is and how fast they attend to customers.\",\n",
    "    \"Highly recommend this store if youre looking for good deals on phones!\",\n",
    "    \"I always leave this store feeling like I made the right purchase.\",\n",
    "    \"I received excellent advice from the sales team, they really know their products.\",\n",
    "    \"Very happy with the repair service here, my phone looks brand new!\"\n",
    "]\n",
    "\n",
    "\n",
    "negative_reviews = [\n",
    "    \"I had to wait over an hour to be helped, and the staff wasnt apologetic at all.\",\n",
    "    \"Bought a phone here that stopped working within a week, very disappointing.\",\n",
    "    \"Their prices are too high, and the selection is limited.\",\n",
    "    \"Customer service is poor, no one seemed interested in helping me.\",\n",
    "    \"I had a terrible experience, the phone they sold me was defective.\",\n",
    "    \"The staff was rude and unhelpful, Im never coming back.\",\n",
    "    \"They charged me extra for services I didnt need, felt like a scam.\",\n",
    "    \"Phone repairs took way too long, I had to come back multiple times.\",\n",
    "    \"I bought a phone, but they didnt inform me of all the hidden fees.\",\n",
    "    \"Staff seemed untrained and gave me incorrect information about the phone plan.\",\n",
    "    \"Their warranty is useless, they refused to fix my phone under it.\",\n",
    "    \"I had to return a faulty phone twice before they finally gave me a refund.\",\n",
    "    \"Very disorganized, I waited forever just to get a simple issue resolved.\",\n",
    "    \"The phone I purchased here was overpriced compared to other stores.\",\n",
    "    \"They refused to honor the promotion I came in for, very misleading.\",\n",
    "    \"I felt pressured to buy accessories I didnt need.\",\n",
    "    \"The repair was done poorly, and my phone broke again within a week.\",\n",
    "    \"Customer service was extremely slow, they need to hire more staff.\",\n",
    "    \"They didnt even check if my phone was working after the repair.\",\n",
    "    \"Terrible experience, my phone still has the same issue after getting it 'fixed'.\",\n",
    "    \"They upsold me on a phone plan I didnt need, very deceptive.\",\n",
    "    \"The staff was unprofessional and seemed like they didnt want to be there.\",\n",
    "    \"Their return policy is awful, I couldnt exchange my phone despite its defects.\",\n",
    "    \"They didnt apply the discount I was promised.\",\n",
    "    \"The store was messy and understaffed.\",\n",
    "    \"My phone broke down just after the warranty expired, very frustrating.\",\n",
    "    \"They kept trying to sell me more expensive phones when I clearly stated my budget.\",\n",
    "    \"The repair job was incomplete, and they refused to refund me.\",\n",
    "    \"Their customer service representatives were extremely rude on the phone.\",\n",
    "    \"I had to call multiple times just to get a response, very unprofessional.\",\n",
    "    \"They didnt explain anything clearly and rushed me through the purchase.\",\n",
    "    \"I regret buying from here, their post-purchase support is non-existent.\",\n",
    "    \"Phone stopped working just outside the return window, terrible quality.\",\n",
    "    \"The store was chaotic, with long lines and unhelpful staff.\",\n",
    "    \"They didnt even have the phone I wanted in stock after promising me it was available.\",\n",
    "    \"Terrible follow-up, they lost my repair order, and I had to start over.\",\n",
    "    \"I felt overcharged for a simple screen repair.\",\n",
    "    \"Bought a refurbished phone that had several issues they didnt disclose.\",\n",
    "    \"The technician damaged my phone during the repair, and they didnt take responsibility.\",\n",
    "    \"Im extremely disappointed, will not be coming back here again.\"\n",
    "]\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'review': positive_reviews + negative_reviews})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fcbd1c83-8c4e-4279-9868-f85a7870afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_words(corpus, thres_tfidf=75, thres_idf=25):\n",
    "    # Define local variables\n",
    "    storage_idf = set()\n",
    "    # storage_tfidf = set()\n",
    "    storage_tfidf = {}\n",
    "\n",
    "    # Define the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Preprocessing text\n",
    "    def preprocessing(text):\n",
    "        text = remove_extra_spaces(text)\n",
    "        text = expand_contractions(text)\n",
    "        text = remove_non_ascii(text)\n",
    "\n",
    "        # Get token of words\n",
    "        doc = nlp(text)\n",
    "        result = []\n",
    "        for token in doc:\n",
    "            t = token.lemma_.lower()\n",
    "\n",
    "            # If only contains special characters or numbers and length less than 3\n",
    "            if re.match(r'^[0-9\\W]+$', t) or len(t) < 3 or t in stop_words:\n",
    "                continue\n",
    "            else:\n",
    "                result.append(t)\n",
    "        return result\n",
    "\n",
    "    ##========= GENERATE MODEL =========##\n",
    "    # Create texts\n",
    "    texts = [preprocessing(document) for document in corpus]\n",
    "\n",
    "    # Create dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    # Convert documents into Bag-of-words format\n",
    "    corpus_bow = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Train the TF-IDF model\n",
    "    tfidf_model = gensim.models.TfidfModel(corpus_bow)\n",
    "\n",
    "    ##============ EXTRACT IMPORTANT VALUES =========##\n",
    "    # Get the idf values\n",
    "    idf_values = tfidf_model.idfs # Return (word_id: idf_values)\n",
    "    scores_idf = np.array(list(idf_values.values()))\n",
    "    \n",
    "    idf_dict = {}\n",
    "    for id, value in idf_values.items():\n",
    "        word = dictionary[id]\n",
    "        idf_dict[word] = value\n",
    "        \n",
    "\n",
    "    # Apply the model to the corpus (get corpus tfidf)\n",
    "    corpus_tfidf = tfidf_model[corpus_bow]\n",
    "\n",
    "    # Get dictionary of tfidf values and scores\n",
    "    scores_tfidf = []\n",
    "    tfidf_dict = {}\n",
    "    for doc_idx, doc in enumerate(corpus_tfidf):\n",
    "\n",
    "        dict_doc = {}\n",
    "        for word_id, score in doc:\n",
    "            word = dictionary[word_id]\n",
    "            dict_doc[word] = score\n",
    "            scores_tfidf.append(score)\n",
    "\n",
    "        tfidf_dict[doc_idx] = dict_doc\n",
    "    \n",
    "    ##=========== Get the threshold =========##\n",
    "    threshold_idf = np.percentile(scores_idf, thres_idf)\n",
    "    threshold_tfidf = np.percentile(scores_tfidf, thres_tfidf)\n",
    "\n",
    "\n",
    "    ##========== Get Words =============##\n",
    "    # IDF\n",
    "    for key, value in idf_dict.items():\n",
    "        if value <= threshold_idf:\n",
    "            storage_idf.add(key)\n",
    "\n",
    "    # TF IDF\n",
    "    # for idx_doc, dict_words in tfidf_dict.items():\n",
    "    #     for key, value in dict_words.items():\n",
    "    #         if value >= threshold_tfidf:\n",
    "    #             storage_tfidf.add(key)\n",
    "\n",
    "    for idx_doc, dict_words in tfidf_dict.items():\n",
    "        temp = set()\n",
    "        for key, value in dict_words.items():\n",
    "            if value >= threshold_tfidf:\n",
    "                temp.add(key)\n",
    "            \n",
    "        storage_tfidf[idx_doc] = temp\n",
    "\n",
    "    return storage_idf, storage_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b727df68-409e-4e3e-8f15-8c527807ff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_filter(data, id, mapper_idf=None, mapper_tfidf=None):\n",
    "    # Define the list of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocessing(text):\n",
    "        text = remove_extra_spaces(text)\n",
    "        text = expand_contractions(text)\n",
    "        text = remove_non_ascii(text)\n",
    "\n",
    "        # Get token of words\n",
    "        doc = nlp(text)\n",
    "        result_obj = []\n",
    "        result_verb_adj = []\n",
    "        for token in doc:\n",
    "            t = token.lemma_.lower()\n",
    "            # If only contains special characters or numbers and length less than 3\n",
    "            if re.match(r'^[0-9\\W]+$', t) or len(t) < 3 or t in stop_words:\n",
    "                continue\n",
    "            # If the token is adjective, noun, propn, or verb\n",
    "            if token.pos_ in ['NOUN', 'PROPN']:\n",
    "                result_obj.append(t)\n",
    "            elif token.pos_ in ['ADJ', 'VERB']:\n",
    "                result_verb_adj.append(t)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        return result_obj, result_verb_adj\n",
    "        \n",
    "    if not mapper_idf and not mapper_tfidf:\n",
    "        return data\n",
    "\n",
    "    mapper = mapper_idf | mapper_tfidf[id]\n",
    "    temp = {}\n",
    "    for idx, element in data.items():\n",
    "        temp[idx] = []\n",
    "        for d in element:\n",
    "            text = ' '.join(d)\n",
    "            compare_obj, compare_verb_adj = preprocessing(text)\n",
    "\n",
    "            is_object_pass = False\n",
    "            is_verb_adj_pass = False\n",
    "\n",
    "            # Handling object\n",
    "            if len(compare_obj) == 0:\n",
    "                is_object_pass = True\n",
    "            else:\n",
    "                for w in compare_obj:\n",
    "                    if w in mapper:\n",
    "                        is_object_pass = True\n",
    "                        break\n",
    "            \n",
    "            # Handling verb ajective\n",
    "            for w in compare_verb_adj:\n",
    "                if w in mapper:\n",
    "                    is_verb_adj_pass = True\n",
    "                    break\n",
    "\n",
    "            if is_object_pass and is_verb_adj_pass:\n",
    "                temp[idx].append(d)\n",
    "\n",
    "            # If object passed and verb-adj passed ==> True\n",
    "            # If object passed but verb-adj not passed ==> False\n",
    "            # If object not passed ==> False\n",
    "    return temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf4d8192-c356-49ef-8360-4274700a7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_words_aspect(dict_doc, idx_doc, mapper_1=None, mapper_2=None):\n",
    "    # If mapper_1 and mapper_2 is None, do not filter it.\n",
    "    if not mapper_1 and not mapper_2:\n",
    "        return dict_doc\n",
    "\n",
    "    # Copy dictionary\n",
    "    dictionary = dict_doc.copy()\n",
    "    \n",
    "    # Get mapper based on its document.\n",
    "    if mapper_2 :\n",
    "        mapper_2 = mapper_2[idx_doc]\n",
    "\n",
    "    for key, value in dictionary.items():\n",
    "        temp = []\n",
    "        for v1 in value:\n",
    "            # Since it could be multiple word, we must check one by one\n",
    "            for v in v1.split():\n",
    "                # If aspect is in mapper_1 or mapper_2 then keep it\n",
    "                if v in mapper_1 or v in mapper_2:\n",
    "                    # Append full value\n",
    "                    temp.append(v1)\n",
    "                    break\n",
    "\n",
    "        # Update list of string\n",
    "        dictionary[key] = temp\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b14d9646-27dc-434a-bbc5-ae1e9ac1a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply extraction\n",
    "\n",
    "def process_ability(x):\n",
    "    # Prepare sentence\n",
    "    texts = remove_extra_spaces(x)\n",
    "    texts = expand_contractions(x)\n",
    "    texts = remove_non_ascii(x)\n",
    "\n",
    "    # Get aspect\n",
    "    doc = nlp(texts)\n",
    "    mapper_pron_ant = get_mapper_pron_ant(doc)\n",
    "    result = get_raw_abilities(doc)\n",
    "    \n",
    "    return result\n",
    "\n",
    "df['ability'] = df['review'].apply(process_ability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2e2c44b-7325-4a47-9a97-110358c6e386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['staff were patient. staff were helpful.',\n",
       "       'process was quick. process was smooth.',\n",
       "       'selection is amazing. price are competitive.',\n",
       "       'I appreciate walked me. I appreciate walked through setting new device.',\n",
       "       'question answered. I left with phone.',\n",
       "       'I could not resist offer amazing deals on phones. I could not resist upgrading.',\n",
       "       'I expected. technician fixed faster. technician fixed issue.',\n",
       "       'experience really know stuff.',\n",
       "       'variety was impressive. I found perfect case.', '',\n",
       "       'staff was knowledgeable.', 'price were reasonable.',\n",
       "       'staff really went mile.',\n",
       "       'service service be Excellent. they helped find.',\n",
       "       'staff was friendly. deal was friendly.',\n",
       "       'I love store. buying buying be experience. fixing buying be experience.',\n",
       "       'I got good deal on old phone.',\n",
       "       'service are reliable. service are quick.',\n",
       "       'staff was helpful in setting phone.', 'I m satisfied.',\n",
       "       'staff was patient with questions. variety was patient with questions.',\n",
       "       'process was simple. I m thrilled.',\n",
       "       'they helped choose phone. they helped choose within budget.',\n",
       "       'phone was fixed in minutes.', 'I m. service is outstanding.',\n",
       "       'store hands.', 'I was comfortable with purchase. staff made.',\n",
       "       'I found needed what. they helped get great deal.',\n",
       "       'store has fantastic service.',\n",
       "       'staff was informative. I learned lot about features.',\n",
       "       'variety store be Excellent. store store be Excellent.', '',\n",
       "       'they were quick in setting phone.', '',\n",
       "       'staff are ready. layout is easy.', 'they helped save lot.',\n",
       "       'I been to many stores. one provides best service.',\n",
       "       'they always resolve quickly. service is top notch. they always resolve issues. service is notch.',\n",
       "       'they not fail recommend store. they not fail recommend to family. they not fail recommend to friends.',\n",
       "       'staff took time.', 'place place be Amazing.', 'plan is worth.',\n",
       "       'they were able.', '', 'staff was accommodating.',\n",
       "       'I had great experience with program.',\n",
       "       'service was efficient. service was quick.',\n",
       "       'they even helped transfer contacts. they even helped transfer without extra charge. they even helped data.',\n",
       "       'I bought from here. I bought it. phone has working flawlessly.',\n",
       "       'they fixed perfectly. they even gave discount on repair. they fixed screen.',\n",
       "       'this is reliable.',\n",
       "       'they offer discounts. they offer fantastic promotions.',\n",
       "       'selection selection be Great. service selection be Great. selection selection be phone. service selection be phone.',\n",
       "       'they resolved issue. they resolved very quickly professionally.',\n",
       "       'they fast attend to customers. store is organized.',\n",
       "       'you re looking for good deals.',\n",
       "       'I always leave store. I made right purchase.',\n",
       "       'they really know received excellent advice from team. they really know products.',\n",
       "       'phone looks new.',\n",
       "       'I had to wait over hour. staff was not apologetic.', '',\n",
       "       'price are high. selection is limited.',\n",
       "       'service is poor. one seemed.', 'phone was defective.',\n",
       "       'staff was unhelpful. staff was rude. I m not coming back.',\n",
       "       'they charged me. they charged extra for services. they felt like scam.',\n",
       "       'I had took way. I had took too long.',\n",
       "       'they did not inform of hidden fees. they did not inform me. I bought phone.',\n",
       "       'staff gave incorrect information about plan. staff seemed.',\n",
       "       'warranty is useless. they refused.',\n",
       "       'I had to return faulty phone. they twice gave refund.',\n",
       "       'I very disorganized waited forever.', '',\n",
       "       'they refused to honor promotion.', 'I felt pressured.',\n",
       "       'phone broke again. phone broke within week. repair was done poorly.',\n",
       "       'service was slow. they need.', 'phone was working after repair.',\n",
       "       'phone still has same issue after getting.',\n",
       "       'they upsold me. they upsold on plan.',\n",
       "       'they did not want. staff was unprofessional.',\n",
       "       'policy is awful. I could not exchange phone. I could not exchange despite defects.',\n",
       "       'they did not apply discount.',\n",
       "       'store was understaffed. store was messy.',\n",
       "       'phone broke. warranty just expired.', 'they kept trying.',\n",
       "       'they refused to refund me. job was incomplete.',\n",
       "       'representative were rude on phone.',\n",
       "       'I had to call multiple times.',\n",
       "       'they rushed me. they did not explain clearly. they rushed through purchase. they did not explain anything.',\n",
       "       'support is non.',\n",
       "       'phone stopped quality. phone stopped working outside window.',\n",
       "       'store was with long lines. store was with unhelpful staff. store was chaotic.',\n",
       "       'they did not even have after promising me. they did not even have phone. it was available.',\n",
       "       'they lost order.', 'I felt overcharged.', '',\n",
       "       'technician damaged during repair. technician damaged phone. they did not take responsibility.',\n",
       "       ''], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def contraction(x):\n",
    "    flatten = [item for sublist in x.values() for item in sublist]\n",
    "\n",
    "    temp = []\n",
    "    for t in flatten:\n",
    "        temp.append(' '.join(t))\n",
    "\n",
    "    if len(temp) > 0:\n",
    "        return '. '.join(temp) + '.'\n",
    "    return ''\n",
    "    \n",
    "corpus = df['ability'].apply(contraction).values\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06a6eb43-f9bb-48ef-9914-ba5ef0a61170",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = corpus\n",
    "\n",
    "mapper_1, mapper_2 = get_words(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2ae1235f-97ab-4f04-a0c7-c3d710063960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always',\n",
       " 'amazing',\n",
       " 'break',\n",
       " 'buy',\n",
       " 'charge',\n",
       " 'could',\n",
       " 'deal',\n",
       " 'discount',\n",
       " 'even',\n",
       " 'excellent',\n",
       " 'experience',\n",
       " 'extra',\n",
       " 'fantastic',\n",
       " 'fast',\n",
       " 'feel',\n",
       " 'find',\n",
       " 'fix',\n",
       " 'get',\n",
       " 'give',\n",
       " 'good',\n",
       " 'great',\n",
       " 'help',\n",
       " 'helpful',\n",
       " 'issue',\n",
       " 'know',\n",
       " 'leave',\n",
       " 'long',\n",
       " 'look',\n",
       " 'lot',\n",
       " 'make',\n",
       " 'need',\n",
       " 'new',\n",
       " 'offer',\n",
       " 'one',\n",
       " 'patient',\n",
       " 'phone',\n",
       " 'plan',\n",
       " 'price',\n",
       " 'process',\n",
       " 'promotion',\n",
       " 'purchase',\n",
       " 'question',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'really',\n",
       " 'refund',\n",
       " 'refuse',\n",
       " 'reliable',\n",
       " 'repair',\n",
       " 'resolve',\n",
       " 'rude',\n",
       " 'seem',\n",
       " 'selection',\n",
       " 'service',\n",
       " 'set',\n",
       " 'staff',\n",
       " 'store',\n",
       " 'take',\n",
       " 'technician',\n",
       " 'time',\n",
       " 'unhelpful',\n",
       " 'variety',\n",
       " 'wait',\n",
       " 'warranty',\n",
       " 'within',\n",
       " 'work'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8faf5158-196f-44f8-b093-1b956428d5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: set(),\n",
       " 1: {'process'},\n",
       " 2: set(),\n",
       " 3: {'appreciate', 'walk'},\n",
       " 4: {'answer'},\n",
       " 5: {'resist'},\n",
       " 6: {'technician'},\n",
       " 7: set(),\n",
       " 8: set(),\n",
       " 9: set(),\n",
       " 10: {'knowledgeable'},\n",
       " 11: {'price', 'reasonable'},\n",
       " 12: {'mile'},\n",
       " 13: set(),\n",
       " 14: {'friendly'},\n",
       " 15: set(),\n",
       " 16: {'old'},\n",
       " 17: {'service'},\n",
       " 18: {'helpful', 'set'},\n",
       " 19: {'satisfied'},\n",
       " 20: {'patient', 'question'},\n",
       " 21: {'simple', 'thrilled'},\n",
       " 22: {'choose'},\n",
       " 23: {'minute'},\n",
       " 24: {'outstanding'},\n",
       " 25: {'hand'},\n",
       " 26: {'comfortable'},\n",
       " 27: set(),\n",
       " 28: {'fantastic'},\n",
       " 29: set(),\n",
       " 30: {'excellent', 'store'},\n",
       " 31: set(),\n",
       " 32: {'quick', 'set'},\n",
       " 33: set(),\n",
       " 34: set(),\n",
       " 35: {'save'},\n",
       " 36: set(),\n",
       " 37: set(),\n",
       " 38: {'fail', 'recommend'},\n",
       " 39: {'take', 'time'},\n",
       " 40: {'place'},\n",
       " 41: {'plan', 'worth'},\n",
       " 42: {'able'},\n",
       " 43: set(),\n",
       " 44: {'accommodate'},\n",
       " 45: {'program'},\n",
       " 46: {'efficient', 'service'},\n",
       " 47: set(),\n",
       " 48: {'buy'},\n",
       " 49: set(),\n",
       " 50: {'reliable'},\n",
       " 51: {'offer'},\n",
       " 52: {'selection'},\n",
       " 53: {'resolve'},\n",
       " 54: set(),\n",
       " 55: {'look'},\n",
       " 56: set(),\n",
       " 57: set(),\n",
       " 58: {'look', 'new'},\n",
       " 59: set(),\n",
       " 60: set(),\n",
       " 61: set(),\n",
       " 62: {'poor'},\n",
       " 63: {'defective'},\n",
       " 64: set(),\n",
       " 65: {'charge'},\n",
       " 66: {'take'},\n",
       " 67: {'inform'},\n",
       " 68: set(),\n",
       " 69: {'useless'},\n",
       " 70: set(),\n",
       " 71: {'disorganized', 'forever'},\n",
       " 72: set(),\n",
       " 73: {'honor'},\n",
       " 74: {'feel', 'pressured'},\n",
       " 75: {'break'},\n",
       " 76: {'need', 'slow'},\n",
       " 77: {'repair', 'work'},\n",
       " 78: {'still'},\n",
       " 79: {'upsold'},\n",
       " 80: {'unprofessional', 'want'},\n",
       " 81: set(),\n",
       " 82: {'apply', 'discount'},\n",
       " 83: set(),\n",
       " 84: {'expire'},\n",
       " 85: {'keep', 'try'},\n",
       " 86: set(),\n",
       " 87: {'representative', 'rude'},\n",
       " 88: {'call', 'multiple'},\n",
       " 89: {'explain', 'rush'},\n",
       " 90: {'non', 'support'},\n",
       " 91: {'stop'},\n",
       " 92: {'store'},\n",
       " 93: {'even'},\n",
       " 94: {'lose', 'order'},\n",
       " 95: {'feel', 'overcharge'},\n",
       " 96: set(),\n",
       " 97: {'damage'},\n",
       " 98: set()}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ba6a041b-f714-4b58-90af-9694447cab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ability_filtered'] = [weighted_filter(data, id=id, mapper_idf=mapper_1, mapper_tfidf=mapper_2) for id, data in enumerate(df['ability'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dce8b8ca-90a3-4f3a-b832-ab93956e81a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>ability</th>\n",
       "      <th>ability_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The staff were incredibly helpful and patient,...</td>\n",
       "      <td>{0: [('staff', 'were patient'), ('staff', 'wer...</td>\n",
       "      <td>{0: [('staff', 'were patient'), ('staff', 'wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I had a great experience purchasing my phone h...</td>\n",
       "      <td>{0: [('process', 'was quick'), ('process', 'wa...</td>\n",
       "      <td>{0: [('process', 'was quick')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Their selection of phones is amazing, and the ...</td>\n",
       "      <td>{0: [('selection', 'is amazing'), ('price', 'a...</td>\n",
       "      <td>{0: [('selection', 'is amazing')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I appreciate how the staff walked me through s...</td>\n",
       "      <td>{0: [('I', 'appreciate walked me'), ('I', 'app...</td>\n",
       "      <td>{0: [('I', 'appreciate walked me')]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great customer service, I left with the phone ...</td>\n",
       "      <td>{0: [('question', 'answered'), ('I', 'left wit...</td>\n",
       "      <td>{0: [('question', 'answered'), ('I', 'left wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  The staff were incredibly helpful and patient,...   \n",
       "1  I had a great experience purchasing my phone h...   \n",
       "2  Their selection of phones is amazing, and the ...   \n",
       "3  I appreciate how the staff walked me through s...   \n",
       "4  Great customer service, I left with the phone ...   \n",
       "\n",
       "                                             ability  \\\n",
       "0  {0: [('staff', 'were patient'), ('staff', 'wer...   \n",
       "1  {0: [('process', 'was quick'), ('process', 'wa...   \n",
       "2  {0: [('selection', 'is amazing'), ('price', 'a...   \n",
       "3  {0: [('I', 'appreciate walked me'), ('I', 'app...   \n",
       "4  {0: [('question', 'answered'), ('I', 'left wit...   \n",
       "\n",
       "                                    ability_filtered  \n",
       "0  {0: [('staff', 'were patient'), ('staff', 'wer...  \n",
       "1                    {0: [('process', 'was quick')]}  \n",
       "2                 {0: [('selection', 'is amazing')]}  \n",
       "3               {0: [('I', 'appreciate walked me')]}  \n",
       "4  {0: [('question', 'answered'), ('I', 'left wit...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea8475d9-6105-482f-bc59-653576a0ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel(\"example.xlsx\", index=False)\n",
    "df.to_csv(\"example.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b8cb7-de3d-42d8-bafb-0e1980190975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
