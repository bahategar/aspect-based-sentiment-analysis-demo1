{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01a4892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model('./util/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c4f7ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Baha Tegar\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9562c45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database is locked')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error loading wordnet: <urlopen error [Errno 11001] getaddrinfo failed>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pattern\\text\\en\\wordnet\\__init__.py:57\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorpora/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m token)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Baha Tegar/nltk_data'\n    - 'C:\\\\Users\\\\Baha Tegar\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Baha Tegar\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Baha Tegar\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Baha Tegar\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pattern\\text\\en\\wordnet\\__init__.py:60\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     nltk\u001b[38;5;241m.\u001b[39mdownload(token, quiet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, raise_on_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Sometimes there are problems with the default index.xml URL. Then we will try this...\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:782\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_on_error:\n\u001b[1;32m--> 782\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m halt_on_error:\n",
      "\u001b[1;31mValueError\u001b[0m: Error loading wordnet: <urlopen error [Errno 11001] getaddrinfo failed>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalization\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnorm\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutility\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\aspect-based-sentiment-analysis-demo1\\util\\normalization.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize, word_tokenize\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tag\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet \u001b[38;5;28;01mas\u001b[39;00m wn\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pattern\\text\\en\\__init__.py:61\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     54\u001b[0m     INFINITIVE, PRESENT, PAST, FUTURE,\n\u001b[0;32m     55\u001b[0m     FIRST, SECOND, THIRD,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     PARTICIPLE\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Import inflection functions.\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minflect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     62\u001b[0m     article, referenced, DEFINITE, INDEFINITE,\n\u001b[0;32m     63\u001b[0m     pluralize, singularize, NOUN, VERB, ADJECTIVE,\n\u001b[0;32m     64\u001b[0m     grade, comparative, superlative, COMPARATIVE, SUPERLATIVE,\n\u001b[0;32m     65\u001b[0m     verbs, conjugate, lemma, lexeme, tenses,\n\u001b[0;32m     66\u001b[0m     predicative, attributive\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Import quantification functions.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minflect_quantify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     70\u001b[0m     number, numerals, quantify, reflect\n\u001b[0;32m     71\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pattern\\text\\en\\__init__.py:80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Import all submodules.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inflect\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpattern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordlist\n\u001b[0;32m     83\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pattern\\text\\en\\wordnet\\__init__.py:65\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Downloader \u001b[38;5;28;01mas\u001b[39;00m NLTKDownloader\n\u001b[0;32m     64\u001b[0m             d \u001b[38;5;241m=\u001b[39m NLTKDownloader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://nltk.github.com/nltk_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m             d\u001b[38;5;241m.\u001b[39mdownload(token, quiet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, raise_on_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Use the Brown corpus for calculating information content (IC)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m brown_ic \u001b[38;5;241m=\u001b[39m wn_ic\u001b[38;5;241m.\u001b[39mic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mic-brown.dat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\downloader.py:782\u001b[0m, in \u001b[0;36mDownloader.download\u001b[1;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[0;32m    780\u001b[0m show(msg\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_on_error:\n\u001b[1;32m--> 782\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mmessage)\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m halt_on_error:\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Error loading wordnet: <urlopen error [Errno 11001] getaddrinfo failed>"
     ]
    }
   ],
   "source": [
    "import util.normalization as norm\n",
    "import util.model as models\n",
    "import util.utility as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6667c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./mcd_result/base_mcd.csv\")\n",
    "\n",
    "df['aspect'] = df['aspect'].apply(lambda x: x.split(\", \"))\n",
    "df['token_sentence'] = df['token_sentence'].apply(lambda x: x.split('.\\n'))\n",
    "# df['topic_keys'] = df['topic_keys'].apply(lambda x: x.split(\", \"))\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c116fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_an = df[['reviewer_id', 'aspect']].copy()\n",
    "df_an.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0f37b",
   "metadata": {},
   "source": [
    "# Coding Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_categorize = {\n",
    "#     0: 'order and food',\n",
    "#     1: 'service and time',\n",
    "#     2: 'order and food',\n",
    "#     3: 'order and food',\n",
    "#     4: 'service and time',\n",
    "#     5: 'order and food',\n",
    "#     6: 'service and time',\n",
    "# }\n",
    "\n",
    "# df['category'] = df['topic'].apply(lambda x: map_categorize[x])\n",
    "\n",
    "# print(df.info())\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = \"\"\n",
    "aspects = df_an['aspect'].values\n",
    "for i in range(df_an.shape[0]):\n",
    "    text = text + \" \" + \" \".join(list(aspects[i]))\n",
    "    \n",
    "\n",
    "text = text.strip()\n",
    "\n",
    "wordcloud = WordCloud(background_color='white').generate(text)\n",
    "plt.style.use('classic')\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74436fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = {}\n",
    "\n",
    "words = text.split()\n",
    "\n",
    "for word in words:\n",
    "    word = word.lower()\n",
    "    if word in words_count:\n",
    "        words_count[word] += 1\n",
    "    else:\n",
    "        words_count[word] = 1\n",
    "        \n",
    "# words_count = dict(\n",
    "#                 sorted(words_count.items(),\n",
    "#                        key=lambda x: x[1],\n",
    "#                        reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a24a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word = pd.DataFrame(data=words_count.values(),\n",
    "                       index=words_count.keys(), \n",
    "                       columns=['count'])\n",
    "\n",
    "df_word.head(25).sort_values(by='count', ascending=True).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 75% words\n",
    "\n",
    "threshold = df_word.quantile(q=0.75).iloc[0]\n",
    "# threshold = 0.25 * df_word.max().iloc[0]\n",
    "# threshold = df_word.mean().iloc[0]\n",
    "print(threshold)\n",
    "\n",
    "df_plot = df_word[df_word['count'] > threshold]\\\n",
    "        .head(50)\n",
    "\n",
    "ax = df_plot\\\n",
    "        .sort_values(by='count', ascending=True)\\\n",
    "        .plot(kind='barh', figsize=(10, 20))\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4279e778",
   "metadata": {},
   "source": [
    "## A. First Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81abf75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_first_cycle = list(df_word[df_word['count'] > threshold].index)\n",
    "print(len(list_first_cycle))\n",
    "\n",
    "# Additional filter\n",
    "list_first_cycle = [word for word in list_first_cycle if len(word) >= 3]\n",
    "\n",
    "def first_cycle(x):\n",
    "    global list_first_cycle\n",
    "        \n",
    "    set1 = set(\" \".join(x).split(\" \"))\n",
    "    set2 = set(list_first_cycle)\n",
    "    \n",
    "    temp = set1 & set2\n",
    "    \n",
    "    if len(temp) != len(set1):\n",
    "        temp.add(\"other\")\n",
    "    return temp\n",
    "\n",
    "def get_other(x):\n",
    "    global list_first_cycle\n",
    "    \n",
    "    set1 = set(\" \".join(x).split(\" \"))\n",
    "    set2 = set(list_first_cycle)\n",
    "    \n",
    "    temp = set1.difference(set2)\n",
    "    \n",
    "    return temp\n",
    "\n",
    "def get_residu(x):\n",
    "    texts = x['residu']\n",
    "    array = x['aspect']\n",
    "\n",
    "    temp = []\n",
    "    print(texts)\n",
    "    for text in texts:\n",
    "        pattern = re.compile(rf\"{text}\")\n",
    "        \n",
    "        result = [element for element in array if pattern.search(element)]\n",
    "        temp = temp + result\n",
    "\n",
    "    return set(temp)\n",
    "\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "df_an['first_cycle'] = df_an['aspect'].progress_apply(lambda x: first_cycle(x))\n",
    "\n",
    "df_an['residu'] = df_an['aspect'].progress_apply(lambda x: get_other(x))\n",
    "\n",
    "df_an['residu'] = [get_residu(x[-1]) for x in df_an.iterrows()]\n",
    "\n",
    "df_an = df_an.explode('first_cycle').reset_index(drop=True)\n",
    "df_an"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874238b",
   "metadata": {},
   "source": [
    "## B. Second Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42652aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_second_code = {'situation': ['normal', 'line', 'large', 'long', 'regular', 'people', 'wait',\n",
    "                                 'night', 'hour', 'wrong', 'minute', 'mess', 'day'], \n",
    "                   'place': ['mcdonald', 'mcdonalds', 'location', 'place', 'car', 'restaurant', 'window', 'store', 'drive',],\n",
    "                   'emotion': ['great', 'best', 'bad', 'great', 'horrible', 'good', 'happy', 'rude',], \n",
    "                   'service': ['staff', 'customer', 'service', 'location', 'regular', 'time', 'lady', 'experience', 'crew',\n",
    "                               'manager', 'employee', 'price', 'management', 'attitude', 'work', 'issue'],\n",
    "                   'food': ['food', 'order', 'chicken', 'sandwich', 'meal', 'fry',\n",
    "                            'nugget', 'item', 'cream', 'pickle', 'strawberry', 'burger'],\n",
    "                   'drink': ['coffee', 'cup', 'item', 'drink', 'cream', 'strawberry', 'ice'] \n",
    "                  }\n",
    "\n",
    "def second_cycle(x):\n",
    "    temp = set()\n",
    "    for key in map_second_code.keys():\n",
    "        if x in map_second_code[key]:\n",
    "            temp.add(key)\n",
    "#     if len(temp) == 0:\n",
    "#         temp.add('other')\n",
    "        \n",
    "    return temp\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "df_an['second_cycle'] = df_an['first_cycle'].progress_apply(lambda x: second_cycle(x))\n",
    "# # EXPERIMENT\n",
    "experiment = df_an[['reviewer_id', 'aspect', 'residu', 'first_cycle', 'second_cycle']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1789e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract residu\n",
    "\n",
    "def extract_residu(x):\n",
    "    global map_second_code\n",
    "    \n",
    "    residu = x['residu']\n",
    "    first_cycle = x['first_cycle']\n",
    "    second_keys = list(map_second_code.keys())\n",
    "    temp = []\n",
    "    \n",
    "    if first_cycle != 'other':\n",
    "        return set()\n",
    "    \n",
    "    for s in residu:\n",
    "        result = util.get_nearest_word(s, second_keys,\n",
    "                                       model, threshold=.35)\n",
    "        \n",
    "        if result:\n",
    "            temp.append(result)\n",
    "    return set(temp)\n",
    "\n",
    "def merge_residu(x):\n",
    "    set1 = x['second_cycle']\n",
    "    set2 = x['extract_residu']\n",
    "    \n",
    "    result = set1.union(set2)\n",
    "    \n",
    "    if len(result) == 0:\n",
    "        return set(['other'])\n",
    "    return set1.union(set2)\n",
    "\n",
    "\n",
    "df_an['extract_residu'] = [extract_residu(x[-1]) for x in df_an.iterrows()]\n",
    "df_an['second_cycle'] = [merge_residu(x[-1]) for x in df_an.iterrows()]\n",
    "\n",
    "\n",
    "df_an = df_an.explode('second_cycle').reset_index(drop=True).drop('extract_residu', axis=1)\n",
    "\n",
    "df_an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store residu data\n",
    "\n",
    "residu_frame = df_an.groupby('reviewer_id')['residu']\\\n",
    "                .apply(lambda x: set().union(*x))\\\n",
    "                .reset_index()\n",
    "\n",
    "residu_frame['residu'] = residu_frame['residu'].apply(lambda x: \", \".join(list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_an[['reviewer_id', 'first_cycle', 'second_cycle']]\\\n",
    "            .copy()\\\n",
    "            .reset_index()\\\n",
    "            .rename(columns={'index': 'node_id'})\n",
    "\n",
    "print(result.info())\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbc12d",
   "metadata": {},
   "source": [
    "## C. Categorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a4619",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_categorizing = {\n",
    "    'sentiment': ['emotion'],\n",
    "    'place and service': ['service', 'situation', 'place'],\n",
    "    'food and drink': ['food', 'drink'],\n",
    "}\n",
    "\n",
    "def final_categorizing(x):\n",
    "    for key in map_categorizing.keys():\n",
    "        if x in map_categorizing[key]:\n",
    "            return key\n",
    "    return \"other\"\n",
    "\n",
    "tqdm.pandas()\n",
    "result['category'] = result['second_cycle'].progress_apply(lambda x: final_categorizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4e469",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffe8a0",
   "metadata": {},
   "source": [
    "# Linking Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d773a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "for cat in np.unique(df_an['second_cycle'].values):\n",
    "    df_plot = df_an[df_an['second_cycle'] == cat]\n",
    "    G = nx.from_pandas_edgelist(df_plot, 'first_cycle', 'second_cycle')\n",
    "    nx.draw_kamada_kawai(G, with_labels=True, node_color='skyblue', node_size=1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a45b4",
   "metadata": {},
   "source": [
    "# Add Residu to Data Base (Update Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea9e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(residu_frame, on='reviewer_id', how='left')\n",
    "\n",
    "df['aspect'] = df['aspect'].apply(lambda x: \", \".join(list(x)))\n",
    "df['token_sentence'] = df['token_sentence'].apply(lambda x: \", \".join(list(x)))\n",
    "\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ae5f9",
   "metadata": {},
   "source": [
    "# Save Important Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"./mcd_result/nodes_mcd.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbba13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./mcd_result/base_mcd_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b60a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./mcd_result/map_second_mcd.json\", 'w') as file:\n",
    "    json.dump(map_second_code, file)\n",
    "    \n",
    "with open(\"./mcd_result/map_category_mcd.json\", \"w\") as file:\n",
    "    json.dump(map_categorizing, file)\n",
    "    \n",
    "with open(\"./mcd_result/list_first_cycle_mcd.json\", \"w\") as file:\n",
    "    json.dump(list_first_cycle, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd487cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
